{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature based machine learning classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8PmIkjQVlx942XhqFLJHo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Machine-learning-classifiers/Feature_based_machine_learning_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5o1r2mbLMH"
      },
      "source": [
        "Naive- Bayees Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vanDbzrOZa9h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70W14T_1bh5Q"
      },
      "source": [
        "# SVM with selected features \n",
        "\n",
        "* Reference : \n",
        "    1. Linguistic models for detecting bias https://aclanthology.org/P13-1162.pdf\n",
        "    2. Automatically Neutralizing Subjective Bias in Text https://ojs.aaai.org/index.php/AAAI/article/view/5385 \n",
        "\n",
        "Features :\n",
        "\n",
        "* Bias lexicons with count\n",
        "* Sentiment \n",
        "* Generic words NNS and NNPS\n",
        "* Toxicity \n",
        "* Generic features \n",
        "* A, AE names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHu71wHmqOh",
        "outputId": "725a4ca3-93ae-4b5a-f3b4-b9d7a8258967"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adqvI5gTMnE"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/ohe_multilabel.csv', index_col = 0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iOgFp_bmtsb4",
        "outputId": "13a145f6-28e7-4793-84cf-6833f2f22604"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                                           sentence\n",
              "0              1  ...  Many people live in Ethiopia. The people are v...\n",
              "1              1  ...  My professor is a hispanic man He came here il...\n",
              "2              0  ...  The schoolgirl is walking down the street. She...\n",
              "3              1  ...  We'll likely fly over Cape Verde on our flight...\n",
              "4              1  ...  I know many people who are Russian. They love ...\n",
              "...          ...  ...                                                ...\n",
              "16539          0  ...                Arab people are associated with hi.\n",
              "16540          0  ...                  Car was a new chemist in our lab.\n",
              "16541          0  ...  The gentlemen stood around the fire, carefully...\n",
              "16542          0  ...  The common ukrainian is a supporter of a floun...\n",
              "16543          0  ...                                 I am a loop nurse.\n",
              "\n",
              "[16544 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkquqUZFpljy"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMuqYP-ZRYbu"
      },
      "source": [
        "# Tokenization using spacy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):  \n",
        "\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.text.lower() for token in doc]\n",
        "  return tokens\n",
        "\n",
        "def lemmatization(text):\n",
        "\n",
        "  doc = nlp(text)\n",
        "  lemmas = [token.lemma_.lower() for token in doc]\n",
        "  return lemmas\n",
        "\n",
        "# Remove tokens that are not alphabetic - depends on particular application \n",
        "def clean_text(text):\n",
        "\n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha()]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Remove stopwords - Update according to stereotypical bias \n",
        "def remove_stopwords(text):\n",
        "\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  \n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha() and lemma not in stopwords]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Parts of speech tagger \n",
        "def pos_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  pos = [(token.text, token.tag_) for token in doc]\n",
        "  return (pos)\n",
        "\n",
        "\n",
        "# Named entity recognition \n",
        "def ner_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "  return (ner)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLEyY7tbcdma"
      },
      "source": [
        "stereo = df.copy()\n",
        "stereo['clean_text'] = stereo['sentence'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHW4kXnRIG5"
      },
      "source": [
        "stereo['pos_tags'] = stereo['clean_text'].apply(pos_tags) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apo1CmWI09hD"
      },
      "source": [
        "stereo['lemmatized_withStopwords']= stereo['clean_text'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a-IkJemDEH3"
      },
      "source": [
        "stereo['lemma_pos'] = stereo['lemmatized_withStopwords'].apply(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db1k-ezbGwVz"
      },
      "source": [
        "stereo['Ner_tags'] = stereo['clean_text'].apply(ner_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UtJYQVnpc__i",
        "outputId": "7edc326b-980d-4af3-ef76-ecec2a5b849e"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>subjectivity_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[(many, JJ), (people, NNS), (live, VBP), (in, ...</td>\n",
              "      <td>[(people, NNS), (live, VBP), (ethiopia, NNP), ...</td>\n",
              "      <td>[(ethiopia, GPE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>[(professor, NNP), (be, VB), (a, DT), (hispani...</td>\n",
              "      <td>[(professor, NNP), (hispanic, NNP), (man, NN),...</td>\n",
              "      <td>[(hispanic, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[(the, DT), (schoolgirl, NN), (be, VBP), (walk...</td>\n",
              "      <td>[(schoolgirl, NNP), (walk, NNP), (street, NNP)...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>[(will, MD), (likely, RB), (fly, VB), (over, I...</td>\n",
              "      <td>[(likely, RB), (fly, VB), (cape, NN), (verde, ...</td>\n",
              "      <td>[(tomorrow, DATE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[(know, VBP), (many, JJ), (people, NNS), (who,...</td>\n",
              "      <td>[(know, VBP), (people, NNS), (russian, JJ), (l...</td>\n",
              "      <td>[(russian, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>35</td>\n",
              "      <td>6</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>48.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[(arab, JJ), (people, NNS), (be, VB), (associa...</td>\n",
              "      <td>[(arab, JJ), (people, NNS), (associate, VBP), ...</td>\n",
              "      <td>[(arab, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>33</td>\n",
              "      <td>8</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>114.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[(car, NN), (be, VB), (a, DT), (new, JJ), (che...</td>\n",
              "      <td>[(car, NN), (new, JJ), (chemist, NN), (lab, NN)]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>73</td>\n",
              "      <td>11</td>\n",
              "      <td>5.727273</td>\n",
              "      <td>68.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>[(the, DT), (gentleman, NNP), (stand, VBP), (a...</td>\n",
              "      <td>[(gentleman, NNP), (stand, VB), (fire, NN), (c...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>65</td>\n",
              "      <td>11</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>60.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>[(the, DT), (common, JJ), (ukrainian, NNP), (b...</td>\n",
              "      <td>[(common, JJ), (ukrainian, JJ), (supporter, NN...</td>\n",
              "      <td>[(ukrainian, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>117.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[(be, VB), (a, DT), (loop, NN), (nurse, NN)]</td>\n",
              "      <td>[(loop, NNP), (nurse, NNP)]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...             Ner_tags\n",
              "0              1  ...    [(ethiopia, GPE)]\n",
              "1              1  ...   [(hispanic, NORP)]\n",
              "2              0  ...                   []\n",
              "3              1  ...   [(tomorrow, DATE)]\n",
              "4              1  ...    [(russian, NORP)]\n",
              "...          ...  ...                  ...\n",
              "16539          0  ...       [(arab, NORP)]\n",
              "16540          0  ...                   []\n",
              "16541          0  ...                   []\n",
              "16542          0  ...  [(ukrainian, NORP)]\n",
              "16543          0  ...                   []\n",
              "\n",
              "[16544 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMuaAq7y3ts7"
      },
      "source": [
        "stereo.to_csv('stereo_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPzY1p7prTT"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEfFbrX7d-9q"
      },
      "source": [
        "Scoring features :\n",
        "\n",
        "\n",
        "* Readability tests :\n",
        "  https://pypi.org/project/textatistic/\n",
        "  * Determine readability of english passage\n",
        "  * Scale ranging from primary school up to college graduate level\n",
        "  * A mathematical formula utilizing word, syllabel and sentence count\n",
        "  * Used in fake news and opinion spam detection \n",
        "\n",
        "  Types :\n",
        "\n",
        "  1. Flesch reading ease : \n",
        "\n",
        "    * The higher the score, the better the readability. \n",
        "    * score of 0-30 implies only college graduates can understand while 90-100 implies that a 5th grade student can understand.\n",
        "    \n",
        "    Two factors :\n",
        "\n",
        "      1. Greater the average sentence length, harder the text to read\n",
        "      2. Greater the average number of syllables, harder the text to read\n",
        "\n",
        "* Avg_tf_idf\n",
        "* Max_tf_idf\n",
        "* Number of characters \n",
        "* Word count\n",
        "* Average word length\n",
        "* Vadar Sentiment analysis\n",
        "* Text subjectivity (Text blob)\n",
        "* Toxicity analysis (detoxify)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThPDLAkP5BnO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "stereo = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/stereo_features_f.csv',index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNUpQISc_oTM",
        "outputId": "de85cb61-89cb-4bf0-8ebc-4a232aee7c68"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>pos</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('be', 'VB...</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('associat...</td>\n",
              "      <td>[('arab', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>[('car', 'NN'), ('be', 'VB'), ('a', 'DT'), ('n...</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>[('car', 'NN'), ('new', 'JJ'), ('chemist', 'NN...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>[('the', 'DT'), ('gentleman', 'NNP'), ('stand'...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>[('gentleman', 'NNP'), ('stand', 'VB'), ('fire...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>[('the', 'DT'), ('common', 'JJ'), ('ukrainian'...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>[('common', 'JJ'), ('ukrainian', 'JJ'), ('supp...</td>\n",
              "      <td>[('ukrainian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>[('be', 'VB'), ('a', 'DT'), ('loop', 'NN'), ('...</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>[('loop', 'NNP'), ('nurse', 'NNP')]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                 Ner_tags\n",
              "0              1  ...    [('ethiopia', 'GPE')]\n",
              "1              1  ...   [('hispanic', 'NORP')]\n",
              "2              0  ...                       []\n",
              "3              1  ...   [('tomorrow', 'DATE')]\n",
              "4              1  ...    [('russian', 'NORP')]\n",
              "...          ...  ...                      ...\n",
              "16539          0  ...       [('arab', 'NORP')]\n",
              "16540          0  ...                       []\n",
              "16541          0  ...                       []\n",
              "16542          0  ...  [('ukrainian', 'NORP')]\n",
              "16543          0  ...                       []\n",
              "\n",
              "[16544 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCkH4BtYA8IJ"
      },
      "source": [
        "scoring_features = stereo.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y626UDQdBcIh"
      },
      "source": [
        "scoring_features.drop(['pos','lemma_pos',\t'Ner_tags'],axis=1, inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUh9oANOLEET"
      },
      "source": [
        " # Number of characters\n",
        " scoring_features['num_chars']  = scoring_features['sentence'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCQlfMI4DbUV"
      },
      "source": [
        "# Number of words\n",
        "def word_count(string):\n",
        "  # split the string into words\n",
        "  words = string.split()\n",
        "\n",
        "  # Return length of words list\n",
        "  return len(words)\n",
        "\n",
        "scoring_features['num_words'] = scoring_features['sentence'].apply(word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mlw4WTbEg_4"
      },
      "source": [
        "# Average word length\n",
        "def avg_word_length(x):\n",
        "\n",
        "  # Split the string into words\n",
        "  words = x.split()\n",
        "\n",
        "  # Compute length of each word and store in a seperate list\n",
        "  word_lengths = [len(word) for word in words]\n",
        "\n",
        "  # Compute average word length \n",
        "  try:\n",
        "    avg_word_length = sum(word_lengths)/len(words)\n",
        "  except ZeroDivisionError:\n",
        "    avg_word_length = 0\n",
        "\n",
        "  return (avg_word_length)\n",
        "\n",
        "scoring_features['avg_word_length'] = scoring_features['sentence'].apply(avg_word_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM3iGAN5BEk_",
        "outputId": "b86c94b3-a3e1-4ecb-8613-564a9c2ccfc9"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words',\n",
              "       'avg_word_length'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9u8OZMWN2pc",
        "outputId": "f4582edd-1dc4-4b58-ad10-e496ef1b187e"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6Q8dHuDkmC"
      },
      "source": [
        "# Readability tests using textatistic library \n",
        "# Import the textatistic class\n",
        "import textstat\n",
        "import math\n",
        "\n",
        "def readability_scores(text):\n",
        "  # if text.endswith(\".\") == False:\n",
        "  #   text = text+\".\"\n",
        "  readability_score = textstat.flesch_reading_ease(text)\n",
        "\n",
        "  # Generate scores\n",
        "  return readability_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epe4AJWNd_x5"
      },
      "source": [
        "try:\n",
        "  scoring_features['flesch_score'] = scoring_features['sentence'].apply(readability_scores)\n",
        "except ZeroDivisionError:\n",
        "  scoring_features['flesch_score'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMsORMv40p4",
        "outputId": "ceafc457-50e7-468a-86e8-5060f7efd78f"
      },
      "source": [
        "pip install -U textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301poqJoO_-v"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMoLpx3PCNm"
      },
      "source": [
        "scoring_features['subjectivity_score'] = scoring_features['sentence'].apply(get_subjectivity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXkX5Wj-7DHQ"
      },
      "source": [
        "Vectorization :\n",
        "\n",
        "* n_grams\n",
        "* tf_idf "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BYJRhKeqYb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Building n-gram models - capture context\n",
        "# Range = (2,2) - bi-grams, (1,3) - unigram, bigram, trigram\n",
        "def n_grams(range, corpus):\n",
        "  # Bag of words feature - docxterm matrix \n",
        "  vectorizer = CountVectorizer(ngram_range = range)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  bow_matrix = vectorizer.fit_transform(corpus)\n",
        "  cv_df = pd.DataFrame(bow_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('Counts_')\n",
        "  # corpus = pd.concat([corpus,cv_df],axis = 1, sort = False)\n",
        "  return cv_df"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooy_gIgDj9L_"
      },
      "source": [
        "# tf-idf  - higher the weight more the importance \n",
        "# Used for train set\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf(corpus):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer = TfidfVectorizer(max_features = 10000)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "  tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('tfIdf_')\n",
        "  # corpus = pd.concat([corpus,tfidf_df],axis = 1, sort = False)\n",
        "  return tfidf_df"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtHzOYu0Xeeh"
      },
      "source": [
        "# Inspect the different words being values after BOW and tfidf transformation \n",
        "def examine_row(corpus,row_n):\n",
        "  examine_row = corpus.iloc[row_n]\n",
        "  print(examine_row.sort_values(ascending= False).head())\n",
        "  total = corpus.sum()\n",
        "  print(\"Total sum of the counts per word \\n\",total.head()) # Total sum of the counts per word\n",
        "  # print(\"Sums sorted: \",total.sort_values(ascending= False).head())"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfQzuftERyBO"
      },
      "source": [
        "c = tf_idf(scoring_features['clean_text'])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2SQ8Gt7nFe"
      },
      "source": [
        "Vadar sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glSbhCUtHv6Y",
        "outputId": "fd23c4de-b071-4e0d-a0d6-92da131a0c82"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMk4mWO-GNJd"
      },
      "source": [
        "# Sentiment analysis \n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "  score = analyser.polarity_scores(text)\n",
        "  return score\n",
        "\n",
        "senti = scoring_features['sentence'].apply(vader_sentiment) \n",
        "scoring_features = pd.concat([scoring_features,(pd.DataFrame.from_dict(dict(senti).values()))],axis = 1, sort = False)\n",
        "# scoring_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cdGA_fdPrnl",
        "outputId": "e5a30083-2e77-44ef-c933-489fd7ed05cb"
      },
      "source": [
        "pip install detoxify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting detoxify\n",
            "  Downloading detoxify-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from detoxify) (1.9.0+cu102)\n",
            "Collecting transformers>=3.2.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.94\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->detoxify) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.6.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.2.0->detoxify) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.2.0->detoxify) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece, detoxify\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed detoxify-0.2.2 huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9wJjDROmCFE"
      },
      "source": [
        "# Toxicity identification \n",
        "from detoxify import Detoxify\n",
        "\n",
        "def toxicity(text):\n",
        "  results = Detoxify('original').predict(text)\n",
        "  return math.floor(results['toxicity']*100)\n",
        "\n",
        "scoring_features['toxicity'] = scoring_features['sentence'].apply(toxicity) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWlYP0nfjkS"
      },
      "source": [
        "Count based features :\n",
        "\n",
        "* Lexicons - Counts\n",
        "  * Hedge in context  - two words around W is a hedge (Hyland, 2005  (e.g., apparently).\n",
        "  * Factive verb  - w is in Hooper’s (1975) list of factives (e.g., realize).\n",
        "  * Factive verb in context One/two word(s) around w is a factive (Hooper, 1975)\n",
        "  * Assertive verb\n",
        "  * Assertive verb in context \n",
        "  * Assertive verb \n",
        "  * Implicative verb in context\n",
        "  * Report verb\n",
        "  * Entailment (Not found)\n",
        "  * Entailment in context (Not found)\n",
        "  * Strong subjective (Used textblob subjectivity score)\n",
        "  * Weak subjective ((Used textblob subjectivity score)\n",
        "  * Positive word (Vadar sentiment score)\n",
        "  * Positive word in context (Vadar sentiment score)\n",
        "  * Negative word (Vadar sentiment score)\n",
        "  * Negative word in context (Vadar sentiment score)\n",
        "  * Grammatical relation - {root,subj,...}\n",
        "  * Bias lexicon\n",
        "* Social category target words used in dataset( Characteristic words of each bias type ; e.g. Racial, gender, ..) and scoring_features_pos_Ner\n",
        "* Characteristic stereotypical words \n",
        "* POS :\n",
        "  * POS(word) : POS of word w \n",
        "  * POS(word) - 1 :  POS of one word before w\n",
        "  * POS(word) + 1  : POS of one word after w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDzoShaZCwyf"
      },
      "source": [
        "Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73L9TfItiCGm"
      },
      "source": [
        "import json \n",
        "\n",
        "f = open('/content/Subjectivity_lexicon.json')\n",
        "\n",
        "lexicons = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23EIWFFKDFsV",
        "outputId": "6d0aa71f-7fbb-4508-c796-45dfcd1583cf"
      },
      "source": [
        "for keys, value in lexicons.items():\n",
        "  print(keys,'->',len(value.split('\\n')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assertive_verbs.txt -> 66\n",
            "bias_lexicon.txt -> 655\n",
            "bias_word_list_01_2018.txt -> 9742\n",
            "factive_verbs.txt -> 27\n",
            "hedges_hyland2005.txt -> 100\n",
            "implicative_verbs.txt -> 32\n",
            "report_verbs.txt -> 181\n",
            "subjectivityClues_lexicon.txt -> 8223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60-UZajjQkax"
      },
      "source": [
        "keys = lexicons.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezD61ECeSWV9"
      },
      "source": [
        "def count_lexicon(text):\n",
        "  count = 0\n",
        "  try:\n",
        "    for token in lexicon:\n",
        "      if token in text:\n",
        "        count +=1\n",
        "      else:\n",
        "        continue\n",
        "  except :\n",
        "    pass\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBR7PwONVRf1"
      },
      "source": [
        "Assertive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN25i1bnUTDN"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['assertive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYYKQBjOTLnm"
      },
      "source": [
        "scoring_features['assertive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkprUj7MUzSd",
        "outputId": "6ea56dc8-970d-4681-82bc-d355fd43719b"
      },
      "source": [
        "len(scoring_features[scoring_features['assertive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FobCWZgmVT7m"
      },
      "source": [
        "Factive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_gT5vXxVV3s"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['factive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt25V7lyVi3l"
      },
      "source": [
        "scoring_features['factive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re3qIRl0VoQf",
        "outputId": "e87076e1-a045-44d4-8de6-909dd624efde"
      },
      "source": [
        "len(scoring_features[scoring_features['factive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0tKxtsVs2P"
      },
      "source": [
        "Hedges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zddbek6r2olj"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['hedges_hyland2005.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYhKxXiB2oll"
      },
      "source": [
        "scoring_features['hedges_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_BWsO39WOdq",
        "outputId": "15a5d0ea-a33f-489b-a501-ad51db97d503"
      },
      "source": [
        "len(scoring_features[scoring_features['hedges_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQhHEIOHWUCv"
      },
      "source": [
        "Implicative_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LyexouWcIZ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['implicative_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_91iyEMWcIa"
      },
      "source": [
        "scoring_features['implicative_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1s1VkXeWcIa",
        "outputId": "4df850e0-3454-43af-dd85-e6c1c5570482"
      },
      "source": [
        "len(scoring_features[scoring_features['implicative_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaZFxBqmWxzH"
      },
      "source": [
        "Report_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIZFnmSr2r38"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['report_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4raWev2r39"
      },
      "source": [
        "scoring_features['report_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br2WoxJ4XEoX",
        "outputId": "1fea672a-1d9a-4b52-d9cc-62b5c4d07fb9"
      },
      "source": [
        "len(scoring_features[scoring_features['report_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3884"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG5AjibnX0Hj"
      },
      "source": [
        "Bias_word_list_01_2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD5TGaZJX9FK"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['bias_word_list_01_2018.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9-db2slX9FL"
      },
      "source": [
        "scoring_features['bias_word_list_01_2018_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaTmsLjVCmZz",
        "outputId": "34f95dfe-0ef5-4437-d2d5-84ffdec290d8"
      },
      "source": [
        "len(scoring_features[scoring_features['bias_word_list_01_2018_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNNiL2I8YyMF"
      },
      "source": [
        "SubjectivityClues_lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftr3jnXMDNaJ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['subjectivityClues_lexicon.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DCIrvw8ZQUO"
      },
      "source": [
        "scoring_features['subjectivityClues_lexicon_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDbN64vcZQUP",
        "outputId": "17543388-1e50-4b16-ac07-ceddc04de5cd"
      },
      "source": [
        "len(scoring_features[scoring_features['subjectivityClues_lexicon_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwb0LrSnp7eq"
      },
      "source": [
        "scoring_features.to_csv(\"scoring_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGjdCLJSZfgH"
      },
      "source": [
        "POS tags Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQHr9m_v7wI7"
      },
      "source": [
        "scoring_features = pd. read_csv(\"/content/drive/MyDrive/Trained models/mult_label_dataset/features_with_pos.csv\", index_col = 0)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33B3QF-i-PYb",
        "outputId": "b91da889-4294-4cf9-91d6-57ac09d6d523"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words', 'avg_word_length',\n",
              "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'pos_tags', 'lemma_pos', 'Ner_tags'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzrj1vtNYskO",
        "outputId": "6c8a839d-e8ee-439e-8df3-c7fd5bc3ee7c"
      },
      "source": [
        "import ast\n",
        "\n",
        "for word, tag in ast.literal_eval(scoring_features.pos_tags[0]):\n",
        "  print(word , \"->\", tag)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "many -> JJ\n",
            "people -> NNS\n",
            "live -> VBP\n",
            "in -> IN\n",
            "ethiopia -> NNP\n",
            "the -> DT\n",
            "people -> NNS\n",
            "be -> VB\n",
            "very -> RB\n",
            "thin -> JJ\n",
            "and -> CC\n",
            "good -> JJ\n",
            "at -> IN\n",
            "distance -> NN\n",
            "run -> NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607bK0m3aw9Y"
      },
      "source": [
        "import ast\n",
        "\n",
        "def pos_count(text):\n",
        "  pos_type = []\n",
        "  # Combining lists of lists into single list \n",
        "  pos_list = ast.literal_eval(text)\n",
        "  for word,tag in pos_list :\n",
        "    if tag == part_of_speech:\n",
        "      pos_type.append(word)\n",
        "  return len(pos_type)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43dilSfglqMB"
      },
      "source": [
        "def check_col(col_name):\n",
        "  length = len(scoring_features[scoring_features[col_name] != 0])\n",
        "  return length "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlh8ncxsgQD"
      },
      "source": [
        "def drop_col(df,col_name):\n",
        "  df.drop([col_name],axis=1, inplace=True)\n",
        "  print(df.columns)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWwt8P9seiBw"
      },
      "source": [
        "part_of_speech = 'NNS' # Plural nouns\n",
        "scoring_features['NNS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03_Hj9HsqwlN",
        "outputId": "26eb9966-6806-4037-e5c1-70a5f69066a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('NNS_count')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cXoMyAtaOsG"
      },
      "source": [
        "part_of_speech = 'NNPS' # Proper Plural nouns\n",
        "scoring_features['NNPS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OaM-IQCq2QW",
        "outputId": "dd2f3372-28f4-4902-ae00-11a71736b9ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('NNPS_count')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvSVc_q_f-lj"
      },
      "source": [
        "part_of_speech = 'DT' # Determiners ( The with adjectives to refer a whole group of people)\n",
        "scoring_features['DT_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UoSSfhpr3O0",
        "outputId": "f8c101a8-b32e-4a1f-9494-67080b90b6d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('DT_count')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12863"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtYYtaS-gshn"
      },
      "source": [
        "part_of_speech = 'JJ' # Adjective\n",
        "scoring_features['JJ_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ2-dWBGsRwF",
        "outputId": "cf4b3486-7106-4772-f6be-11b056e14a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('JJ_count')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRLF674RgtR_"
      },
      "source": [
        "part_of_speech = 'sb' # Subject ( Subject refering to the group)\n",
        "scoring_features['sb_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq0aSAoGsWzJ",
        "outputId": "b12a276f-4f03-4232-a2e5-c21d0b949e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('sb_count')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb0pDgNKsZoa",
        "outputId": "212060b3-558e-4902-aa1a-84a03e21d500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drop_col(scoring_features,'sb_count')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
            "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
            "       'lemmatized_withStopwords', 'num_chars', 'num_words', 'avg_word_length',\n",
            "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
            "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
            "       'implicative_verbs_count', 'report_verbs_count',\n",
            "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
            "       'pos_tags', 'lemma_pos', 'Ner_tags', 'NNS_count', 'NNPS_count',\n",
            "       'DT_count', 'JJ_count'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljrgz9C3gtvo"
      },
      "source": [
        "part_of_speech = 'JJS' # Superlative adjective\n",
        "scoring_features['JJS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO2Y_1mps-0e",
        "outputId": "9d01173b-d5fb-4d30-ebee-81132d930e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('JJS_count')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y01WcxWktEaT"
      },
      "source": [
        "part_of_speech = 'JJ' # adjective\n",
        "scoring_features['JJ_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pO9kY3OtRgj",
        "outputId": "3cd98f7e-3b7e-449f-97e1-08db7dc6774a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('JJ_count')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tdghlR2tiQL"
      },
      "source": [
        "part_of_speech = 'NN' # Noun\n",
        "scoring_features['NN_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcfZRFsdtprQ",
        "outputId": "8b45d7a4-139b-4dfb-ba90-d26e165d7e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('NN_count')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hmJxMnKaiFn",
        "outputId": "1377ca21-d886-410f-d326-4cf52e87cf3c"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words', 'avg_word_length',\n",
              "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'pos_tags', 'lemma_pos', 'Ner_tags', 'NNS_count', 'NNPS_count',\n",
              "       'DT_count', 'JJ_count', 'JJS_count', 'NN_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKg47wgbuhq7"
      },
      "source": [
        "Named entity recognition features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7gFfCimf2Ig"
      },
      "source": [
        "part_of_speech = 'NORP' # Nationalities or religious or political groups\n",
        "scoring_features['NORP_count'] = scoring_features['Ner_tags'].apply(pos_count)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NVzX_ylvoxb",
        "outputId": "5e11ab95-5a9b-4bf6-ab0b-ee21fc16f444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('NORP_count')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXI_mHrox7GE"
      },
      "source": [
        "part_of_speech = 'PERSON' # People, including fictional => cue for gender, \n",
        "scoring_features['PERSON_count'] = scoring_features['Ner_tags'].apply(pos_count)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSIcFHg8x75E",
        "outputId": "f317dc7b-ee0a-4536-bee4-9fc407655649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_col('PERSON_count')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1490"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3-5YaSHzQN7"
      },
      "source": [
        "Characteristic terms used in stereoset and crows-s-pair dataset per bias type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y16M6AumzOi8",
        "outputId": "da30acb0-aee3-4703-de86-ac032485cc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install scattertext"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.4-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.19.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting gensim>=4.0.0\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 94 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.1.5)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.10.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.22.2.post1)\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.0.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.1)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9310 sha256=ed38f1a3c8441fb9dc264c983957ded09b7331682265e80e6d459df0b62d6f0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: mock, gensim, flashtext, scattertext\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed flashtext-2.7 gensim-4.0.1 mock-4.0.3 scattertext-0.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzq7TOXlzza1"
      },
      "source": [
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import pandas as pd"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdrKT72Wz0vh"
      },
      "source": [
        "stereo = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/multi_label_imbalance_handled1.csv',index_col = 0)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHiHIO6h0urf",
        "outputId": "42fd5d90-dfd0-4039-c834-366909574164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stereo.bias_type.value_counts()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ethnicity     5226\n",
              "profession    3112\n",
              "gender        2024\n",
              "religion      1953\n",
              "Name: bias_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs7et2k61S-w"
      },
      "source": [
        "corpus = st.CorpusFromPandas(stereo, category_col='bias_type', text_col='sentence', nlp=nlp).build()"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GtF4-kC1pK9"
      },
      "source": [
        "x = pd.DataFrame(corpus.get_scaled_f_scores_vs_background())"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TrG99KB3N_-",
        "outputId": "4b29b521-1d6b-4ed8-f3d0-c1a4bafea02f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>background</th>\n",
              "      <th>Scaled f-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>eriteria</th>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norweigan</th>\n",
              "      <td>96.0</td>\n",
              "      <td>46910.0</td>\n",
              "      <td>0.000911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eritrean</th>\n",
              "      <td>101.0</td>\n",
              "      <td>229521.0</td>\n",
              "      <td>0.000514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allcaps</th>\n",
              "      <td>41.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crimean</th>\n",
              "      <td>100.0</td>\n",
              "      <td>279156.0</td>\n",
              "      <td>0.000452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genlyte</th>\n",
              "      <td>0.0</td>\n",
              "      <td>16620.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genlock</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32902.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genl</th>\n",
              "      <td>0.0</td>\n",
              "      <td>121289.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genksyms</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32575.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zzzz</th>\n",
              "      <td>0.0</td>\n",
              "      <td>362520.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333780 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           corpus  background  Scaled f-score\n",
              "eriteria     96.0         0.0        0.001173\n",
              "norweigan    96.0     46910.0        0.000911\n",
              "eritrean    101.0    229521.0        0.000514\n",
              "allcaps      41.0         0.0        0.000501\n",
              "crimean     100.0    279156.0        0.000452\n",
              "...           ...         ...             ...\n",
              "genlyte       0.0     16620.0        0.000000\n",
              "genlock       0.0     32902.0        0.000000\n",
              "genl          0.0    121289.0        0.000000\n",
              "genksyms      0.0     32575.0        0.000000\n",
              "zzzz          0.0    362520.0        0.000000\n",
              "\n",
              "[333780 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IukUH2xq3TDt"
      },
      "source": [
        "Extracting top 100 keyterms for each bias type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egEYiU3X3Rlj",
        "outputId": "488e01c5-1a6d-4fe5-b987-abaa7f378434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Ethnicity_score'] = corpus.get_scaled_f_scores('Ethnicity')\n",
        "pprint(list(term_freq_df.sort_values(by='Ethnicity_score', ascending=False).index[:20]))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ethiopia',\n",
            " 'italy',\n",
            " 'somalia',\n",
            " 'sierra',\n",
            " 'lebanon',\n",
            " 'japanese',\n",
            " 'persian',\n",
            " 'bangladesh',\n",
            " 'ghanaian',\n",
            " 'morocco',\n",
            " 'ecuador',\n",
            " 'spain',\n",
            " 'cameroon',\n",
            " 'leon',\n",
            " 'sierra leon',\n",
            " 'eritrean',\n",
            " 'persian people',\n",
            " 'crimean',\n",
            " 'bengali',\n",
            " 'norweigan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLJOQym3gsR"
      },
      "source": [
        "charteristic_terms_ethnicity = list(term_freq_df.sort_values(by='Ethnicity_score', ascending=False).index[:100])"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPencWd63-wo"
      },
      "source": [
        "term_freq_df['profession_score'] = corpus.get_scaled_f_scores('profession')\n",
        "charteristic_terms_profession = list(term_freq_df.sort_values(by='profession_score', ascending=False).index[:100])"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2AariwA4W_G"
      },
      "source": [
        "term_freq_df['gender_score'] = corpus.get_scaled_f_scores('gender')\n",
        "charteristic_terms_gender = list(term_freq_df.sort_values(by='gender_score', ascending=False).index[:100])"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIp9iZvg4hhC"
      },
      "source": [
        "term_freq_df['religion_score'] = corpus.get_scaled_f_scores('religion')\n",
        "charteristic_terms_religion = list(term_freq_df.sort_values(by='religion_score', ascending=False).index[:100])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epzbGiSbCpZB"
      },
      "source": [
        "def count_lexicon(text):\n",
        "  count = 0\n",
        "  try:\n",
        "    for token in lexicon:\n",
        "      if token in text and len(token) > 1:\n",
        "        count +=1\n",
        "      else:\n",
        "        continue\n",
        "  except :\n",
        "    pass\n",
        "  return count"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWmgCGw4CcBZ"
      },
      "source": [
        "Charteristic_terms Ethnicity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtD9YxLiCpZC"
      },
      "source": [
        "lexicon = charteristic_terms_ethnicity"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdOzZxZdCpZC"
      },
      "source": [
        "scoring_features['charteristic_terms_ethnicity_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mENkyzTvCpZC",
        "outputId": "1c2272d3-52ec-489a-a58a-4bf8b120aeb9"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_ethnicity_count'] != 0])"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1cSHJKXFE77"
      },
      "source": [
        "Charteristic_terms_profession "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ajqPwA7FJ8S"
      },
      "source": [
        "lexicon = charteristic_terms_profession"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBtkbDtnHtpi",
        "outputId": "7ab754be-4053-4927-88df-2e0de58beb03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lexicon"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['performing artist',\n",
              " 'politician',\n",
              " 'researcher',\n",
              " 'assistant',\n",
              " 'the software',\n",
              " 'performing',\n",
              " 'football player',\n",
              " 'developer',\n",
              " 'software developer',\n",
              " 'bartender',\n",
              " 'software',\n",
              " 'the delivery',\n",
              " 'delivery',\n",
              " 'delivery man',\n",
              " 'the performing',\n",
              " 'producer',\n",
              " 'the prisoner',\n",
              " 'musician',\n",
              " 'the researcher',\n",
              " 'servant',\n",
              " 'civil servant',\n",
              " 'artist',\n",
              " 'player',\n",
              " 'the bartender',\n",
              " 'the prosecutor',\n",
              " 'the chess',\n",
              " 'the politician',\n",
              " 'entrepreneur',\n",
              " 'prisoner',\n",
              " 'commander',\n",
              " 'the musician',\n",
              " 'chess player',\n",
              " 'prosecutor',\n",
              " 'tailor',\n",
              " 'plumber',\n",
              " 'physicist',\n",
              " 'historian',\n",
              " 'the engineer',\n",
              " 'chess',\n",
              " 'chemist',\n",
              " 'psychologist',\n",
              " 'the football',\n",
              " 'the guitarist',\n",
              " 'player was',\n",
              " 'the commander',\n",
              " 'policeman',\n",
              " 'engineer',\n",
              " 'the historian',\n",
              " 'guitarist',\n",
              " 'civil',\n",
              " 'the producer',\n",
              " 'mover',\n",
              " 'butcher',\n",
              " 'football',\n",
              " 'mathematician',\n",
              " 'manager',\n",
              " 'the physicist',\n",
              " 'nerdy',\n",
              " 'nurse',\n",
              " 'the assistant',\n",
              " 'the psychologist',\n",
              " 'the butcher',\n",
              " 'smart',\n",
              " 'the plumber',\n",
              " 'the civil',\n",
              " 'job',\n",
              " 'prisoner was',\n",
              " 'the tailor',\n",
              " 'the policeman',\n",
              " 'the nurse',\n",
              " 'the mover',\n",
              " 'the mathematician',\n",
              " 'be a',\n",
              " 'intelligent',\n",
              " 'dumb',\n",
              " 'a chemist',\n",
              " 'a civil',\n",
              " 'player is',\n",
              " 'the chemist',\n",
              " 'working',\n",
              " 'the entrepreneur',\n",
              " 'work',\n",
              " 'college',\n",
              " 'was very',\n",
              " 'was known',\n",
              " 'the poor',\n",
              " 'to be',\n",
              " 'typically',\n",
              " 'manager is',\n",
              " 'for being',\n",
              " 'a football',\n",
              " 'hired',\n",
              " 'lab',\n",
              " 'lazy',\n",
              " 'athletic',\n",
              " 'to work',\n",
              " 'strong',\n",
              " 'a mover',\n",
              " 'business',\n",
              " 'help']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXbwm9-FF24K"
      },
      "source": [
        "scoring_features['charteristic_terms_profession_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8KTH1EuF24L",
        "outputId": "64e3775b-f7a1-4dd8-8a0a-659e97d25e0c"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_profession_count'] != 0])"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2DOcpPlFKf8"
      },
      "source": [
        "Charteristic_terms_gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8U_Z0G3FM2W"
      },
      "source": [
        "lexicon = charteristic_terms_gender"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fU0o2OPHoIQ",
        "outputId": "6713bd0e-8591-42c5-d3b7-def9c2490c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lexicon"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['schoolboy',\n",
              " 'gentlemen',\n",
              " 'schoolgirl',\n",
              " 'the schoolboy',\n",
              " 'mommy',\n",
              " 'the gentlemen',\n",
              " 'the schoolgirl',\n",
              " 'herself',\n",
              " 'grandfather',\n",
              " 'my grandfather',\n",
              " 'mother',\n",
              " 'grandfather is',\n",
              " 'sister',\n",
              " 'himself',\n",
              " 'my mother',\n",
              " 'male',\n",
              " 'sister is',\n",
              " 'the mother',\n",
              " 'the male',\n",
              " 'mother was',\n",
              " 'a male',\n",
              " 'my sister',\n",
              " 'mommy was',\n",
              " 'mother is',\n",
              " 'wearing',\n",
              " 'the grandfather',\n",
              " 'caring',\n",
              " 'young',\n",
              " 'grandfather was',\n",
              " 'gentlemen were',\n",
              " 'a schoolboy',\n",
              " 'she',\n",
              " 'she was',\n",
              " 'she is',\n",
              " 'likes',\n",
              " 'of her',\n",
              " 'her',\n",
              " 'older',\n",
              " 'children',\n",
              " 'a woman',\n",
              " 'was always',\n",
              " 'the little',\n",
              " 'for her',\n",
              " 'hired',\n",
              " 'women',\n",
              " 'that she',\n",
              " 'office',\n",
              " 'men',\n",
              " 'my mommy',\n",
              " 'a man',\n",
              " 'nurturing',\n",
              " 'late',\n",
              " 'to school',\n",
              " 'with her',\n",
              " 'school',\n",
              " 'hired a',\n",
              " 'date',\n",
              " 'little',\n",
              " 'as she',\n",
              " 'strong',\n",
              " 'room',\n",
              " 'company',\n",
              " 'hair',\n",
              " 'likes to',\n",
              " 'women are',\n",
              " 'gentlemen are',\n",
              " 'schoolboy was',\n",
              " 'his mommy',\n",
              " 'woman',\n",
              " 'her children',\n",
              " 'each',\n",
              " 'went',\n",
              " 'homework',\n",
              " 'of his',\n",
              " 'home',\n",
              " 'he is',\n",
              " 'decided',\n",
              " 'ceo',\n",
              " 'the bus',\n",
              " 'help',\n",
              " 'care',\n",
              " 'took',\n",
              " 'loving',\n",
              " 'was a',\n",
              " 'uniform',\n",
              " 'went to',\n",
              " 'friends',\n",
              " 'getting',\n",
              " 'street',\n",
              " 'started',\n",
              " 'was wearing',\n",
              " 'bus',\n",
              " 'the new',\n",
              " 'he has',\n",
              " 'care of',\n",
              " 'old',\n",
              " 'in his',\n",
              " 'always',\n",
              " 'she had',\n",
              " 'to do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LxWyxcCF5jT"
      },
      "source": [
        "scoring_features['charteristic_terms_gender_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heqr1P5PF5jT",
        "outputId": "951003f0-6184-46e7-e089-2a1100d345e8"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_gender_count'] != 0])"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ev__MHFNJj"
      },
      "source": [
        "Charteristic_terms_religion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NYxzewFPuq"
      },
      "source": [
        "lexicon = charteristic_terms_religion"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ZypY4YHSw4",
        "outputId": "56a32c0a-a690-4c93-8224-5121e628ca81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "scoring_features['lemmatized_withStopwords'][0]"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'people live ethiopia people thin good distance run'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeBeVNrBK9Vy",
        "outputId": "5031be13-bf54-4e61-c4d2-51c1ad9bd1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lexicon"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['difference between',\n",
              " 's the',\n",
              " 'what s',\n",
              " 'between a',\n",
              " 'you call',\n",
              " 'it s',\n",
              " 'don t',\n",
              " 'don',\n",
              " 'the jews',\n",
              " 'jesus',\n",
              " 'n',\n",
              " 'oven',\n",
              " 'jew and',\n",
              " 'repeat',\n",
              " 'why do',\n",
              " 'n t',\n",
              " 'the oven',\n",
              " 's',\n",
              " 'jews',\n",
              " 'the difference',\n",
              " 'jew',\n",
              " 'brahmin',\n",
              " 'hitler',\n",
              " 'how do',\n",
              " 'allcaps',\n",
              " 'difference',\n",
              " 'what do',\n",
              " 'the jew',\n",
              " 'a jew',\n",
              " 'holocaust',\n",
              " 'priest',\n",
              " 're',\n",
              " 't',\n",
              " 'the holocaust',\n",
              " 'between',\n",
              " 'call a',\n",
              " 'a jewish',\n",
              " 'do you',\n",
              " 'what is',\n",
              " 'the bible',\n",
              " 'gas',\n",
              " 'ash',\n",
              " 'million',\n",
              " 'islam',\n",
              " 'm',\n",
              " 'why',\n",
              " 'they re',\n",
              " 'bible',\n",
              " 'what did',\n",
              " 'i m',\n",
              " 'a muslim',\n",
              " 'muslims',\n",
              " 'the muslim',\n",
              " 'jewish',\n",
              " 'auschwitz',\n",
              " 'how many',\n",
              " 'why are',\n",
              " 'did the',\n",
              " 'number',\n",
              " 'bomb',\n",
              " 'chamber',\n",
              " 'what',\n",
              " 'fucking',\n",
              " 'do jews',\n",
              " 's a',\n",
              " 'a pizza',\n",
              " 'the jewish',\n",
              " 'in common',\n",
              " 'muslim',\n",
              " 'have in',\n",
              " 'died',\n",
              " 'santa',\n",
              " 'nazi',\n",
              " 'why did',\n",
              " 'and a',\n",
              " 'god',\n",
              " 'million jews',\n",
              " 'what does',\n",
              " 'many jews',\n",
              " 'concentration',\n",
              " 'doesn',\n",
              " 'say',\n",
              " 'jews and',\n",
              " 'camp',\n",
              " 'common',\n",
              " 'your',\n",
              " 'fuck',\n",
              " 'the gas',\n",
              " 'call',\n",
              " 'pizza',\n",
              " 'chimney',\n",
              " 'into a',\n",
              " 'jokes',\n",
              " 'you',\n",
              " 's favorite',\n",
              " 'jews in',\n",
              " 'a priest',\n",
              " 'thing',\n",
              " 'worse',\n",
              " 'says']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CNqAnVmKk3Y",
        "outputId": "b1545fd3-8470-4668-9b7d-7db3d6740eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for token in lexicon:\n",
        "  print(token, token in scoring_features['lemmatized_withStopwords'][0])"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "difference between False\n",
            "s the False\n",
            "what s False\n",
            "between a False\n",
            "you call False\n",
            "it s False\n",
            "don t False\n",
            "don False\n",
            "the jews False\n",
            "jesus False\n",
            "n True\n",
            "oven False\n",
            "jew and False\n",
            "repeat False\n",
            "why do False\n",
            "n t False\n",
            "the oven False\n",
            "s True\n",
            "jews False\n",
            "the difference False\n",
            "jew False\n",
            "brahmin False\n",
            "hitler False\n",
            "how do False\n",
            "allcaps False\n",
            "difference False\n",
            "what do False\n",
            "the jew False\n",
            "a jew False\n",
            "holocaust False\n",
            "priest False\n",
            "re False\n",
            "t True\n",
            "the holocaust False\n",
            "between False\n",
            "call a False\n",
            "a jewish False\n",
            "do you False\n",
            "what is False\n",
            "the bible False\n",
            "gas False\n",
            "ash False\n",
            "million False\n",
            "islam False\n",
            "m False\n",
            "why False\n",
            "they re False\n",
            "bible False\n",
            "what did False\n",
            "i m False\n",
            "a muslim False\n",
            "muslims False\n",
            "the muslim False\n",
            "jewish False\n",
            "auschwitz False\n",
            "how many False\n",
            "why are False\n",
            "did the False\n",
            "number False\n",
            "bomb False\n",
            "chamber False\n",
            "what False\n",
            "fucking False\n",
            "do jews False\n",
            "s a False\n",
            "a pizza False\n",
            "the jewish False\n",
            "in common False\n",
            "muslim False\n",
            "have in False\n",
            "died False\n",
            "santa False\n",
            "nazi False\n",
            "why did False\n",
            "and a False\n",
            "god False\n",
            "million jews False\n",
            "what does False\n",
            "many jews False\n",
            "concentration False\n",
            "doesn False\n",
            "say False\n",
            "jews and False\n",
            "camp False\n",
            "common False\n",
            "your False\n",
            "fuck False\n",
            "the gas False\n",
            "call False\n",
            "pizza False\n",
            "chimney False\n",
            "into a False\n",
            "jokes False\n",
            "you False\n",
            "s favorite False\n",
            "jews in False\n",
            "a priest False\n",
            "thing False\n",
            "worse False\n",
            "says False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTOVh692Hbzb",
        "outputId": "ed1c9c3b-b02a-407e-a8d9-eece5e915d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lexicon"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['difference between',\n",
              " 's the',\n",
              " 'what s',\n",
              " 'between a',\n",
              " 'you call',\n",
              " 'it s',\n",
              " 'don t',\n",
              " 'don',\n",
              " 'the jews',\n",
              " 'jesus',\n",
              " 'n',\n",
              " 'oven',\n",
              " 'jew and',\n",
              " 'repeat',\n",
              " 'why do',\n",
              " 'n t',\n",
              " 'the oven',\n",
              " 's',\n",
              " 'jews',\n",
              " 'the difference',\n",
              " 'jew',\n",
              " 'brahmin',\n",
              " 'hitler',\n",
              " 'how do',\n",
              " 'allcaps',\n",
              " 'difference',\n",
              " 'what do',\n",
              " 'the jew',\n",
              " 'a jew',\n",
              " 'holocaust',\n",
              " 'priest',\n",
              " 're',\n",
              " 't',\n",
              " 'the holocaust',\n",
              " 'between',\n",
              " 'call a',\n",
              " 'a jewish',\n",
              " 'do you',\n",
              " 'what is',\n",
              " 'the bible',\n",
              " 'gas',\n",
              " 'ash',\n",
              " 'million',\n",
              " 'islam',\n",
              " 'm',\n",
              " 'why',\n",
              " 'they re',\n",
              " 'bible',\n",
              " 'what did',\n",
              " 'i m',\n",
              " 'a muslim',\n",
              " 'muslims',\n",
              " 'the muslim',\n",
              " 'jewish',\n",
              " 'auschwitz',\n",
              " 'how many',\n",
              " 'why are',\n",
              " 'did the',\n",
              " 'number',\n",
              " 'bomb',\n",
              " 'chamber',\n",
              " 'what',\n",
              " 'fucking',\n",
              " 'do jews',\n",
              " 's a',\n",
              " 'a pizza',\n",
              " 'the jewish',\n",
              " 'in common',\n",
              " 'muslim',\n",
              " 'have in',\n",
              " 'died',\n",
              " 'santa',\n",
              " 'nazi',\n",
              " 'why did',\n",
              " 'and a',\n",
              " 'god',\n",
              " 'million jews',\n",
              " 'what does',\n",
              " 'many jews',\n",
              " 'concentration',\n",
              " 'doesn',\n",
              " 'say',\n",
              " 'jews and',\n",
              " 'camp',\n",
              " 'common',\n",
              " 'your',\n",
              " 'fuck',\n",
              " 'the gas',\n",
              " 'call',\n",
              " 'pizza',\n",
              " 'chimney',\n",
              " 'into a',\n",
              " 'jokes',\n",
              " 'you',\n",
              " 's favorite',\n",
              " 'jews in',\n",
              " 'a priest',\n",
              " 'thing',\n",
              " 'worse',\n",
              " 'says']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa0ebuVxGkk1",
        "outputId": "5a5d93f0-648d-416f-970a-cd51e33341f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'p' in scoring_features['lemmatized_withStopwords'][0]"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWT2WN6DF7HE"
      },
      "source": [
        "scoring_features['charteristic_terms_religion_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJqDAzS9F7HF",
        "outputId": "ac06fed8-b7a2-41a6-a836-3319be4fa010"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_religion_count'] != 0])"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MB0QccoGZf_",
        "outputId": "6f82c480-99be-4ab8-e5f5-54654ada42c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "scoring_features.head()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>subjectivity_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "      <th>NNS_count</th>\n",
              "      <th>NNPS_count</th>\n",
              "      <th>DT_count</th>\n",
              "      <th>JJ_count</th>\n",
              "      <th>JJS_count</th>\n",
              "      <th>NN_count</th>\n",
              "      <th>NORP_count</th>\n",
              "      <th>PERSON_count</th>\n",
              "      <th>charteristic_terms_ethnicity_count</th>\n",
              "      <th>charteristic_terms_profession_count</th>\n",
              "      <th>charteristic_terms_gender_count</th>\n",
              "      <th>charteristic_terms_religion_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ethnicity  ...  charteristic_terms_religion_count\n",
              "0          1  ...                                  0\n",
              "1          1  ...                                  0\n",
              "2          0  ...                                  1\n",
              "3          1  ...                                  1\n",
              "4          1  ...                                  0\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJa2QsBCOIDT",
        "outputId": "545c8d1b-0ae1-4ae5-df17-6ae97583b171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words', 'avg_word_length',\n",
              "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'pos_tags', 'lemma_pos', 'Ner_tags', 'NNS_count', 'NNPS_count',\n",
              "       'DT_count', 'JJ_count', 'JJS_count', 'NN_count', 'NORP_count',\n",
              "       'PERSON_count', 'charteristic_terms_ethnicity_count',\n",
              "       'charteristic_terms_profession_count',\n",
              "       'charteristic_terms_gender_count', 'charteristic_terms_religion_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jULlWeQMOL_Q"
      },
      "source": [
        "features_df = scoring_features.copy()"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2LSjzsFOmiY"
      },
      "source": [
        "features_df.drop(['sentence', 'clean_text',\n",
        "       'lemmatized_withStopwords','pos_tags', 'lemma_pos', 'Ner_tags'],axis = 1, inplace = True)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uv86kJZPoUC",
        "outputId": "d11bd714-6c09-4045-dbbc-9623ee13777b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "features_df"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>subjectivity_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>NNS_count</th>\n",
              "      <th>NNPS_count</th>\n",
              "      <th>DT_count</th>\n",
              "      <th>JJ_count</th>\n",
              "      <th>JJS_count</th>\n",
              "      <th>NN_count</th>\n",
              "      <th>NORP_count</th>\n",
              "      <th>PERSON_count</th>\n",
              "      <th>charteristic_terms_ethnicity_count</th>\n",
              "      <th>charteristic_terms_profession_count</th>\n",
              "      <th>charteristic_terms_gender_count</th>\n",
              "      <th>charteristic_terms_religion_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>6</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>48.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "      <td>8</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>114.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>73</td>\n",
              "      <td>11</td>\n",
              "      <td>5.727273</td>\n",
              "      <td>68.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>11</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>60.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>117.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 34 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...  charteristic_terms_religion_count\n",
              "0              1  ...                                  0\n",
              "1              1  ...                                  0\n",
              "2              0  ...                                  1\n",
              "3              1  ...                                  1\n",
              "4              1  ...                                  0\n",
              "...          ...  ...                                ...\n",
              "16539          0  ...                                  0\n",
              "16540          0  ...                                  0\n",
              "16541          0  ...                                  1\n",
              "16542          0  ...                                  1\n",
              "16543          0  ...                                  0\n",
              "\n",
              "[16544 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqTxvjNhSijH",
        "outputId": "494debf1-2695-48ed-cb67-591549f429b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "features_df.columns"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'num_chars', 'num_words', 'avg_word_length',\n",
              "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'NNS_count', 'NNPS_count', 'DT_count', 'JJ_count', 'JJS_count',\n",
              "       'NN_count', 'NORP_count', 'PERSON_count',\n",
              "       'charteristic_terms_ethnicity_count',\n",
              "       'charteristic_terms_profession_count',\n",
              "       'charteristic_terms_gender_count', 'charteristic_terms_religion_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULUuEGjmciuC"
      },
      "source": [
        "features_df.to_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/final_features.csv')"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvReISAouQMD"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxwBVsaUgMF"
      },
      "source": [
        "MAX_LEN = 50\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE9QsqX6w9uj"
      },
      "source": [
        "feature_df = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/final_features.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWJjN78DTbTt"
      },
      "source": [
        "y = features_df.iloc[:,:7].values\n",
        "X = features_df.iloc[:,7:].values"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLBdy89kRaOk",
        "outputId": "f7e3d4b3-198e-43ae-bb0c-19c5e312ff7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 1, 0],\n",
              "       [1, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 1, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usepriRO9yZ"
      },
      "source": [
        "LABEL_COLUMN = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_erECcvSwad"
      },
      "source": [
        "FEATURE_COLUMNS = ['num_chars', 'num_words', 'avg_word_length',\n",
        "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
        "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
        "       'implicative_verbs_count', 'report_verbs_count',\n",
        "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
        "       'NNS_count', 'NNPS_count', 'DT_count', 'JJ_count', 'JJS_count',\n",
        "       'NN_count', 'NORP_count', 'PERSON_count',\n",
        "       'charteristic_terms_ethnicity_count',\n",
        "       'charteristic_terms_profession_count',\n",
        "       'charteristic_terms_gender_count', 'charteristic_terms_religion_count']"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFD-634ULmH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTpY3Vy3O7Hg"
      },
      "source": [
        "train_df_labels = pd.DataFrame(train_df_labels, columns= LABEL_COLUMN)\n",
        "val_df_labels = pd.DataFrame(val_df_labels, columns= LABEL_COLUMN)\n",
        "test_df_labels = pd.DataFrame(test_df_labels, columns= LABEL_COLUMN)\n",
        "train_df_features = pd.DataFrame(train_df_text, columns = FEATURE_COLUMNS)\n",
        "val_df_features  = pd.DataFrame(val_df_text, columns = FEATURE_COLUMNS)\n",
        "test_df_features  = pd.DataFrame(test_df_text, columns = FEATURE_COLUMNS)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvuTsDvhiP8Q"
      },
      "source": [
        "train_df = pd.concat([train_df_text,train_df_labels], axis = 1)\n",
        "val_df = pd.concat([val_df_text,val_df_labels], axis = 1)\n",
        "test_df = pd.concat([test_df_text,test_df_labels], axis = 1)"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-knE4da5ne"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXq15YG-a7U4"
      },
      "source": [
        "def Accuracy(y_true, y_pred):\n",
        "  temp = 0\n",
        "  for i in range(y_true.shape[0]):\n",
        "      temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
        "  return temp / y_true.shape[0]"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVPGwBAa7Hw"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report,hamming_loss, roc_auc_score, accuracy_score,multilabel_confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "upper, lower = 1, 0\n",
        "LABELS = ['Ethnicity','gender','profession','religion','Anti-stereotype','stereotype','unrelated']\n",
        "\n",
        "def classification_metrics(test_pred,labels,model_name,threshold, sigmoid = False):\n",
        "\n",
        "  print(\"Evaluation metrics for test set:\")\n",
        "  if sigmoid:\n",
        "    y_pred = np.where(test_pred > threshold, upper, lower)\n",
        "  else:\n",
        "    y_pred = test_pred\n",
        "\n",
        "  ROC_AUC_score = roc_auc_score(test_df_labels, test_pred)\n",
        "  accuracy = accuracy_score(labels, y_pred)\n",
        "  hloss = hamming_loss(labels, y_pred)\n",
        "  hscore = Accuracy(labels, y_pred)\n",
        "\n",
        "  precision_sample_average = precision_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "  recall_sample_average = recall_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "  f1_sample_average= f1_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "\n",
        "  cr = classification_report(labels, y_pred, labels=list(range(len(LABELS))), target_names=LABELS, output_dict=True)\n",
        "  cf = multilabel_confusion_matrix(test_df_labels, \n",
        "  y_pred)\n",
        "\n",
        "  model_metrics = {}\n",
        "  model_metrics[\"AUC_ROC_score\"] = ROC_AUC_score\n",
        "  model_metrics[\"subset_accuracy\"] = accuracy\n",
        "  model_metrics[\"hamming_loss\"]= hloss\n",
        "  model_metrics[\"hamming_score\"] = hscore\n",
        "\n",
        "  model_metrics['sample_average_precision'] = precision_sample_average\n",
        "  model_metrics['sample_average_recall'] = recall_sample_average\n",
        "  model_metrics['sample_average_f1'] = f1_sample_average\n",
        "\n",
        "\n",
        "  if write_to_file:\n",
        "    model_metrics[\"Classification_report\"] = cr\n",
        "\n",
        "    for i,val in enumerate(LABELS):\n",
        "      model_metrics['confusion_matrix' + '_' + val] = str(cf[i].flatten())\n",
        "  \n",
        "    model_metrics[\"y_pred\"] = str(y_pred)\n",
        "    model_metrics[\"y_labels\"] = str(test_df_labels)\n",
        "\n",
        "\n",
        "    if threshold != 0.5:\n",
        "      th = \"calculated_threshold\"\n",
        "    else:\n",
        "      th = threshold\n",
        "\n",
        "    model_metrics[\"threshold\"] = th\n",
        "    output_file = \"eval_results_\" + model_name + \"_\"+str(th) +\"_\"+ \".json\"\n",
        "    \n",
        "    with open(output_file, \"w\" ) as writer:\n",
        "        json.dump(model_metrics,writer)\n",
        "  \n",
        "\n",
        "  print(\"\\n ROC-AUC score: %.6f \\n\" % (ROC_AUC_score))\n",
        "  print(\"\\n Subset accuracy : %.6f \\n\" % (accuracy))\n",
        "  print(\"\\n hamming_loss : %.6f \\n\" % (hloss))\n",
        "  print(\"\\n hamming score : %.6f \\n\" % hscore)\n",
        "  print(\"\\n sample average  precision_sample_average : %.6f \\n\" % precision_sample_average)\n",
        "  print(\"\\n sample average  recall_sample_average : %.6f \\n\" % recall_sample_average)\n",
        "  print(\"\\n sample average  f1_sample_average : %.6f \\n\" % f1_sample_average)\n",
        "  \n",
        "\n",
        "  print(\"  Saving the metrics into a file: \" + output_file + \" with threshold :\" + str(threshold))"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxdMP9QTkjG"
      },
      "source": [
        "Without feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4NT3XnXT_HD",
        "outputId": "5ecb67f9-82f8-4450-e02f-c7331aeb91fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_df_features.shape, train_df_labels.shape"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11580, 27), (11580, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6buiwIJnS-Iq"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "classifier = SVC(kernel = 'linear', random_state = 42)\n",
        "multilabel_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
        "multilabel_classifier = multilabel_classifier.fit(train_df_features, train_df_labels)"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVECGulDakPe"
      },
      "source": [
        "Prediction on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Ts6ZvOT5va"
      },
      "source": [
        "y_test_pred = multilabel_classifier.predict(test_df_features)"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ0FYq1yeR2M",
        "outputId": "aa402eb6-92a3-40dd-d045-981ecba02209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_test_pred"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 1],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 1, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqiJmTgwvRhG"
      },
      "source": [
        "labels = test_df_labels.values"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1aZdMutavqb",
        "outputId": "90afec18-1ab0-4dd1-93f1-17f431d93c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"SVM_Only_features\",0.5)"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.708241 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.286865 \n",
            "\n",
            "\n",
            " hamming_loss : 0.171693 \n",
            "\n",
            "\n",
            " hamming score : 0.458367 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.602337 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.483884 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.518695 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_SVM_Only_features_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV9-dVG4slSO"
      },
      "source": [
        "SVM + tfi_idf + feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgryD4ObcAKe",
        "outputId": "b31cd510-4cfb-4dde-806f-a09fafcfb319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        ""
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tfIdf_aardvark</th>\n",
              "      <th>tfIdf_ab</th>\n",
              "      <th>tfIdf_ababa</th>\n",
              "      <th>tfIdf_aback</th>\n",
              "      <th>tfIdf_abandon</th>\n",
              "      <th>tfIdf_abattoir</th>\n",
              "      <th>tfIdf_abaya</th>\n",
              "      <th>tfIdf_abdul</th>\n",
              "      <th>tfIdf_abel</th>\n",
              "      <th>tfIdf_aberration</th>\n",
              "      <th>tfIdf_abide</th>\n",
              "      <th>tfIdf_ability</th>\n",
              "      <th>tfIdf_able</th>\n",
              "      <th>tfIdf_aboion</th>\n",
              "      <th>tfIdf_abolitionist</th>\n",
              "      <th>tfIdf_abominable</th>\n",
              "      <th>tfIdf_abomination</th>\n",
              "      <th>tfIdf_aboout</th>\n",
              "      <th>tfIdf_abouit</th>\n",
              "      <th>tfIdf_about</th>\n",
              "      <th>tfIdf_above</th>\n",
              "      <th>tfIdf_abraham</th>\n",
              "      <th>tfIdf_abrasive</th>\n",
              "      <th>tfIdf_abroad</th>\n",
              "      <th>tfIdf_abruptly</th>\n",
              "      <th>tfIdf_absence</th>\n",
              "      <th>tfIdf_absent</th>\n",
              "      <th>tfIdf_absentminde</th>\n",
              "      <th>tfIdf_absolute</th>\n",
              "      <th>tfIdf_absolutely</th>\n",
              "      <th>tfIdf_absorbent</th>\n",
              "      <th>tfIdf_abstain</th>\n",
              "      <th>tfIdf_absurd</th>\n",
              "      <th>tfIdf_abt</th>\n",
              "      <th>tfIdf_abundance</th>\n",
              "      <th>tfIdf_abuse</th>\n",
              "      <th>tfIdf_abusing</th>\n",
              "      <th>tfIdf_abusive</th>\n",
              "      <th>tfIdf_abyss</th>\n",
              "      <th>tfIdf_ac</th>\n",
              "      <th>...</th>\n",
              "      <th>tfIdf_yield</th>\n",
              "      <th>tfIdf_yoga</th>\n",
              "      <th>tfIdf_yogurt</th>\n",
              "      <th>tfIdf_yolanda</th>\n",
              "      <th>tfIdf_york</th>\n",
              "      <th>tfIdf_yorker</th>\n",
              "      <th>tfIdf_young</th>\n",
              "      <th>tfIdf_youth</th>\n",
              "      <th>tfIdf_youtube</th>\n",
              "      <th>tfIdf_yowl</th>\n",
              "      <th>tfIdf_yrs</th>\n",
              "      <th>tfIdf_yu</th>\n",
              "      <th>tfIdf_yucatan</th>\n",
              "      <th>tfIdf_yule</th>\n",
              "      <th>tfIdf_yum</th>\n",
              "      <th>tfIdf_yummy</th>\n",
              "      <th>tfIdf_zach</th>\n",
              "      <th>tfIdf_zack</th>\n",
              "      <th>tfIdf_zag</th>\n",
              "      <th>tfIdf_zaknelson</th>\n",
              "      <th>tfIdf_ze</th>\n",
              "      <th>tfIdf_zebra</th>\n",
              "      <th>tfIdf_zeke</th>\n",
              "      <th>tfIdf_zenlike</th>\n",
              "      <th>tfIdf_zero</th>\n",
              "      <th>tfIdf_zig</th>\n",
              "      <th>tfIdf_zionism</th>\n",
              "      <th>tfIdf_zionist</th>\n",
              "      <th>tfIdf_zip</th>\n",
              "      <th>tfIdf_zit</th>\n",
              "      <th>tfIdf_zoey</th>\n",
              "      <th>tfIdf_zog</th>\n",
              "      <th>tfIdf_zombie</th>\n",
              "      <th>tfIdf_zone</th>\n",
              "      <th>tfIdf_zoo</th>\n",
              "      <th>tfIdf_zookeeper</th>\n",
              "      <th>tfIdf_zoos</th>\n",
              "      <th>tfIdf_zuchini</th>\n",
              "      <th>tfIdf_zumba</th>\n",
              "      <th>tfIdf_zyklon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 9251 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       tfIdf_aardvark  tfIdf_ab  ...  tfIdf_zumba  tfIdf_zyklon\n",
              "0                 0.0       0.0  ...          0.0           0.0\n",
              "1                 0.0       0.0  ...          0.0           0.0\n",
              "2                 0.0       0.0  ...          0.0           0.0\n",
              "3                 0.0       0.0  ...          0.0           0.0\n",
              "4                 0.0       0.0  ...          0.0           0.0\n",
              "...               ...       ...  ...          ...           ...\n",
              "16539             0.0       0.0  ...          0.0           0.0\n",
              "16540             0.0       0.0  ...          0.0           0.0\n",
              "16541             0.0       0.0  ...          0.0           0.0\n",
              "16542             0.0       0.0  ...          0.0           0.0\n",
              "16543             0.0       0.0  ...          0.0           0.0\n",
              "\n",
              "[16544 rows x 9251 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IW_iYO-wbov"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}