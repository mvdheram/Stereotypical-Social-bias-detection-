{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature based machine learning classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7+pn674B7T6ypfujUf5zO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Machine-learning-classifiers/Feature_based_machine_learning_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5o1r2mbLMH"
      },
      "source": [
        "Naive- Bayees Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vanDbzrOZa9h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70W14T_1bh5Q"
      },
      "source": [
        "# SVM with selected features \n",
        "\n",
        "* Reference : \n",
        "    1. Linguistic models for detecting bias https://aclanthology.org/P13-1162.pdf\n",
        "    2. Automatically Neutralizing Subjective Bias in Text https://ojs.aaai.org/index.php/AAAI/article/view/5385 \n",
        "\n",
        "Features :\n",
        "\n",
        "* Bias lexicons with count\n",
        "* Sentiment \n",
        "* Generic words NNS and NNPS\n",
        "* Toxicity \n",
        "* Generic features \n",
        "* A, AE names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHu71wHmqOh",
        "outputId": "9fabe477-c3bb-4cdc-9613-34b6ee97dd8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adqvI5gTMnE"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/ohe_multilabel.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "iOgFp_bmtsb4",
        "outputId": "861e24a8-60a6-4c8b-aa72-601adf986844"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                                           sentence\n",
              "0              1  ...  Many people live in Ethiopia. The people are v...\n",
              "1              1  ...  My professor is a hispanic man He came here il...\n",
              "2              0  ...  The schoolgirl is walking down the street. She...\n",
              "3              1  ...  We'll likely fly over Cape Verde on our flight...\n",
              "4              1  ...  I know many people who are Russian. They love ...\n",
              "...          ...  ...                                                ...\n",
              "16539          0  ...                Arab people are associated with hi.\n",
              "16540          0  ...                  Car was a new chemist in our lab.\n",
              "16541          0  ...  The gentlemen stood around the fire, carefully...\n",
              "16542          0  ...  The common ukrainian is a supporter of a floun...\n",
              "16543          0  ...                                 I am a loop nurse.\n",
              "\n",
              "[16544 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkquqUZFpljy"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMuqYP-ZRYbu"
      },
      "source": [
        "# Tokenization using spacy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):  \n",
        "\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.text.lower() for token in doc]\n",
        "  return tokens\n",
        "\n",
        "def lemmatization(text):\n",
        "\n",
        "  doc = nlp(text)\n",
        "  lemmas = [token.lemma_.lower() for token in doc]\n",
        "  return lemmas\n",
        "\n",
        "# Remove tokens that are not alphabetic - depends on particular application \n",
        "def clean_text(text):\n",
        "\n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha()]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Remove stopwords - Update according to stereotypical bias \n",
        "def remove_stopwords(text):\n",
        "\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  \n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha() and lemma not in stopwords]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Parts of speech tagger \n",
        "def pos_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  pos = [(token.text, token.tag_) for token in doc]\n",
        "  return (pos)\n",
        "\n",
        "\n",
        "# Named entity recognition \n",
        "def ner_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "  return (ner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLEyY7tbcdma"
      },
      "source": [
        "stereo = df.copy()\n",
        "stereo['clean_text'] = stereo['sentence'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHW4kXnRIG5"
      },
      "source": [
        "stereo['pos_tags'] = stereo['clean_text'].apply(pos_tags) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apo1CmWI09hD"
      },
      "source": [
        "stereo['lemmatized_withStopwords']= stereo['clean_text'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a-IkJemDEH3"
      },
      "source": [
        "stereo['lemma_pos'] = stereo['lemmatized_withStopwords'].apply(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db1k-ezbGwVz"
      },
      "source": [
        "stereo['Ner_tags'] = stereo['clean_text'].apply(ner_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UtJYQVnpc__i",
        "outputId": "7edc326b-980d-4af3-ef76-ecec2a5b849e"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>subjectivity_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[(many, JJ), (people, NNS), (live, VBP), (in, ...</td>\n",
              "      <td>[(people, NNS), (live, VBP), (ethiopia, NNP), ...</td>\n",
              "      <td>[(ethiopia, GPE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>[(professor, NNP), (be, VB), (a, DT), (hispani...</td>\n",
              "      <td>[(professor, NNP), (hispanic, NNP), (man, NN),...</td>\n",
              "      <td>[(hispanic, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[(the, DT), (schoolgirl, NN), (be, VBP), (walk...</td>\n",
              "      <td>[(schoolgirl, NNP), (walk, NNP), (street, NNP)...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>[(will, MD), (likely, RB), (fly, VB), (over, I...</td>\n",
              "      <td>[(likely, RB), (fly, VB), (cape, NN), (verde, ...</td>\n",
              "      <td>[(tomorrow, DATE)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[(know, VBP), (many, JJ), (people, NNS), (who,...</td>\n",
              "      <td>[(know, VBP), (people, NNS), (russian, JJ), (l...</td>\n",
              "      <td>[(russian, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>35</td>\n",
              "      <td>6</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>48.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[(arab, JJ), (people, NNS), (be, VB), (associa...</td>\n",
              "      <td>[(arab, JJ), (people, NNS), (associate, VBP), ...</td>\n",
              "      <td>[(arab, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>33</td>\n",
              "      <td>8</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>114.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[(car, NN), (be, VB), (a, DT), (new, JJ), (che...</td>\n",
              "      <td>[(car, NN), (new, JJ), (chemist, NN), (lab, NN)]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>73</td>\n",
              "      <td>11</td>\n",
              "      <td>5.727273</td>\n",
              "      <td>68.77</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>[(the, DT), (gentleman, NNP), (stand, VBP), (a...</td>\n",
              "      <td>[(gentleman, NNP), (stand, VB), (fire, NN), (c...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>65</td>\n",
              "      <td>11</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>60.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>[(the, DT), (common, JJ), (ukrainian, NNP), (b...</td>\n",
              "      <td>[(common, JJ), (ukrainian, JJ), (supporter, NN...</td>\n",
              "      <td>[(ukrainian, NORP)]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>117.16</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[(be, VB), (a, DT), (loop, NN), (nurse, NN)]</td>\n",
              "      <td>[(loop, NNP), (nurse, NNP)]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...             Ner_tags\n",
              "0              1  ...    [(ethiopia, GPE)]\n",
              "1              1  ...   [(hispanic, NORP)]\n",
              "2              0  ...                   []\n",
              "3              1  ...   [(tomorrow, DATE)]\n",
              "4              1  ...    [(russian, NORP)]\n",
              "...          ...  ...                  ...\n",
              "16539          0  ...       [(arab, NORP)]\n",
              "16540          0  ...                   []\n",
              "16541          0  ...                   []\n",
              "16542          0  ...  [(ukrainian, NORP)]\n",
              "16543          0  ...                   []\n",
              "\n",
              "[16544 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMuaAq7y3ts7"
      },
      "source": [
        "stereo.to_csv('stereo_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPzY1p7prTT"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEfFbrX7d-9q"
      },
      "source": [
        "Scoring features :\n",
        "\n",
        "\n",
        "* Readability tests :\n",
        "  https://pypi.org/project/textatistic/\n",
        "  * Determine readability of english passage\n",
        "  * Scale ranging from primary school up to college graduate level\n",
        "  * A mathematical formula utilizing word, syllabel and sentence count\n",
        "  * Used in fake news and opinion spam detection \n",
        "\n",
        "  Types :\n",
        "\n",
        "  1. Flesch reading ease : \n",
        "\n",
        "    * The higher the score, the better the readability. \n",
        "    * score of 0-30 implies only college graduates can understand while 90-100 implies that a 5th grade student can understand.\n",
        "    \n",
        "    Two factors :\n",
        "\n",
        "      1. Greater the average sentence length, harder the text to read\n",
        "      2. Greater the average number of syllables, harder the text to read\n",
        "\n",
        "* Avg_tf_idf\n",
        "* Max_tf_idf\n",
        "* Number of characters \n",
        "* Word count\n",
        "* Average word length\n",
        "* Vadar Sentiment analysis\n",
        "* Text subjectivity (Text blob)\n",
        "* Toxicity analysis (detoxify)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThPDLAkP5BnO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "stereo = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/stereo_features_f.csv',index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNUpQISc_oTM",
        "outputId": "de85cb61-89cb-4bf0-8ebc-4a232aee7c68"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>pos</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('be', 'VB...</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('associat...</td>\n",
              "      <td>[('arab', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>[('car', 'NN'), ('be', 'VB'), ('a', 'DT'), ('n...</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>[('car', 'NN'), ('new', 'JJ'), ('chemist', 'NN...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>[('the', 'DT'), ('gentleman', 'NNP'), ('stand'...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>[('gentleman', 'NNP'), ('stand', 'VB'), ('fire...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>[('the', 'DT'), ('common', 'JJ'), ('ukrainian'...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>[('common', 'JJ'), ('ukrainian', 'JJ'), ('supp...</td>\n",
              "      <td>[('ukrainian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>[('be', 'VB'), ('a', 'DT'), ('loop', 'NN'), ('...</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>[('loop', 'NNP'), ('nurse', 'NNP')]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                 Ner_tags\n",
              "0              1  ...    [('ethiopia', 'GPE')]\n",
              "1              1  ...   [('hispanic', 'NORP')]\n",
              "2              0  ...                       []\n",
              "3              1  ...   [('tomorrow', 'DATE')]\n",
              "4              1  ...    [('russian', 'NORP')]\n",
              "...          ...  ...                      ...\n",
              "16539          0  ...       [('arab', 'NORP')]\n",
              "16540          0  ...                       []\n",
              "16541          0  ...                       []\n",
              "16542          0  ...  [('ukrainian', 'NORP')]\n",
              "16543          0  ...                       []\n",
              "\n",
              "[16544 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCkH4BtYA8IJ"
      },
      "source": [
        "scoring_features = stereo.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y626UDQdBcIh"
      },
      "source": [
        "scoring_features.drop(['pos','lemma_pos',\t'Ner_tags'],axis=1, inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUh9oANOLEET"
      },
      "source": [
        " # Number of characters\n",
        " scoring_features['num_chars']  = scoring_features['sentence'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCQlfMI4DbUV"
      },
      "source": [
        "# Number of words\n",
        "def word_count(string):\n",
        "  # split the string into words\n",
        "  words = string.split()\n",
        "\n",
        "  # Return length of words list\n",
        "  return len(words)\n",
        "\n",
        "scoring_features['num_words'] = scoring_features['sentence'].apply(word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mlw4WTbEg_4"
      },
      "source": [
        "# Average word length\n",
        "def avg_word_length(x):\n",
        "\n",
        "  # Split the string into words\n",
        "  words = x.split()\n",
        "\n",
        "  # Compute length of each word and store in a seperate list\n",
        "  word_lengths = [len(word) for word in words]\n",
        "\n",
        "  # Compute average word length \n",
        "  try:\n",
        "    avg_word_length = sum(word_lengths)/len(words)\n",
        "  except ZeroDivisionError:\n",
        "    avg_word_length = 0\n",
        "\n",
        "  return (avg_word_length)\n",
        "\n",
        "scoring_features['avg_word_length'] = scoring_features['sentence'].apply(avg_word_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM3iGAN5BEk_",
        "outputId": "b86c94b3-a3e1-4ecb-8613-564a9c2ccfc9"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words',\n",
              "       'avg_word_length'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9u8OZMWN2pc",
        "outputId": "f4582edd-1dc4-4b58-ad10-e496ef1b187e"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6Q8dHuDkmC"
      },
      "source": [
        "# Readability tests using textatistic library \n",
        "# Import the textatistic class\n",
        "import textstat\n",
        "import math\n",
        "\n",
        "def readability_scores(text):\n",
        "  # if text.endswith(\".\") == False:\n",
        "  #   text = text+\".\"\n",
        "  readability_score = textstat.flesch_reading_ease(text)\n",
        "\n",
        "  # Generate scores\n",
        "  return readability_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epe4AJWNd_x5"
      },
      "source": [
        "try:\n",
        "  scoring_features['flesch_score'] = scoring_features['sentence'].apply(readability_scores)\n",
        "except ZeroDivisionError:\n",
        "  scoring_features['flesch_score'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMsORMv40p4",
        "outputId": "ceafc457-50e7-468a-86e8-5060f7efd78f"
      },
      "source": [
        "pip install -U textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301poqJoO_-v"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMoLpx3PCNm"
      },
      "source": [
        "scoring_features['subjectivity_score'] = scoring_features['sentence'].apply(get_subjectivity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXkX5Wj-7DHQ"
      },
      "source": [
        "Vectorization :\n",
        "\n",
        "* n_grams\n",
        "* tf_idf "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BYJRhKeqYb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Building n-gram models - capture context\n",
        "# Range = (2,2) - bi-grams, (1,3) - unigram, bigram, trigram\n",
        "def n_grams(range, corpus):\n",
        "  # Bag of words feature - docxterm matrix \n",
        "  vectorizer = CountVectorizer(ngram_range = range)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  bow_matrix = vectorizer.fit_transform(corpus)\n",
        "  cv_df = pd.DataFrame(bow_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('Counts_')\n",
        "  # corpus = pd.concat([corpus,cv_df],axis = 1, sort = False)\n",
        "  return cv_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooy_gIgDj9L_"
      },
      "source": [
        "# tf-idf  - higher the weight more the importance \n",
        "# Used for train set\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf(corpus):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer = TfidfVectorizer(max_features = 10000)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "  tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('tfIdf_')\n",
        "  # corpus = pd.concat([corpus,tfidf_df],axis = 1, sort = False)\n",
        "  return tfidf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtHzOYu0Xeeh"
      },
      "source": [
        "# Inspect the different words being values after BOW and tfidf transformation \n",
        "def examine_row(corpus,row_n):\n",
        "  examine_row = corpus.iloc[row_n]\n",
        "  print(examine_row.sort_values(ascending= False).head())\n",
        "  total = corpus.sum()\n",
        "  print(\"Total sum of the counts per word \\n\",total.head()) # Total sum of the counts per word\n",
        "  # print(\"Sums sorted: \",total.sort_values(ascending= False).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfQzuftERyBO"
      },
      "source": [
        "c = tf_idf(scoring_features['clean_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2SQ8Gt7nFe"
      },
      "source": [
        "Vadar sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glSbhCUtHv6Y",
        "outputId": "fd23c4de-b071-4e0d-a0d6-92da131a0c82"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMk4mWO-GNJd"
      },
      "source": [
        "# Sentiment analysis \n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "  score = analyser.polarity_scores(text)\n",
        "  return score\n",
        "\n",
        "senti = scoring_features['sentence'].apply(vader_sentiment) \n",
        "scoring_features = pd.concat([scoring_features,(pd.DataFrame.from_dict(dict(senti).values()))],axis = 1, sort = False)\n",
        "# scoring_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cdGA_fdPrnl",
        "outputId": "e5a30083-2e77-44ef-c933-489fd7ed05cb"
      },
      "source": [
        "pip install detoxify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting detoxify\n",
            "  Downloading detoxify-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from detoxify) (1.9.0+cu102)\n",
            "Collecting transformers>=3.2.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.94\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->detoxify) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.6.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.2.0->detoxify) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.2.0->detoxify) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece, detoxify\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed detoxify-0.2.2 huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9wJjDROmCFE"
      },
      "source": [
        "# Toxicity identification \n",
        "from detoxify import Detoxify\n",
        "\n",
        "def toxicity(text):\n",
        "  results = Detoxify('original').predict(text)\n",
        "  return math.floor(results['toxicity']*100)\n",
        "\n",
        "scoring_features['toxicity'] = scoring_features['sentence'].apply(toxicity) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWlYP0nfjkS"
      },
      "source": [
        "Count based features :\n",
        "\n",
        "* Lexicons - Counts\n",
        "  * Hedge in context  - two words around W is a hedge (Hyland, 2005  (e.g., apparently).\n",
        "  * Factive verb  - w is in Hooper’s (1975) list of factives (e.g., realize).\n",
        "  * Factive verb in context One/two word(s) around w is a factive (Hooper, 1975)\n",
        "  * Assertive verb\n",
        "  * Assertive verb in context \n",
        "  * Assertive verb \n",
        "  * Implicative verb in context\n",
        "  * Report verb\n",
        "  * Entailment\n",
        "  * Entailment in context\n",
        "  * Strong subjective\n",
        "  * Weak subjective\n",
        "  * Positive word\n",
        "  * Positive word in context\n",
        "  * Negative word\n",
        "  * Negative word in context\n",
        "  * Grammatical relation - {root,subj,...}\n",
        "  * Bias lexicon\n",
        "* Social category target words used in dataset( Characteristic words of each bias type ; e.g. Racial, gender, ..) and scoring_features_pos_Ner\n",
        "* Characteristic stereotypical words \n",
        "* POS :\n",
        "  * POS(word) : POS of word w \n",
        "  * POS(word) - 1 :  POS of one word before w\n",
        "  * POS(word) + 1  : POS of one word after w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDzoShaZCwyf"
      },
      "source": [
        "Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73L9TfItiCGm"
      },
      "source": [
        "import json \n",
        "\n",
        "f = open('/content/Subjectivity_lexicon.json')\n",
        "\n",
        "lexicons = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23EIWFFKDFsV",
        "outputId": "6d0aa71f-7fbb-4508-c796-45dfcd1583cf"
      },
      "source": [
        "for keys, value in lexicons.items():\n",
        "  print(keys,'->',len(value.split('\\n')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assertive_verbs.txt -> 66\n",
            "bias_lexicon.txt -> 655\n",
            "bias_word_list_01_2018.txt -> 9742\n",
            "factive_verbs.txt -> 27\n",
            "hedges_hyland2005.txt -> 100\n",
            "implicative_verbs.txt -> 32\n",
            "report_verbs.txt -> 181\n",
            "subjectivityClues_lexicon.txt -> 8223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60-UZajjQkax"
      },
      "source": [
        "keys = lexicons.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezD61ECeSWV9"
      },
      "source": [
        "def count_lexicon(text):\n",
        "  count = 0\n",
        "  try:\n",
        "    for token in lexicon:\n",
        "      if token in text:\n",
        "        count +=1\n",
        "      else:\n",
        "        continue\n",
        "  except :\n",
        "    pass\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBR7PwONVRf1"
      },
      "source": [
        "Assertive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN25i1bnUTDN"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['assertive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYYKQBjOTLnm"
      },
      "source": [
        "scoring_features['assertive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkprUj7MUzSd",
        "outputId": "6ea56dc8-970d-4681-82bc-d355fd43719b"
      },
      "source": [
        "len(scoring_features[scoring_features['assertive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1520"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FobCWZgmVT7m"
      },
      "source": [
        "Factive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_gT5vXxVV3s"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['factive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt25V7lyVi3l"
      },
      "source": [
        "scoring_features['factive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re3qIRl0VoQf",
        "outputId": "e87076e1-a045-44d4-8de6-909dd624efde"
      },
      "source": [
        "len(scoring_features[scoring_features['factive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0tKxtsVs2P"
      },
      "source": [
        "Hedges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zddbek6r2olj"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['hedges_hyland2005.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYhKxXiB2oll"
      },
      "source": [
        "scoring_features['hedges_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_BWsO39WOdq",
        "outputId": "15a5d0ea-a33f-489b-a501-ad51db97d503"
      },
      "source": [
        "len(scoring_features[scoring_features['hedges_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9331"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQhHEIOHWUCv"
      },
      "source": [
        "Implicative_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LyexouWcIZ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['implicative_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_91iyEMWcIa"
      },
      "source": [
        "scoring_features['implicative_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1s1VkXeWcIa",
        "outputId": "4df850e0-3454-43af-dd85-e6c1c5570482"
      },
      "source": [
        "len(scoring_features[scoring_features['implicative_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaZFxBqmWxzH"
      },
      "source": [
        "Report_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIZFnmSr2r38"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['report_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4raWev2r39"
      },
      "source": [
        "scoring_features['report_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br2WoxJ4XEoX",
        "outputId": "1fea672a-1d9a-4b52-d9cc-62b5c4d07fb9"
      },
      "source": [
        "len(scoring_features[scoring_features['report_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3884"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG5AjibnX0Hj"
      },
      "source": [
        "Bias_word_list_01_2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD5TGaZJX9FK"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['bias_word_list_01_2018.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9-db2slX9FL"
      },
      "source": [
        "scoring_features['bias_word_list_01_2018_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaTmsLjVCmZz",
        "outputId": "34f95dfe-0ef5-4437-d2d5-84ffdec290d8"
      },
      "source": [
        "len(scoring_features[scoring_features['bias_word_list_01_2018_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNNiL2I8YyMF"
      },
      "source": [
        "SubjectivityClues_lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftr3jnXMDNaJ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['subjectivityClues_lexicon.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DCIrvw8ZQUO"
      },
      "source": [
        "scoring_features['subjectivityClues_lexicon_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDbN64vcZQUP",
        "outputId": "17543388-1e50-4b16-ac07-ceddc04de5cd"
      },
      "source": [
        "len(scoring_features[scoring_features['subjectivityClues_lexicon_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwb0LrSnp7eq"
      },
      "source": [
        "scoring_features.to_csv(\"scoring_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGjdCLJSZfgH"
      },
      "source": [
        "POS tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQHr9m_v7wI7"
      },
      "source": [
        "scoring_features = pd. read_csv(\"/content/drive/MyDrive/Trained models/mult_label_dataset/features_with_pos.csv\", index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33B3QF-i-PYb",
        "outputId": "5bb517cb-f4b5-4423-ea2a-d890bc57d3cc"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words', 'avg_word_length',\n",
              "       'flesch_score', 'subjectivity_score', 'neg', 'neu', 'pos',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'pos_tags', 'lemma_pos', 'Ner_tags'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psffIJ9GKxL2",
        "outputId": "a3907c78-0f23-4411-a4ef-0aaf397efb8b"
      },
      "source": [
        "scoring_features.pos_tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [('many', 'JJ'), ('people', 'NNS'), ('live', '...\n",
              "1        [('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...\n",
              "2        [('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...\n",
              "3        [('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...\n",
              "4        [('know', 'VBP'), ('many', 'JJ'), ('people', '...\n",
              "                               ...                        \n",
              "16539    [('arab', 'JJ'), ('people', 'NNS'), ('be', 'VB...\n",
              "16540    [('car', 'NN'), ('be', 'VB'), ('a', 'DT'), ('n...\n",
              "16541    [('the', 'DT'), ('gentleman', 'NNP'), ('stand'...\n",
              "16542    [('the', 'DT'), ('common', 'JJ'), ('ukrainian'...\n",
              "16543    [('be', 'VB'), ('a', 'DT'), ('loop', 'NN'), ('...\n",
              "Name: pos_tags, Length: 16544, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzrj1vtNYskO",
        "outputId": "6c8a839d-e8ee-439e-8df3-c7fd5bc3ee7c"
      },
      "source": [
        "import ast\n",
        "\n",
        "for word, tag in ast.literal_eval(scoring_features.pos_tags[0]):\n",
        "  print(word , \"->\", tag)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "many -> JJ\n",
            "people -> NNS\n",
            "live -> VBP\n",
            "in -> IN\n",
            "ethiopia -> NNP\n",
            "the -> DT\n",
            "people -> NNS\n",
            "be -> VB\n",
            "very -> RB\n",
            "thin -> JJ\n",
            "and -> CC\n",
            "good -> JJ\n",
            "at -> IN\n",
            "distance -> NN\n",
            "run -> NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scx4O_0SJ1W2"
      },
      "source": [
        "for index, tuple in enumerate(scoring_features.pos_tags[0]):\n",
        "  print(index, tuple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607bK0m3aw9Y"
      },
      "source": [
        "import ast\n",
        "\n",
        "def pos_count(text,part_of_speech = 'NNS'):\n",
        "  pos_type = []\n",
        "  # Combining lists of lists into single list \n",
        "  pos_list = ast.literal_eval(text)\n",
        "  for word,tag in pos_list :\n",
        "    if tag == part_of_speech:\n",
        "      pos_type.append(word)\n",
        "  return len(pos_type)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWwt8P9seiBw"
      },
      "source": [
        "scoring_features['NNS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvReISAouQMD"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxwBVsaUgMF"
      },
      "source": [
        "MAX_LEN = 50\n",
        "RANDOM_SEED = 47"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWJjN78DTbTt"
      },
      "source": [
        "y = df.iloc[:,:-1].values\n",
        "X = df.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usepriRO9yZ"
      },
      "source": [
        "LABEL_COLUMN = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFD-634ULmH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTpY3Vy3O7Hg"
      },
      "source": [
        "train_df_labels = pd.DataFrame(train_df_labels, columns= LABEL_COLUMN)\n",
        "val_df_labels = pd.DataFrame(val_df_labels, columns= LABEL_COLUMN)\n",
        "test_df_labels = pd.DataFrame(test_df_labels, columns= LABEL_COLUMN)\n",
        "train_df_text = pd.DataFrame(train_df_text, columns = ['sentence'])\n",
        "val_df_text = pd.DataFrame(val_df_text, columns = ['sentence'])\n",
        "test_df_text = pd.DataFrame(test_df_text, columns = ['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvuTsDvhiP8Q"
      },
      "source": [
        "train_df = pd.concat([train_df_text,train_df_labels], axis = 1)\n",
        "val_df = pd.concat([val_df_text,val_df_labels], axis = 1)\n",
        "test_df = pd.concat([test_df_text,test_df_labels], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}