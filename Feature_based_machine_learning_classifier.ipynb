{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature based machine learning classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Machine-learning-classifiers/Feature_based_machine_learning_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70W14T_1bh5Q"
      },
      "source": [
        "# Features based machine learning models \n",
        "\n",
        "* Reference : \n",
        "    1. Linguistic models for detecting bias https://aclanthology.org/P13-1162.pdf\n",
        "    2. Automatically Neutralizing Subjective Bias in Text https://ojs.aaai.org/index.php/AAAI/article/view/5385 \n",
        "\n",
        "Features :\n",
        "\n",
        "* Bias lexicons with count\n",
        "* Sentiment \n",
        "* Generic words NNS and NNPS\n",
        "* Toxicity \n",
        "* Generic features \n",
        "* A, AE names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBHu71wHmqOh",
        "outputId": "8818830f-ce5d-40fe-ad94-e638dbbf66ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8adqvI5gTMnE"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/ohe_multilabel.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOgFp_bmtsb4"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkquqUZFpljy"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMuqYP-ZRYbu"
      },
      "source": [
        "# Tokenization using spacy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize(text):  \n",
        "\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.text.lower() for token in doc]\n",
        "  return tokens\n",
        "\n",
        "def lemmatization(text):\n",
        "\n",
        "  doc = nlp(text)\n",
        "  lemmas = [token.lemma_.lower() for token in doc]\n",
        "  return lemmas\n",
        "\n",
        "# Remove tokens that are not alphabetic - depends on particular application \n",
        "def clean_text(text):\n",
        "\n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha()]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Remove stopwords - Update according to stereotypical bias \n",
        "def remove_stopwords(text):\n",
        "\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  \n",
        "  lemmas = lemmatization(text)\n",
        "  a_lemmas = [lemma for  lemma in lemmas\n",
        "              if lemma.isalpha() and lemma not in stopwords]\n",
        "  \n",
        "  return (' '.join(a_lemmas))\n",
        "\n",
        "\n",
        "# Parts of speech tagger \n",
        "def pos_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  pos = [(token.text, token.tag_) for token in doc]\n",
        "  return (pos)\n",
        "\n",
        "\n",
        "# Named entity recognition \n",
        "def ner_tags(text):\n",
        "  \n",
        "  doc = nlp(text)\n",
        "  ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "  return (ner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLEyY7tbcdma"
      },
      "source": [
        "stereo = df.copy()\n",
        "stereo['clean_text'] = stereo['sentence'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHW4kXnRIG5"
      },
      "source": [
        "stereo['pos_tags'] = stereo['clean_text'].apply(pos_tags) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apo1CmWI09hD"
      },
      "source": [
        "stereo['lemmatized_withStopwords']= stereo['clean_text'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQDcBY8oHuD0"
      },
      "source": [
        "stereo['tokenized_lemmatized_withstopwords'] = stereo['lemmatized_withStopwords'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a-IkJemDEH3"
      },
      "source": [
        "stereo['lemma_pos'] = stereo['lemmatized_withStopwords'].apply(pos_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db1k-ezbGwVz"
      },
      "source": [
        "stereo['Ner_tags'] = stereo['clean_text'].apply(ner_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "UtJYQVnpc__i",
        "outputId": "923f716c-27cf-407a-d722-2e5cfa62a7e8"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>tokenized_lemmatized_withstopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>[people, live, ethiopia, people, thin, good, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>[professor, hispanic, man, come, illegally]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>[schoolgirl, walk, street, target, child, pred...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>[likely, fly, cape, verde, flight, tomorrow, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>[know, people, russian, love, vodka]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>[arab, people, associate, hi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>[car, new, chemist, lab]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>[gentleman, stand, fire, carefully, pass, boxing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>[common, ukrainian, supporter, flounder, run, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>[loop, nurse]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                 tokenized_lemmatized_withstopwords\n",
              "0              1  ...  [people, live, ethiopia, people, thin, good, d...\n",
              "1              1  ...        [professor, hispanic, man, come, illegally]\n",
              "2              0  ...  [schoolgirl, walk, street, target, child, pred...\n",
              "3              1  ...  [likely, fly, cape, verde, flight, tomorrow, b...\n",
              "4              1  ...               [know, people, russian, love, vodka]\n",
              "...          ...  ...                                                ...\n",
              "16539          0  ...                      [arab, people, associate, hi]\n",
              "16540          0  ...                           [car, new, chemist, lab]\n",
              "16541          0  ...  [gentleman, stand, fire, carefully, pass, boxing]\n",
              "16542          0  ...  [common, ukrainian, supporter, flounder, run, ...\n",
              "16543          0  ...                                      [loop, nurse]\n",
              "\n",
              "[16544 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMuaAq7y3ts7"
      },
      "source": [
        "stereo.to_csv('stereo_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPzY1p7prTT"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEfFbrX7d-9q"
      },
      "source": [
        "Scoring features :\n",
        "\n",
        "\n",
        "* Readability tests :\n",
        "  https://pypi.org/project/textatistic/\n",
        "  * Determine readability of english passage\n",
        "  * Scale ranging from primary school up to college graduate level\n",
        "  * A mathematical formula utilizing word, syllabel and sentence count\n",
        "  * Used in fake news and opinion spam detection \n",
        "\n",
        "  Types :\n",
        "\n",
        "  1. Flesch reading ease : \n",
        "\n",
        "    * The higher the score, the better the readability. \n",
        "    * score of 0-30 implies only college graduates can understand while 90-100 implies that a 5th grade student can understand.\n",
        "    \n",
        "    Two factors :\n",
        "\n",
        "      1. Greater the average sentence length, harder the text to read\n",
        "      2. Greater the average number of syllables, harder the text to read\n",
        "\n",
        "* Avg_tf_idf\n",
        "* Max_tf_idf\n",
        "* Number of characters \n",
        "* Word count\n",
        "* Average word length\n",
        "* Vadar Sentiment analysis\n",
        "* Text subjectivity (Text blob)\n",
        "* Toxicity analysis (detoxify)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThPDLAkP5BnO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "stereo = pd.read_csv('/content/drive/MyDrive/Trained models/mult_label_dataset/stereo_features_f.csv',index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNUpQISc_oTM",
        "outputId": "de85cb61-89cb-4bf0-8ebc-4a232aee7c68"
      },
      "source": [
        "stereo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>pos</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16539</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Arab people are associated with hi.</td>\n",
              "      <td>arab people be associate with hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('be', 'VB...</td>\n",
              "      <td>arab people associate hi</td>\n",
              "      <td>[('arab', 'JJ'), ('people', 'NNS'), ('associat...</td>\n",
              "      <td>[('arab', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16540</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Car was a new chemist in our lab.</td>\n",
              "      <td>car be a new chemist in lab</td>\n",
              "      <td>[('car', 'NN'), ('be', 'VB'), ('a', 'DT'), ('n...</td>\n",
              "      <td>car new chemist lab</td>\n",
              "      <td>[('car', 'NN'), ('new', 'JJ'), ('chemist', 'NN...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16541</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The gentlemen stood around the fire, carefully...</td>\n",
              "      <td>the gentleman stand around the fire carefully ...</td>\n",
              "      <td>[('the', 'DT'), ('gentleman', 'NNP'), ('stand'...</td>\n",
              "      <td>gentleman stand fire carefully pass boxing</td>\n",
              "      <td>[('gentleman', 'NNP'), ('stand', 'VB'), ('fire...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16542</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The common ukrainian is a supporter of a floun...</td>\n",
              "      <td>the common ukrainian be a supporter of a floun...</td>\n",
              "      <td>[('the', 'DT'), ('common', 'JJ'), ('ukrainian'...</td>\n",
              "      <td>common ukrainian supporter flounder run govern...</td>\n",
              "      <td>[('common', 'JJ'), ('ukrainian', 'JJ'), ('supp...</td>\n",
              "      <td>[('ukrainian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16543</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I am a loop nurse.</td>\n",
              "      <td>be a loop nurse</td>\n",
              "      <td>[('be', 'VB'), ('a', 'DT'), ('loop', 'NN'), ('...</td>\n",
              "      <td>loop nurse</td>\n",
              "      <td>[('loop', 'NNP'), ('nurse', 'NNP')]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16544 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                 Ner_tags\n",
              "0              1  ...    [('ethiopia', 'GPE')]\n",
              "1              1  ...   [('hispanic', 'NORP')]\n",
              "2              0  ...                       []\n",
              "3              1  ...   [('tomorrow', 'DATE')]\n",
              "4              1  ...    [('russian', 'NORP')]\n",
              "...          ...  ...                      ...\n",
              "16539          0  ...       [('arab', 'NORP')]\n",
              "16540          0  ...                       []\n",
              "16541          0  ...                       []\n",
              "16542          0  ...  [('ukrainian', 'NORP')]\n",
              "16543          0  ...                       []\n",
              "\n",
              "[16544 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCkH4BtYA8IJ"
      },
      "source": [
        "scoring_features = stereo.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y626UDQdBcIh"
      },
      "source": [
        "scoring_features.drop(['pos','lemma_pos',\t'Ner_tags'],axis=1, inplace= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUh9oANOLEET"
      },
      "source": [
        " # Number of characters\n",
        " scoring_features['num_chars']  = scoring_features['sentence'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCQlfMI4DbUV"
      },
      "source": [
        "# Number of words\n",
        "def word_count(string):\n",
        "  # split the string into words\n",
        "  words = string.split()\n",
        "\n",
        "  # Return length of words list\n",
        "  return len(words)\n",
        "\n",
        "scoring_features['num_words'] = scoring_features['sentence'].apply(word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mlw4WTbEg_4"
      },
      "source": [
        "# Average word length\n",
        "def avg_word_length(x):\n",
        "\n",
        "  # Split the string into words\n",
        "  words = x.split()\n",
        "\n",
        "  # Compute length of each word and store in a seperate list\n",
        "  word_lengths = [len(word) for word in words]\n",
        "\n",
        "  # Compute average word length \n",
        "  try:\n",
        "    avg_word_length = sum(word_lengths)/len(words)\n",
        "  except ZeroDivisionError:\n",
        "    avg_word_length = 0\n",
        "\n",
        "  return (avg_word_length)\n",
        "\n",
        "scoring_features['avg_word_length'] = scoring_features['sentence'].apply(avg_word_length) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM3iGAN5BEk_",
        "outputId": "b86c94b3-a3e1-4ecb-8613-564a9c2ccfc9"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'num_chars', 'num_words',\n",
              "       'avg_word_length'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9u8OZMWN2pc",
        "outputId": "f4582edd-1dc4-4b58-ad10-e496ef1b187e"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat) (0.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6Q8dHuDkmC"
      },
      "source": [
        "# Readability tests using textatistic library \n",
        "# Import the textatistic class\n",
        "import textstat\n",
        "import math\n",
        "\n",
        "def readability_scores(text):\n",
        "  # if text.endswith(\".\") == False:\n",
        "  #   text = text+\".\"\n",
        "  readability_score = textstat.flesch_reading_ease(text)\n",
        "\n",
        "  # Generate scores\n",
        "  return readability_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epe4AJWNd_x5"
      },
      "source": [
        "try:\n",
        "  scoring_features['flesch_score'] = scoring_features['sentence'].apply(readability_scores)\n",
        "except ZeroDivisionError:\n",
        "  scoring_features['flesch_score'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDMsORMv40p4",
        "outputId": "ceafc457-50e7-468a-86e8-5060f7efd78f"
      },
      "source": [
        "pip install -U textblob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301poqJoO_-v"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMoLpx3PCNm"
      },
      "source": [
        "scoring_features['subjectivity_score'] = scoring_features['sentence'].apply(get_subjectivity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXkX5Wj-7DHQ"
      },
      "source": [
        "Vectorization :\n",
        "\n",
        "* n_grams\n",
        "* tf_idf "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BYJRhKeqYb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Building n-gram models - capture context\n",
        "# Range = (2,2) - bi-grams, (1,3) - unigram, bigram, trigram\n",
        "def n_grams(range, corpus):\n",
        "  # Bag of words feature - docxterm matrix \n",
        "  vectorizer = CountVectorizer(ngram_range = range)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  bow_matrix = vectorizer.fit_transform(corpus)\n",
        "  cv_df = pd.DataFrame(bow_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('Counts_')\n",
        "  # corpus = pd.concat([corpus,cv_df],axis = 1, sort = False)\n",
        "  return cv_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ooy_gIgDj9L_"
      },
      "source": [
        "# tf-idf  - higher the weight more the importance \n",
        "# Used for train set\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tf_idf(corpus):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectorizer = TfidfVectorizer(max_features = 10000)\n",
        "  corpus = corpus.values.astype('U')\n",
        "  tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "  tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names()).add_prefix('tfIdf_')\n",
        "  # corpus = pd.concat([corpus,tfidf_df],axis = 1, sort = False)\n",
        "  return tfidf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6-pUhgJJlR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0662ea1-2cb7-4ab5-a002-a2d6f6c1e045"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text', 'pos_tags',\n",
              "       'lemmatized_withStopwords', 'tokenized_lemmatized_withstopwords',\n",
              "       'lemma_pos', 'Ner_tags'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfQzuftERyBO"
      },
      "source": [
        "tf_idf_feature = tf_idf(df['tokenized_lemmatized_withstopwords'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtHzOYu0Xeeh"
      },
      "source": [
        "# Inspect the different words being values after BOW and tfidf transformation \n",
        "def examine_row(corpus,row_n):\n",
        "  examine_row = corpus.iloc[row_n]\n",
        "  print(examine_row.sort_values(ascending= False).head())\n",
        "  total = corpus.sum()\n",
        "  print(\"Total sum of the counts per word \\n\",total.head()) # Total sum of the counts per word\n",
        "  # print(\"Sums sorted: \",total.sort_values(ascending= False).head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R8zAY_KMueW"
      },
      "source": [
        "clean_text = scoring_features['clean_text']\n",
        "tfidf = TfidfVectorizer()\n",
        "# corpus = clean_text.values.astype('U')\n",
        "# Whole dataset has to  be given for tfidf model\n",
        "tfidf_model = tfidf.fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GIax2gJP5Wv",
        "outputId": "9913f977-16e0-4272-96d5-90e20dfeeee3"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKikIdXiMSdT"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "def tfidf_max_avg_features(sent):\n",
        "    avg_tfidf_feature = 0\n",
        "    max_tfidf_feature = 0\n",
        "    tokenized_words = nltk.word_tokenize(sent)\n",
        "    tfidf_vector = tfidf_model.transform([sent])\n",
        "    if avg == True:\n",
        "      avg_tfidf_feature = np.sum(tfidf_vector.toarray())/len(tokenized_words)\n",
        "      return avg_tfidf_feature\n",
        "    else:\n",
        "      max_tfidf_feature = np.max(tfidf_vector.toarray())\n",
        "      return max_tfidf_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUN4b8uiOMXC"
      },
      "source": [
        "avg = True\n",
        "scoring_features['avg_tfidf_feature'] = scoring_features['sentence'].apply(tfidf_max_avg_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vd19NZIPzks"
      },
      "source": [
        "avg = False\n",
        "scoring_features['max_tfidf_feature'] = scoring_features['sentence'].apply(tfidf_max_avg_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x2SQ8Gt7nFe"
      },
      "source": [
        "Vadar sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glSbhCUtHv6Y",
        "outputId": "fd23c4de-b071-4e0d-a0d6-92da131a0c82"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMk4mWO-GNJd"
      },
      "source": [
        "# Sentiment analysis \n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "  score = analyser.polarity_scores(text)\n",
        "  return score\n",
        "\n",
        "senti = scoring_features['sentence'].apply(vader_sentiment) \n",
        "scoring_features = pd.concat([scoring_features,(pd.DataFrame.from_dict(dict(senti).values()))],axis = 1, sort = False)\n",
        "# scoring_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cdGA_fdPrnl",
        "outputId": "e5a30083-2e77-44ef-c933-489fd7ed05cb"
      },
      "source": [
        "pip install detoxify"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting detoxify\n",
            "  Downloading detoxify-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from detoxify) (1.9.0+cu102)\n",
            "Collecting transformers>=3.2.0\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.94\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->detoxify) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (4.6.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 71.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.2.0->detoxify) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.2.0->detoxify) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.2.0->detoxify) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.2.0->detoxify) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.2.0->detoxify) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers, sentencepiece, detoxify\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed detoxify-0.2.2 huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9wJjDROmCFE"
      },
      "source": [
        "# Toxicity identification \n",
        "from detoxify import Detoxify\n",
        "\n",
        "def toxicity(text):\n",
        "  results = Detoxify('original').predict(text)\n",
        "  return math.floor(results['toxicity']*100)\n",
        "\n",
        "scoring_features['toxicity'] = scoring_features['sentence'].apply(toxicity) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO9Dl_Z1UcEs"
      },
      "source": [
        "scoring_features.to_csv('/content/drive/MyDrive/temp_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWlYP0nfjkS"
      },
      "source": [
        "Count based features :\n",
        "\n",
        "* Lexicons - Counts\n",
        "  * Hedge in context  - two words around W is a hedge (Hyland, 2005  (e.g., apparently).\n",
        "  * Factive verb  - w is in Hooper’s (1975) list of factives (e.g., realize).\n",
        "  * Factive verb in context One/two word(s) around w is a factive (Hooper, 1975)\n",
        "  * Assertive verb\n",
        "  * Assertive verb in context \n",
        "  * Assertive verb \n",
        "  * Implicative verb in context\n",
        "  * Report verb\n",
        "  * Entailment (Not found)\n",
        "  * Entailment in context (Not found)\n",
        "  * Strong subjective (Used textblob subjectivity score)\n",
        "  * Weak subjective ((Used textblob subjectivity score)\n",
        "  * Positive word (Vadar sentiment score)\n",
        "  * Positive word in context (Vadar sentiment score)\n",
        "  * Negative word (Vadar sentiment score)\n",
        "  * Negative word in context (Vadar sentiment score)\n",
        "  * Grammatical relation - {root,subj,...}\n",
        "  * Bias lexicon\n",
        "* Social category target words used in dataset( Characteristic words of each bias type ; e.g. Racial, gender, ..) and scoring_features_pos_Ner\n",
        "* Characteristic stereotypical words \n",
        "* POS :\n",
        "  * POS(word) : POS of word w \n",
        "  * POS(word) - 1 :  POS of one word before w\n",
        "  * POS(word) + 1  : POS of one word after w\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15n4vrgKLvPS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "scoring_features = pd.read_csv('/content/drive/MyDrive/temp_df.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDzoShaZCwyf"
      },
      "source": [
        "Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73L9TfItiCGm"
      },
      "source": [
        "import json \n",
        "\n",
        "f = open('/content/Subjectivity_lexicon.json')\n",
        "\n",
        "lexicons = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23EIWFFKDFsV",
        "outputId": "1831a426-a44b-4e4a-a9ff-0be424baba89"
      },
      "source": [
        "for keys, value in lexicons.items():\n",
        "  print(keys,'->',len(value.split('\\n')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assertive_verbs.txt -> 66\n",
            "bias_lexicon.txt -> 655\n",
            "bias_word_list_01_2018.txt -> 9742\n",
            "factive_verbs.txt -> 27\n",
            "hedges_hyland2005.txt -> 100\n",
            "implicative_verbs.txt -> 32\n",
            "report_verbs.txt -> 181\n",
            "subjectivityClues_lexicon.txt -> 8223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60-UZajjQkax"
      },
      "source": [
        "keys = lexicons.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezD61ECeSWV9"
      },
      "source": [
        "def count_lexicon(text):\n",
        "  count = 0\n",
        "  try:\n",
        "    for token in lexicon:\n",
        "      if token in text:\n",
        "        count +=1\n",
        "      else:\n",
        "        continue\n",
        "  except :\n",
        "    pass\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBR7PwONVRf1"
      },
      "source": [
        "Assertive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN25i1bnUTDN"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['assertive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYYKQBjOTLnm"
      },
      "source": [
        "scoring_features['assertive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkprUj7MUzSd",
        "outputId": "eef8f0e0-7848-47ce-edf6-6e03a76a3480"
      },
      "source": [
        "len(scoring_features[scoring_features['assertive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1572"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FobCWZgmVT7m"
      },
      "source": [
        "Factive verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_gT5vXxVV3s"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['factive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt25V7lyVi3l"
      },
      "source": [
        "scoring_features['factive_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re3qIRl0VoQf",
        "outputId": "5c1e8606-61d2-42d5-fb30-25a28c7c015b"
      },
      "source": [
        "len(scoring_features[scoring_features['factive_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1725"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0tKxtsVs2P"
      },
      "source": [
        "Hedges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zddbek6r2olj"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['hedges_hyland2005.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYhKxXiB2oll"
      },
      "source": [
        "scoring_features['hedges_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_BWsO39WOdq",
        "outputId": "ab563454-c659-4177-e9a8-402a325a2bdb"
      },
      "source": [
        "len(scoring_features[scoring_features['hedges_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9458"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQhHEIOHWUCv"
      },
      "source": [
        "Implicative_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LyexouWcIZ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['implicative_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_91iyEMWcIa"
      },
      "source": [
        "scoring_features['implicative_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1s1VkXeWcIa",
        "outputId": "8f2a8e9b-b4ff-4c0b-f0f6-94ae326519e9"
      },
      "source": [
        "len(scoring_features[scoring_features['implicative_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1719"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaZFxBqmWxzH"
      },
      "source": [
        "Report_verbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIZFnmSr2r38"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['report_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO4raWev2r39"
      },
      "source": [
        "scoring_features['report_verbs_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br2WoxJ4XEoX",
        "outputId": "995e5463-dac6-4c79-e8e5-ede812a8d438"
      },
      "source": [
        "len(scoring_features[scoring_features['report_verbs_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3922"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG5AjibnX0Hj"
      },
      "source": [
        "Bias_word_list_01_2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD5TGaZJX9FK"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['bias_word_list_01_2018.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9-db2slX9FL"
      },
      "source": [
        "scoring_features['bias_word_list_01_2018_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaTmsLjVCmZz",
        "outputId": "9b2412dc-8884-495a-9be0-08dcb978235e"
      },
      "source": [
        "len(scoring_features[scoring_features['bias_word_list_01_2018_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15198"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNNiL2I8YyMF"
      },
      "source": [
        "SubjectivityClues_lexicon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftr3jnXMDNaJ"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['subjectivityClues_lexicon.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DCIrvw8ZQUO"
      },
      "source": [
        "scoring_features['subjectivityClues_lexicon_count'] = scoring_features['lemmatized_withStopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDbN64vcZQUP",
        "outputId": "f2580dfe-1311-480b-dce3-9dde6066e93d"
      },
      "source": [
        "len(scoring_features[scoring_features['subjectivityClues_lexicon_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15831"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwb0LrSnp7eq"
      },
      "source": [
        "scoring_features.to_csv(\"scoring_features.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGjdCLJSZfgH"
      },
      "source": [
        "POS tags Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4w9Z0L6WCtc"
      },
      "source": [
        "features_pos_ner = pd.read_csv('/content/drive/MyDrive/Trained_models/Results_compilation /Unrelated_adjusted/Baselines/Feature based/Feature_df/stereo_features_pos_ner.csv',index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pskCRNI2WdM-"
      },
      "source": [
        "features_pos_ner = features_pos_ner.iloc[:,9:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "o5QD913_WmtF",
        "outputId": "7263b92c-a31e-4e7f-d09d-0110eee55916"
      },
      "source": [
        "features_pos_ner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>tokenized_lemmatized_withstopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>['people', 'live', 'ethiopia', 'people', 'thin...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>['professor', 'hispanic', 'man', 'come', 'ille...</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>['schoolgirl', 'walk', 'street', 'target', 'ch...</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>['likely', 'fly', 'cape', 'verde', 'flight', '...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>['know', 'people', 'russian', 'love', 'vodka']</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16555</th>\n",
              "      <td>[('cookie', 'NN'), ('be', 'VB'), ('good', 'JJ'...</td>\n",
              "      <td>cookie good substitute liquid milk</td>\n",
              "      <td>['cookie', 'good', 'substitute', 'liquid', 'mi...</td>\n",
              "      <td>[('cookie', 'NNP'), ('good', 'JJ'), ('substitu...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16556</th>\n",
              "      <td>[('jollof', 'NNP'), ('rice', 'NNP'), ('cereal'...</td>\n",
              "      <td>jollof rice cereal totally jambalaya</td>\n",
              "      <td>['jollof', 'rice', 'cereal', 'totally', 'jamba...</td>\n",
              "      <td>[('jollof', 'NNP'), ('rice', 'NNP'), ('cereal'...</td>\n",
              "      <td>[('jollof rice', 'PERSON')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16557</th>\n",
              "      <td>[('bike', 'NN'), ('out', 'RP'), ('be', 'VB'), ...</td>\n",
              "      <td>bike economical energy efficient mode unk tran...</td>\n",
              "      <td>['bike', 'economical', 'energy', 'efficient', ...</td>\n",
              "      <td>[('bike', 'NNP'), ('economical', 'JJ'), ('ener...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16558</th>\n",
              "      <td>[('may', 'MD'), ('see', 'VB'), ('the', 'DT'), ...</td>\n",
              "      <td>little dog need food park grocery store find f...</td>\n",
              "      <td>['little', 'dog', 'need', 'food', 'park', 'gro...</td>\n",
              "      <td>[('little', 'JJ'), ('dog', 'NN'), ('need', 'VB...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16559</th>\n",
              "      <td>[('go', 'VB'), ('to', 'IN'), ('the', 'DT'), ('...</td>\n",
              "      <td>california beach morning cool</td>\n",
              "      <td>['california', 'beach', 'morning', 'cool']</td>\n",
              "      <td>[('california', 'NNP'), ('beach', 'NNP'), ('mo...</td>\n",
              "      <td>[('california', 'GPE')]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16560 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                pos_tags  ...                     Ner_tags\n",
              "0      [('many', 'JJ'), ('people', 'NNS'), ('live', '...  ...        [('ethiopia', 'GPE')]\n",
              "1      [('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...  ...       [('hispanic', 'NORP')]\n",
              "2      [('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...  ...                           []\n",
              "3      [('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...  ...       [('tomorrow', 'DATE')]\n",
              "4      [('know', 'VBP'), ('many', 'JJ'), ('people', '...  ...        [('russian', 'NORP')]\n",
              "...                                                  ...  ...                          ...\n",
              "16555  [('cookie', 'NN'), ('be', 'VB'), ('good', 'JJ'...  ...                           []\n",
              "16556  [('jollof', 'NNP'), ('rice', 'NNP'), ('cereal'...  ...  [('jollof rice', 'PERSON')]\n",
              "16557  [('bike', 'NN'), ('out', 'RP'), ('be', 'VB'), ...  ...                           []\n",
              "16558  [('may', 'MD'), ('see', 'VB'), ('the', 'DT'), ...  ...                           []\n",
              "16559  [('go', 'VB'), ('to', 'IN'), ('the', 'DT'), ('...  ...      [('california', 'GPE')]\n",
              "\n",
              "[16560 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8xH6IMKWIKN"
      },
      "source": [
        "scoring_features = pd.concat([scoring_features,features_pos_ner],axis =1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33B3QF-i-PYb",
        "outputId": "08d17c42-e54e-4e3c-a03e-0310ce0fe582"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text', 'num_chars',\n",
              "       'num_words', 'avg_word_length', 'flesch_score', 'subjectivity_score',\n",
              "       'neg', 'neu', 'pos', 'compound', 'avg_tfidf ', 'Max_tfidf ',\n",
              "       'avg_tfidf_feature', 'max_tfidf_feature', 'assertive_verbs_count',\n",
              "       'factive_verbs_count', 'hedges_count', 'implicative_verbs_count',\n",
              "       'report_verbs_count', 'bias_word_list_01_2018_count',\n",
              "       'subjectivityClues_lexicon_count', 'pos_tags', 'lemma_pos', 'Ner_tags',\n",
              "       'NNS_count', 'NNPS_count', 'DT_count', 'JJ_count', 'JJS_count',\n",
              "       'NN_count', 'NORP_count', 'PERSON_count', 'adverb_count', 'GPE_count',\n",
              "       'pos_tags', 'lemmatized_withStopwords',\n",
              "       'tokenized_lemmatized_withstopwords', 'lemma_pos', 'Ner_tags'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzrj1vtNYskO",
        "outputId": "df05e33b-d0a2-4883-da26-bf5a8c5a4cc1"
      },
      "source": [
        "import ast\n",
        "\n",
        "for word, tag in ast.literal_eval(scoring_features.pos_tags[0]):\n",
        "  print(word , \"->\", tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "many -> JJ\n",
            "people -> NNS\n",
            "live -> VBP\n",
            "in -> IN\n",
            "ethiopia -> NNP\n",
            "the -> DT\n",
            "people -> NNS\n",
            "be -> VB\n",
            "very -> RB\n",
            "thin -> JJ\n",
            "and -> CC\n",
            "good -> JJ\n",
            "at -> IN\n",
            "distance -> NN\n",
            "run -> NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607bK0m3aw9Y"
      },
      "source": [
        "import ast\n",
        "\n",
        "def pos_count(text):\n",
        "  pos_type = []\n",
        "  # Combining lists of lists into single list \n",
        "  pos_list = ast.literal_eval(text)\n",
        "  for word,tag in pos_list :\n",
        "    if tag == part_of_speech:\n",
        "      # pos_type.append(word)\n",
        "      return 1\n",
        "  return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43dilSfglqMB"
      },
      "source": [
        "def check_col(col_name):\n",
        "  length = len(scoring_features[scoring_features[col_name] != 0])\n",
        "  return length "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVlh8ncxsgQD"
      },
      "source": [
        "def drop_col(df,col_name):\n",
        "  df.drop([col_name],axis=1, inplace=True)\n",
        "  print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWwt8P9seiBw"
      },
      "source": [
        "part_of_speech = 'NNS' # Plural nouns\n",
        "scoring_features['NNS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03_Hj9HsqwlN",
        "outputId": "1caa89c4-728c-4c16-999e-cbe694eef8d0"
      },
      "source": [
        "check_col('NNS_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2025"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cXoMyAtaOsG"
      },
      "source": [
        "part_of_speech = 'NNPS' # Proper Plural nouns\n",
        "scoring_features['NNPS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OaM-IQCq2QW",
        "outputId": "1ad440f3-1bde-40dc-c3ef-20ec95324ee6"
      },
      "source": [
        "check_col('NNPS_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "847"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvSVc_q_f-lj"
      },
      "source": [
        "part_of_speech = 'DT' # Determiners ( The with adjectives to refer a whole group of people)\n",
        "scoring_features['DT_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UoSSfhpr3O0",
        "outputId": "0bada0ef-5613-4fbd-f38f-3d32d5797093"
      },
      "source": [
        "check_col('DT_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12602"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtYYtaS-gshn"
      },
      "source": [
        "part_of_speech = 'JJ' # Adjective\n",
        "scoring_features['JJ_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ2-dWBGsRwF",
        "outputId": "0a2c1570-5d7b-4e9b-85fb-e398d81f69fb"
      },
      "source": [
        "check_col('JJ_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12399"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRLF674RgtR_"
      },
      "source": [
        "part_of_speech = 'sb' # Subject ( Subject refering to the group)\n",
        "scoring_features['sb_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq0aSAoGsWzJ",
        "outputId": "b20ca762-ab2b-4232-f982-9467a7188049"
      },
      "source": [
        "check_col('sb_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb0pDgNKsZoa",
        "outputId": "55c2aa6d-a473-4866-c830-9634dd02d443"
      },
      "source": [
        "drop_col(scoring_features,'sb_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
            "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
            "       'lemmatized_withStopwords', 'tokenized_lemmatized_withstopwords',\n",
            "       'num_chars', 'num_words', 'avg_word_length', 'flesch_score',\n",
            "       'subjectivity_score', 'neg', 'neu', 'pos', 'compound', 'avg_tfidf ',\n",
            "       'Max_tfidf ', 'avg_tfidf_feature', 'max_tfidf_feature',\n",
            "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
            "       'implicative_verbs_count', 'report_verbs_count',\n",
            "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
            "       'pos_tags', 'lemmatized_withStopwords',\n",
            "       'tokenized_lemmatized_withstopwords', 'lemma_pos', 'Ner_tags',\n",
            "       'NNS_count', 'NNPS_count', 'DT_count', 'JJ_count', 'JJS_count',\n",
            "       'NN_count', 'NORP_count', 'PERSON_count'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljrgz9C3gtvo"
      },
      "source": [
        "part_of_speech = 'JJS' # Superlative adjective for subjectivity\n",
        "scoring_features['JJS_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO2Y_1mps-0e",
        "outputId": "c8ecc6a5-44a3-4de7-9c60-83730545f7a9"
      },
      "source": [
        "check_col('JJS_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "279"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y01WcxWktEaT"
      },
      "source": [
        "part_of_speech = 'JJ' # adjective indicates \n",
        "scoring_features['JJ_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pO9kY3OtRgj",
        "outputId": "f6a1a826-1b48-43f1-f7ed-0c2f9d034956"
      },
      "source": [
        "check_col('JJ_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12399"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tdghlR2tiQL"
      },
      "source": [
        "part_of_speech = 'NN' # Noun\n",
        "scoring_features['NN_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcfZRFsdtprQ",
        "outputId": "128cd4bb-0961-4f77-f161-51574d47da71"
      },
      "source": [
        "check_col('NN_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15371"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cPLjY_ARXE"
      },
      "source": [
        "part_of_speech = \"RB\" # Frequency adverb to indicate generic sentences ['usually', 'typically', 'generally', 'sometimes', 'always']\n",
        "scoring_features['adverb_count'] = scoring_features['pos_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DojM7H4LN_Qg",
        "outputId": "675d7a42-37c7-4a49-9bf4-578ab356d88b"
      },
      "source": [
        "check_col('adverb_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7636"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hmJxMnKaiFn",
        "outputId": "a53c942a-5f8d-4b71-bb8b-b2e45d2400bc"
      },
      "source": [
        "scoring_features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Ethnicity', 'gender', 'profession', 'religion', 'Anti-stereotype',\n",
              "       'stereotype', 'unrelated', 'sentence', 'clean_text',\n",
              "       'lemmatized_withStopwords', 'tokenized_lemmatized_withstopwords',\n",
              "       'num_chars', 'num_words', 'avg_word_length', 'flesch_score',\n",
              "       'subjectivity_score', 'neg', 'neu', 'pos', 'compound', 'avg_tfidf ',\n",
              "       'Max_tfidf ', 'avg_tfidf_feature', 'max_tfidf_feature',\n",
              "       'assertive_verbs_count', 'factive_verbs_count', 'hedges_count',\n",
              "       'implicative_verbs_count', 'report_verbs_count',\n",
              "       'bias_word_list_01_2018_count', 'subjectivityClues_lexicon_count',\n",
              "       'pos_tags', 'lemmatized_withStopwords',\n",
              "       'tokenized_lemmatized_withstopwords', 'lemma_pos', 'Ner_tags',\n",
              "       'NNS_count', 'NNPS_count', 'DT_count', 'JJ_count', 'JJS_count',\n",
              "       'NN_count', 'NORP_count', 'PERSON_count', 'verb_count', 'article_count',\n",
              "       'adverb_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKg47wgbuhq7"
      },
      "source": [
        "Named entity recognition features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7gFfCimf2Ig"
      },
      "source": [
        "part_of_speech = 'NORP' # Nationalities or religious or political groups\n",
        "scoring_features['NORP_count'] = scoring_features['Ner_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NVzX_ylvoxb",
        "outputId": "9805bf75-aba2-44a0-fde7-438efc2257df"
      },
      "source": [
        "check_col('NORP_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3282"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXI_mHrox7GE"
      },
      "source": [
        "part_of_speech = 'PERSON' # People, including fictional => cue for gender, \n",
        "scoring_features['PERSON_count'] = scoring_features['Ner_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSIcFHg8x75E",
        "outputId": "3b560c03-ee2c-4c0b-b82c-18b99307202a"
      },
      "source": [
        "check_col('PERSON_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1551"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYjEHTwcaMcg"
      },
      "source": [
        "part_of_speech = 'GPE' # Countires, cities, states\n",
        "scoring_features['GPE_count'] = scoring_features['Ner_tags'].apply(pos_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8fTThhraZhM",
        "outputId": "0d8a420b-da8f-484f-cca4-c627d0990419"
      },
      "source": [
        "check_col('GPE_count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2239"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3-5YaSHzQN7"
      },
      "source": [
        "Characteristic terms used in stereoset and crows-s-pair dataset per bias type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y16M6AumzOi8",
        "outputId": "f9ff96f8-4012-4b74-9283-ebb35f73b124"
      },
      "source": [
        "pip install scattertext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.4-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.1.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting gensim>=4.0.0\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 95 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.4.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.10.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.0.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.1)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9310 sha256=b39ef2480e29d783e1eb8c96925b29e0712eb47b15f9c2af04ae5b94fac1abc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: mock, gensim, flashtext, scattertext\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed flashtext-2.7 gensim-4.0.1 mock-4.0.3 scattertext-0.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzq7TOXlzza1"
      },
      "source": [
        "import scattertext as st\n",
        "import spacy\n",
        "from pprint import pprint\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdrKT72Wz0vh"
      },
      "source": [
        "stereo = pd.read_csv('/content/drive/MyDrive/Trained_models/mult_label_dataset/Unrelated_samples_adjusted/Multi_label_stereo.csv',index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHiHIO6h0urf",
        "outputId": "c6ddb369-41e6-4ffb-9a51-ec9fd56adfa7"
      },
      "source": [
        "stereo.bias_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ethnicity     5226\n",
              "profession    3112\n",
              "gender        2024\n",
              "religion      1953\n",
              "Name: bias_type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs7et2k61S-w"
      },
      "source": [
        "corpus = st.CorpusFromPandas(stereo, category_col='bias_type', text_col='sentence', nlp=nlp).build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GtF4-kC1pK9"
      },
      "source": [
        "x = pd.DataFrame(corpus.get_scaled_f_scores_vs_background())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7TrG99KB3N_-",
        "outputId": "2614d330-2ba1-416c-abf0-c8ef45ccd21d"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>background</th>\n",
              "      <th>Scaled f-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>eriteria</th>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>norweigan</th>\n",
              "      <td>96.0</td>\n",
              "      <td>46910.0</td>\n",
              "      <td>0.000911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eritrean</th>\n",
              "      <td>101.0</td>\n",
              "      <td>229521.0</td>\n",
              "      <td>0.000514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>allcaps</th>\n",
              "      <td>41.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>crimean</th>\n",
              "      <td>100.0</td>\n",
              "      <td>279156.0</td>\n",
              "      <td>0.000452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genlyte</th>\n",
              "      <td>0.0</td>\n",
              "      <td>16620.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genlock</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32902.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genl</th>\n",
              "      <td>0.0</td>\n",
              "      <td>121289.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>genksyms</th>\n",
              "      <td>0.0</td>\n",
              "      <td>32575.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zzzz</th>\n",
              "      <td>0.0</td>\n",
              "      <td>362520.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333780 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           corpus  background  Scaled f-score\n",
              "eriteria     96.0         0.0        0.001173\n",
              "norweigan    96.0     46910.0        0.000911\n",
              "eritrean    101.0    229521.0        0.000514\n",
              "allcaps      41.0         0.0        0.000501\n",
              "crimean     100.0    279156.0        0.000452\n",
              "...           ...         ...             ...\n",
              "genlyte       0.0     16620.0        0.000000\n",
              "genlock       0.0     32902.0        0.000000\n",
              "genl          0.0    121289.0        0.000000\n",
              "genksyms      0.0     32575.0        0.000000\n",
              "zzzz          0.0    362520.0        0.000000\n",
              "\n",
              "[333780 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IukUH2xq3TDt"
      },
      "source": [
        "Extracting top 1000 keyterms for each bias type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egEYiU3X3Rlj",
        "outputId": "ed287673-f01d-4992-d8cc-20dcfdcde694"
      },
      "source": [
        "term_freq_df = corpus.get_term_freq_df()\n",
        "term_freq_df['Ethnicity_score'] = corpus.get_scaled_f_scores('Ethnicity')\n",
        "pprint(list(term_freq_df.sort_values(by='Ethnicity_score', ascending=False).index[:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ethiopia',\n",
            " 'italy',\n",
            " 'somalia',\n",
            " 'sierra',\n",
            " 'lebanon',\n",
            " 'japanese',\n",
            " 'persian',\n",
            " 'bangladesh',\n",
            " 'ghanaian',\n",
            " 'morocco',\n",
            " 'ecuador',\n",
            " 'spain',\n",
            " 'cameroon',\n",
            " 'leon',\n",
            " 'sierra leon',\n",
            " 'eritrean',\n",
            " 'persian people',\n",
            " 'crimean',\n",
            " 'bengali',\n",
            " 'norweigan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLJOQym3gsR"
      },
      "source": [
        "charteristic_terms_ethnicity = list(term_freq_df.sort_values(by='Ethnicity_score', ascending=False).index[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPencWd63-wo"
      },
      "source": [
        "term_freq_df['profession_score'] = corpus.get_scaled_f_scores('profession')\n",
        "charteristic_terms_profession = list(term_freq_df.sort_values(by='profession_score', ascending=False).index[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2AariwA4W_G"
      },
      "source": [
        "term_freq_df['gender_score'] = corpus.get_scaled_f_scores('gender')\n",
        "charteristic_terms_gender = list(term_freq_df.sort_values(by='gender_score', ascending=False).index[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIp9iZvg4hhC"
      },
      "source": [
        "term_freq_df['religion_score'] = corpus.get_scaled_f_scores('religion')\n",
        "charteristic_terms_religion = list(term_freq_df.sort_values(by='religion_score', ascending=False).index[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epzbGiSbCpZB"
      },
      "source": [
        "def count_lexicon(text):\n",
        "  count = 0\n",
        "  try:\n",
        "    for token in lexicon:\n",
        "      if token in text and len(token) > 1:\n",
        "        count +=1\n",
        "      else:\n",
        "        continue\n",
        "  except :\n",
        "    pass\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWmgCGw4CcBZ"
      },
      "source": [
        "Charteristic_terms Ethnicity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtD9YxLiCpZC"
      },
      "source": [
        "lexicon = charteristic_terms_ethnicity "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdOzZxZdCpZC"
      },
      "source": [
        "scoring_features['charteristic_terms_ethnicity_count'] = scoring_features['tokenized_lemmatized_withstopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mENkyzTvCpZC",
        "outputId": "42ab4e15-c25b-4cc9-d60f-e9a2a7fde288"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_ethnicity_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16019"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1cSHJKXFE77"
      },
      "source": [
        "Charteristic_terms_profession "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ajqPwA7FJ8S"
      },
      "source": [
        "lexicon = charteristic_terms_profession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBtkbDtnHtpi",
        "outputId": "90339f5c-4845-44df-a0ae-f7a3948fb0f0"
      },
      "source": [
        "lexicon[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['schoolboy',\n",
              " 'gentlemen',\n",
              " 'schoolgirl',\n",
              " 'the schoolboy',\n",
              " 'mommy',\n",
              " 'the gentlemen',\n",
              " 'the schoolgirl',\n",
              " 'herself',\n",
              " 'grandfather',\n",
              " 'my grandfather']"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXbwm9-FF24K"
      },
      "source": [
        "scoring_features['charteristic_terms_profession_count'] = scoring_features['tokenized_lemmatized_withstopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8KTH1EuF24L",
        "outputId": "2bee4bbe-4996-48ce-dd52-208670920e84"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_profession_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15650"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2DOcpPlFKf8"
      },
      "source": [
        "Charteristic_terms_gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8U_Z0G3FM2W"
      },
      "source": [
        "lexicon = set(tokenize(lexicons['assertive_verbs.txt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fU0o2OPHoIQ",
        "outputId": "7fa86476-d5c8-4667-f06a-c568a963f770"
      },
      "source": [
        "lexicon[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['schoolboy',\n",
              " 'gentlemen',\n",
              " 'schoolgirl',\n",
              " 'the schoolboy',\n",
              " 'mommy',\n",
              " 'the gentlemen',\n",
              " 'the schoolgirl',\n",
              " 'herself',\n",
              " 'grandfather',\n",
              " 'my grandfather']"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LxWyxcCF5jT"
      },
      "source": [
        "scoring_features['charteristic_terms_gender_count'] = scoring_features['tokenized_lemmatized_withstopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heqr1P5PF5jT",
        "outputId": "69edf5f3-5605-4d81-bb9d-590309359432"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_gender_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14758"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ev__MHFNJj"
      },
      "source": [
        "Charteristic_terms_religion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NYxzewFPuq"
      },
      "source": [
        "lexicon = charteristic_terms_religion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeBeVNrBK9Vy",
        "outputId": "e066085b-0549-450a-baba-3b3c2ebaa5ea"
      },
      "source": [
        "lexicon[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['difference between',\n",
              " 's the',\n",
              " 'what s',\n",
              " 'between a',\n",
              " 'you call',\n",
              " 'it s',\n",
              " 'don t',\n",
              " 'don',\n",
              " 'the jews',\n",
              " 'jesus']"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWT2WN6DF7HE"
      },
      "source": [
        "scoring_features['charteristic_terms_religion_count'] = scoring_features['tokenized_lemmatized_withstopwords'].apply(count_lexicon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJqDAzS9F7HF",
        "outputId": "273e82a5-0b32-4ea4-dc7f-705d2e4c888a"
      },
      "source": [
        "len(scoring_features[scoring_features['charteristic_terms_religion_count'] != 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15586"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvReISAouQMD"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTLqLS2YkZcf"
      },
      "source": [
        "### SVM with selected features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxwBVsaUgMF"
      },
      "source": [
        "MAX_LEN = 50\n",
        "RANDOM_SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE9QsqX6w9uj"
      },
      "source": [
        "import pandas as pd\n",
        "feature_df = pd.read_csv('/content/drive/MyDrive/Trained_models/Results_compilation /Unrelated_adjusted/Baselines/Feature based/Feature_df/final_features.csv', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "H5XsCkaJ4bPh",
        "outputId": "a08d328d-f5ae-4295-9d34-933451b4d6c8"
      },
      "source": [
        "feature_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>max_tfidf_feature</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>NNS_count</th>\n",
              "      <th>NNPS_count</th>\n",
              "      <th>DT_count</th>\n",
              "      <th>JJ_count</th>\n",
              "      <th>JJS_count</th>\n",
              "      <th>NN_count</th>\n",
              "      <th>NORP_count</th>\n",
              "      <th>PERSON_count</th>\n",
              "      <th>adverb_count</th>\n",
              "      <th>GPE_count</th>\n",
              "      <th>charteristic_terms_ethnicity_count</th>\n",
              "      <th>charteristic_terms_profession_count</th>\n",
              "      <th>charteristic_terms_gender_count</th>\n",
              "      <th>charteristic_terms_religion_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.442653</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.494477</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.510763</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.344728</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.508637</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ethnicity  ...  charteristic_terms_religion_count\n",
              "0        1.0  ...                                  2\n",
              "1        1.0  ...                                  5\n",
              "2        0.0  ...                                  9\n",
              "3        1.0  ...                                  5\n",
              "4        1.0  ...                                  4\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IZ8yVgoEFgy"
      },
      "source": [
        "# feature_df.dropna(axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldRNkiiAFx3J",
        "outputId": "bde83793-9c24-4b2d-911f-19a152c6eeb7"
      },
      "source": [
        "feature_df.iloc[:,7:].columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['num_chars', 'num_words', 'avg_word_length', 'flesch_score', 'neg',\n",
              "       'neu', 'pos', 'max_tfidf_feature', 'assertive_verbs_count',\n",
              "       'factive_verbs_count', 'hedges_count', 'implicative_verbs_count',\n",
              "       'report_verbs_count', 'bias_word_list_01_2018_count',\n",
              "       'subjectivityClues_lexicon_count', 'NNS_count', 'NNPS_count',\n",
              "       'DT_count', 'JJ_count', 'JJS_count', 'NN_count', 'NORP_count',\n",
              "       'PERSON_count', 'adverb_count', 'GPE_count',\n",
              "       'charteristic_terms_ethnicity_count',\n",
              "       'charteristic_terms_profession_count',\n",
              "       'charteristic_terms_gender_count', 'charteristic_terms_religion_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWJjN78DTbTt"
      },
      "source": [
        "y = feature_df.iloc[:,:7].values\n",
        "X = feature_df.iloc[:,7:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7usepriRO9yZ"
      },
      "source": [
        "LABEL_COLUMN = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6--cX0RNkOa6"
      },
      "source": [
        "features = feature_df.iloc[:,7:]\n",
        "FEATURE_COLUMNS = features.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF2QCbhL7QZ8",
        "outputId": "9a101c72-55bc-4d5a-ca47-5968ba07dbb4"
      },
      "source": [
        "features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['num_chars', 'num_words', 'avg_word_length', 'flesch_score', 'neg',\n",
              "       'neu', 'pos', 'max_tfidf_feature', 'assertive_verbs_count',\n",
              "       'factive_verbs_count', 'hedges_count', 'implicative_verbs_count',\n",
              "       'report_verbs_count', 'bias_word_list_01_2018_count',\n",
              "       'subjectivityClues_lexicon_count', 'NNS_count', 'NNPS_count',\n",
              "       'DT_count', 'JJ_count', 'JJS_count', 'NN_count', 'NORP_count',\n",
              "       'PERSON_count', 'adverb_count', 'GPE_count',\n",
              "       'charteristic_terms_ethnicity_count',\n",
              "       'charteristic_terms_profession_count',\n",
              "       'charteristic_terms_gender_count', 'charteristic_terms_religion_count'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFD-634ULmH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTpY3Vy3O7Hg"
      },
      "source": [
        "train_df_labels = pd.DataFrame(train_df_labels, columns= LABEL_COLUMN)\n",
        "val_df_labels = pd.DataFrame(val_df_labels, columns= LABEL_COLUMN)\n",
        "test_df_labels = pd.DataFrame(test_df_labels, columns= LABEL_COLUMN)\n",
        "train_df_features = pd.DataFrame(train_df_text, columns = FEATURE_COLUMNS)\n",
        "val_df_features  = pd.DataFrame(val_df_text, columns = FEATURE_COLUMNS)\n",
        "test_df_features  = pd.DataFrame(test_df_text, columns = FEATURE_COLUMNS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvuTsDvhiP8Q"
      },
      "source": [
        "# train_df = pd.concat([train_df_text,train_df_labels], axis = 1)\n",
        "# val_df = pd.concat([val_df_text,val_df_labels], axis = 1)\n",
        "# test_df = pd.concat([test_df_text,test_df_labels], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-knE4da5ne"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXq15YG-a7U4"
      },
      "source": [
        "def Accuracy(y_true, y_pred):\n",
        "  temp = 0\n",
        "  for i in range(y_true.shape[0]):\n",
        "      temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
        "  return temp / y_true.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVPGwBAa7Hw"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report,hamming_loss, roc_auc_score, accuracy_score,multilabel_confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "upper, lower = 1, 0\n",
        "LABELS = ['Ethnicity','gender','profession','religion','Anti-stereotype','stereotype','unrelated']\n",
        "\n",
        "def classification_metrics(test_pred,labels,model_name,threshold, sigmoid = False):\n",
        "\n",
        "  print(\"Evaluation metrics for test set:\")\n",
        "  if sigmoid:\n",
        "    y_pred = np.where(test_pred > threshold, upper, lower)\n",
        "  else:\n",
        "    y_pred = test_pred\n",
        "\n",
        "  ROC_AUC_score = roc_auc_score(test_df_labels, test_pred)\n",
        "  accuracy = accuracy_score(labels, y_pred)\n",
        "  hloss = hamming_loss(labels, y_pred)\n",
        "  hscore = Accuracy(labels, y_pred)\n",
        "\n",
        "  precision_sample_average = precision_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "  recall_sample_average = recall_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "  f1_sample_average= f1_score(y_true=labels, y_pred=y_pred, average='samples')\n",
        "\n",
        "  cr = classification_report(labels, y_pred, labels=list(range(len(LABELS))), target_names=LABELS, output_dict=True)\n",
        "  cf = multilabel_confusion_matrix(test_df_labels, \n",
        "  y_pred)\n",
        "\n",
        "  model_metrics = {}\n",
        "  model_metrics[\"AUC_ROC_score\"] = ROC_AUC_score\n",
        "  model_metrics[\"subset_accuracy\"] = accuracy\n",
        "  model_metrics[\"hamming_loss\"]= hloss\n",
        "  model_metrics[\"hamming_score\"] = hscore\n",
        "\n",
        "  model_metrics['sample_average_precision'] = precision_sample_average\n",
        "  model_metrics['sample_average_recall'] = recall_sample_average\n",
        "  model_metrics['sample_average_f1'] = f1_sample_average\n",
        "\n",
        "\n",
        "  if write_to_file:\n",
        "    model_metrics[\"Classification_report\"] = cr\n",
        "\n",
        "    for i,val in enumerate(LABELS):\n",
        "      model_metrics['confusion_matrix' + '_' + val] = str(cf[i].flatten())\n",
        "  \n",
        "    model_metrics[\"y_pred\"] = str(y_pred)\n",
        "    model_metrics[\"y_labels\"] = str(test_df_labels)\n",
        "\n",
        "\n",
        "    if threshold != 0.5:\n",
        "      th = \"calculated_threshold\"\n",
        "    else:\n",
        "      th = threshold\n",
        "\n",
        "    model_metrics[\"threshold\"] = th\n",
        "    output_file = \"eval_results_\" + model_name + \"_\"+str(th) +\"_\"+ \".json\"\n",
        "    output_file = '/content/drive/MyDrive/Trained_models/Results_compilation /Unrelated_adjusted/Baselines/Feature based'+output_file\n",
        "    with open(output_file, \"w\" ) as writer:\n",
        "        json.dump(model_metrics,writer)\n",
        "  \n",
        "\n",
        "  print(\"\\n ROC-AUC score: %.6f \\n\" % (ROC_AUC_score))\n",
        "  print(\"\\n Subset accuracy : %.6f \\n\" % (accuracy))\n",
        "  print(\"\\n hamming_loss : %.6f \\n\" % (hloss))\n",
        "  print(\"\\n hamming score : %.6f \\n\" % hscore)\n",
        "  print(\"\\n sample average  precision_sample_average : %.6f \\n\" % precision_sample_average)\n",
        "  print(\"\\n sample average  recall_sample_average : %.6f \\n\" % recall_sample_average)\n",
        "  print(\"\\n sample average  f1_sample_average : %.6f \\n\" % f1_sample_average)\n",
        "  \n",
        "\n",
        "  print(\"  Saving the metrics into a file: \" + output_file + \" with threshold :\" + str(threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKxdMP9QTkjG"
      },
      "source": [
        "Without feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4NT3XnXT_HD",
        "outputId": "ec58a551-28ba-47e9-a1bc-f55d9bc48aab"
      },
      "source": [
        "train_df_features.shape, train_df_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11592, 29), (11592, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGW4vnfR6SEH",
        "outputId": "966ec76d-90aa-4620-ebf7-ef91ec8735ae"
      },
      "source": [
        "val_df_features.shape, val_df_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2484, 29), (2484, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv8YDvqxy9-l",
        "outputId": "7a7faaaa-d437-431a-be24-b21275305eb7"
      },
      "source": [
        "np.any(np.isnan(train_df_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0emmQ1r290b"
      },
      "source": [
        "train_df_features = pd.concat([train_df_features,val_df_features])\n",
        "train_df_labels = pd.concat([train_df_labels,val_df_labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI4-IpYCuFCN"
      },
      "source": [
        "train_df_features.reset_index(drop=True,inplace=True)\n",
        "train_df_labels.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq-53sX8x7by",
        "outputId": "83201637-ce40-4ac5-cf63-5bdbacdb6a9a"
      },
      "source": [
        "np.any(np.isnan(train_df_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 487
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "g-qgciZ0yytN",
        "outputId": "69fd6c42-fd6e-4dc9-89df-2b631115b677"
      },
      "source": [
        "train_df_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>max_tfidf_feature</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>NNS_count</th>\n",
              "      <th>NNPS_count</th>\n",
              "      <th>DT_count</th>\n",
              "      <th>JJ_count</th>\n",
              "      <th>JJS_count</th>\n",
              "      <th>NN_count</th>\n",
              "      <th>NORP_count</th>\n",
              "      <th>PERSON_count</th>\n",
              "      <th>adverb_count</th>\n",
              "      <th>GPE_count</th>\n",
              "      <th>charteristic_terms_ethnicity_count</th>\n",
              "      <th>charteristic_terms_profession_count</th>\n",
              "      <th>charteristic_terms_gender_count</th>\n",
              "      <th>charteristic_terms_religion_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>104.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>4.526316</td>\n",
              "      <td>78.75</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.451396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>93.14</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.839</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.527550</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>96.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>4.705882</td>\n",
              "      <td>71.31</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.753407</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.923077</td>\n",
              "      <td>73.34</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.476005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>52.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.777778</td>\n",
              "      <td>53.88</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.510271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14071</th>\n",
              "      <td>74.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.769231</td>\n",
              "      <td>83.66</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.845</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.428266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14072</th>\n",
              "      <td>32.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>115.13</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.654300</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14073</th>\n",
              "      <td>69.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.384615</td>\n",
              "      <td>90.26</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.647</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.447422</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14074</th>\n",
              "      <td>28.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.800000</td>\n",
              "      <td>100.24</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.512425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14075</th>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.600000</td>\n",
              "      <td>100.24</td>\n",
              "      <td>0.411</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.646481</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14076 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       num_chars  ...  charteristic_terms_religion_count\n",
              "0          104.0  ...                                5.0\n",
              "1           56.0  ...                                2.0\n",
              "2           96.0  ...                                5.0\n",
              "3           76.0  ...                                5.0\n",
              "4           52.0  ...                                7.0\n",
              "...          ...  ...                                ...\n",
              "14071       74.0  ...                                4.0\n",
              "14072       32.0  ...                                2.0\n",
              "14073       69.0  ...                                4.0\n",
              "14074       28.0  ...                                2.0\n",
              "14075       27.0  ...                                1.0\n",
              "\n",
              "[14076 rows x 29 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 488
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6buiwIJnS-Iq"
      },
      "source": [
        "from sklearn.svm import SVC # To be run again after running the above cell (train, test : 85, 15)\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "classifier = SVC(kernel = 'linear', random_state = 42)\n",
        "multilabel_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
        "multilabel_classifier = multilabel_classifier.fit(train_df_features, train_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVECGulDakPe"
      },
      "source": [
        "Prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Ts6ZvOT5va"
      },
      "source": [
        "y_test_pred = multilabel_classifier.predict(test_df_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ0FYq1yeR2M",
        "outputId": "6a7bfab8-3c6d-48db-dc81-95d8f2095438"
      },
      "source": [
        "y_test_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 491
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqiJmTgwvRhG"
      },
      "source": [
        "labels = test_df_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1aZdMutavqb",
        "outputId": "aa04a9d0-d4c9-4594-c37f-8ec3b7c86a41"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"SVM_Only_features\",0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.692699 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.274155 \n",
            "\n",
            "\n",
            " hamming_loss : 0.175063 \n",
            "\n",
            "\n",
            " hamming score : 0.437970 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.578167 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.458736 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.496578 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_SVM_Only_features_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkoKNnRzkjKj"
      },
      "source": [
        "### SVM with selected features and tfidf features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV9-dVG4slSO"
      },
      "source": [
        "SVM + tfi_idf + feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgryD4ObcAKe"
      },
      "source": [
        "feature_vector_tfidf = pd.concat([feature_df,tf_idf_feature], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "7IW_iYO-wbov",
        "outputId": "5d516bbc-9a07-434d-f2a6-217b32f4eb92"
      },
      "source": [
        "feature_vector_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>num_words</th>\n",
              "      <th>avg_word_length</th>\n",
              "      <th>flesch_score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>max_tfidf_feature</th>\n",
              "      <th>assertive_verbs_count</th>\n",
              "      <th>factive_verbs_count</th>\n",
              "      <th>hedges_count</th>\n",
              "      <th>implicative_verbs_count</th>\n",
              "      <th>report_verbs_count</th>\n",
              "      <th>bias_word_list_01_2018_count</th>\n",
              "      <th>subjectivityClues_lexicon_count</th>\n",
              "      <th>NNS_count</th>\n",
              "      <th>NNPS_count</th>\n",
              "      <th>DT_count</th>\n",
              "      <th>JJ_count</th>\n",
              "      <th>JJS_count</th>\n",
              "      <th>NN_count</th>\n",
              "      <th>NORP_count</th>\n",
              "      <th>PERSON_count</th>\n",
              "      <th>adverb_count</th>\n",
              "      <th>GPE_count</th>\n",
              "      <th>charteristic_terms_ethnicity_count</th>\n",
              "      <th>charteristic_terms_profession_count</th>\n",
              "      <th>charteristic_terms_gender_count</th>\n",
              "      <th>charteristic_terms_religion_count</th>\n",
              "      <th>tfIdf_aardvark</th>\n",
              "      <th>tfIdf_ab</th>\n",
              "      <th>tfIdf_ababa</th>\n",
              "      <th>tfIdf_aback</th>\n",
              "      <th>...</th>\n",
              "      <th>tfIdf_yiddishkeit</th>\n",
              "      <th>tfIdf_yield</th>\n",
              "      <th>tfIdf_yo</th>\n",
              "      <th>tfIdf_yoga</th>\n",
              "      <th>tfIdf_yogurt</th>\n",
              "      <th>tfIdf_yolanda</th>\n",
              "      <th>tfIdf_york</th>\n",
              "      <th>tfIdf_yorker</th>\n",
              "      <th>tfIdf_yorkshire</th>\n",
              "      <th>tfIdf_young</th>\n",
              "      <th>tfIdf_youth</th>\n",
              "      <th>tfIdf_youtube</th>\n",
              "      <th>tfIdf_yrs</th>\n",
              "      <th>tfIdf_yu</th>\n",
              "      <th>tfIdf_yucatan</th>\n",
              "      <th>tfIdf_yum</th>\n",
              "      <th>tfIdf_yummy</th>\n",
              "      <th>tfIdf_zach</th>\n",
              "      <th>tfIdf_zack</th>\n",
              "      <th>tfIdf_zag</th>\n",
              "      <th>tfIdf_zaknelson</th>\n",
              "      <th>tfIdf_ze</th>\n",
              "      <th>tfIdf_zebra</th>\n",
              "      <th>tfIdf_zeke</th>\n",
              "      <th>tfIdf_zenlike</th>\n",
              "      <th>tfIdf_zero</th>\n",
              "      <th>tfIdf_zig</th>\n",
              "      <th>tfIdf_zionism</th>\n",
              "      <th>tfIdf_zionist</th>\n",
              "      <th>tfIdf_zip</th>\n",
              "      <th>tfIdf_zit</th>\n",
              "      <th>tfIdf_zoey</th>\n",
              "      <th>tfIdf_zog</th>\n",
              "      <th>tfIdf_zombie</th>\n",
              "      <th>tfIdf_zone</th>\n",
              "      <th>tfIdf_zoo</th>\n",
              "      <th>tfIdf_zookeeper</th>\n",
              "      <th>tfIdf_zoos</th>\n",
              "      <th>tfIdf_zumba</th>\n",
              "      <th>tfIdf_zyklon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>84</td>\n",
              "      <td>15</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>89.24</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.442653</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53</td>\n",
              "      <td>10</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>52.87</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.494477</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79</td>\n",
              "      <td>14</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>89.75</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.510763</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>106</td>\n",
              "      <td>20</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>86.71</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.344728</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>58</td>\n",
              "      <td>11</td>\n",
              "      <td>4.363636</td>\n",
              "      <td>91.27</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.508637</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16555</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>46</td>\n",
              "      <td>7</td>\n",
              "      <td>5.714286</td>\n",
              "      <td>64.37</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.674</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.705372</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16556</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>54</td>\n",
              "      <td>9</td>\n",
              "      <td>5.111111</td>\n",
              "      <td>29.52</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.447653</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16557</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>73</td>\n",
              "      <td>9</td>\n",
              "      <td>7.222222</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.416111</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16558</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141</td>\n",
              "      <td>31</td>\n",
              "      <td>3.580645</td>\n",
              "      <td>82.31</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.410444</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16559</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>65</td>\n",
              "      <td>13</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>66.74</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.782</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.467763</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16560 rows × 9065 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  gender  profession  ...  tfIdf_zoos  tfIdf_zumba  tfIdf_zyklon\n",
              "0            1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "1            1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "2            0.0     1.0         0.0  ...         0.0          0.0           0.0\n",
              "3            1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "4            1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "...          ...     ...         ...  ...         ...          ...           ...\n",
              "16555        0.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "16556        0.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "16557        0.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "16558        0.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "16559        0.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "\n",
              "[16560 rows x 9065 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKiuGn_A7A98"
      },
      "source": [
        "y = feature_vector_tfidf.iloc[:,:7].values\n",
        "X = feature_vector_tfidf.iloc[:,7:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BN5MQphQf1o",
        "outputId": "599a717b-c3e2-4d8d-d9f0-2e4cd9c08a7a"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 84.        ,  15.        ,   4.66666667, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 53.        ,  10.        ,   4.4       , ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 79.        ,  14.        ,   4.71428571, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       ...,\n",
              "       [ 73.        ,   9.        ,   7.22222222, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [141.        ,  31.        ,   3.58064516, ...,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [ 65.        ,  13.        ,   4.        , ...,   0.        ,\n",
              "          0.        ,   0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8lq3uin_FPF",
        "outputId": "10c69d8a-8deb-464c-a01a-bf5af333dffd"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 1., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3wKfQt7A99"
      },
      "source": [
        "LABEL_COLUMN = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPOqt---QcK-"
      },
      "source": [
        "features = feature_vector_tfidf.iloc[:,7:]\n",
        "FEATURE_COLUMNS = features.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwIeoZRhQcK_",
        "outputId": "335fcc48-e54c-4950-cda9-4b3b52adb706"
      },
      "source": [
        "features.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['num_chars', 'num_words', 'avg_word_length', 'flesch_score', 'neg',\n",
              "       'neu', 'pos', 'max_tfidf_feature', 'assertive_verbs_count',\n",
              "       'factive_verbs_count',\n",
              "       ...\n",
              "       'tfIdf_zit', 'tfIdf_zoey', 'tfIdf_zog', 'tfIdf_zombie', 'tfIdf_zone',\n",
              "       'tfIdf_zoo', 'tfIdf_zookeeper', 'tfIdf_zoos', 'tfIdf_zumba',\n",
              "       'tfIdf_zyklon'],\n",
              "      dtype='object', length=9058)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chzm53SA7A9-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMhPkgDQ7A9-"
      },
      "source": [
        "train_df_labels = pd.DataFrame(train_df_labels, columns= LABEL_COLUMN)\n",
        "val_df_labels = pd.DataFrame(val_df_labels, columns= LABEL_COLUMN)\n",
        "test_df_labels = pd.DataFrame(test_df_labels, columns= LABEL_COLUMN)\n",
        "train_df_features = pd.DataFrame(train_df_text, columns = FEATURE_COLUMNS)\n",
        "val_df_features  = pd.DataFrame(val_df_text, columns = FEATURE_COLUMNS)\n",
        "test_df_features  = pd.DataFrame(test_df_text, columns = FEATURE_COLUMNS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOSQM7mq3V1k"
      },
      "source": [
        "train_df_features = pd.concat([train_df_features,val_df_features])\n",
        "train_df_labels = pd.concat([train_df_labels,val_df_labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INayQOYkzxzC"
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler\n",
        "# sc = StandardScaler()\n",
        "# X_train = sc.fit_transform(train_df_features)\n",
        "# X_test = sc.transform(test_df_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8pjiKHdZuU8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "98b8975d-6a3a-4b80-bee5-c109edc3b1cb"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-931765772341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iaqHtxY3rLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1027b6ee-19bc-48fb-de91-762fc4d62f35"
      },
      "source": [
        "from sklearn.svm import SVC # To be run again after running the above cell (train, test : 85, 15)\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "classifier = SVC(kernel = 'linear', random_state = 42)\n",
        "multilabel_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
        "multilabel_classifier = multilabel_classifier.fit(train_df_features, train_df_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-66208c10287c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmultilabel_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmultilabel_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilabel_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    168\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    169\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[0;32m--> 170\u001b[0;31m             for i in range(y.shape[1]))\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9e0UtRi3rLC"
      },
      "source": [
        "y_test_pred = multilabel_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpPrgTvC3rLD"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN37mvdp3rLD"
      },
      "source": [
        "labels = test_df_labels.values\n",
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8lqkjn3rLD"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"SVM_tfidf_Selectedfeatures\",0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INyVGybgk0Fm"
      },
      "source": [
        "### Naive Bayes with bag of words features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EkXcufOAGVC"
      },
      "source": [
        "Naive Bayes with bag of words features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-SdKW-HCpZZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Trained_models/Results_compilation /Unrelated_adjusted/Baselines/Feature based/Feature_df/stereo_features_pos_ner.csv',index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAsZS9yu0rSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63551fb9-d749-46cd-9f5b-fb9ed419e75a"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>tokenized_lemmatized_withstopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>['people', 'live', 'ethiopia', 'people', 'thin...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>['professor', 'hispanic', 'man', 'come', 'ille...</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>['schoolgirl', 'walk', 'street', 'target', 'ch...</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>['likely', 'fly', 'cape', 'verde', 'flight', '...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>['know', 'people', 'russian', 'love', 'vodka']</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16555</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Cookies are good substitutes with liquid milk.</td>\n",
              "      <td>cookie be good substitute with liquid milk</td>\n",
              "      <td>[('cookie', 'NN'), ('be', 'VB'), ('good', 'JJ'...</td>\n",
              "      <td>cookie good substitute liquid milk</td>\n",
              "      <td>['cookie', 'good', 'substitute', 'liquid', 'mi...</td>\n",
              "      <td>[('cookie', 'NNP'), ('good', 'JJ'), ('substitu...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16556</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Jollof - Rice cereal is totally better than Ja...</td>\n",
              "      <td>jollof rice cereal be totally well than jambalaya</td>\n",
              "      <td>[('jollof', 'NNP'), ('rice', 'NNP'), ('cereal'...</td>\n",
              "      <td>jollof rice cereal totally jambalaya</td>\n",
              "      <td>['jollof', 'rice', 'cereal', 'totally', 'jamba...</td>\n",
              "      <td>[('jollof', 'NNP'), ('rice', 'NNP'), ('cereal'...</td>\n",
              "      <td>[('jollof rice', 'PERSON')]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16557</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Biking out is an economical energy-efficient m...</td>\n",
              "      <td>bike out be an economical energy efficient mod...</td>\n",
              "      <td>[('bike', 'NN'), ('out', 'RP'), ('be', 'VB'), ...</td>\n",
              "      <td>bike economical energy efficient mode unk tran...</td>\n",
              "      <td>['bike', 'economical', 'energy', 'efficient', ...</td>\n",
              "      <td>[('bike', 'NNP'), ('economical', 'JJ'), ('ener...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16558</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>I might see the little dog is a still in need ...</td>\n",
              "      <td>may see the little dog be a still in need of f...</td>\n",
              "      <td>[('may', 'MD'), ('see', 'VB'), ('the', 'DT'), ...</td>\n",
              "      <td>little dog need food park grocery store find f...</td>\n",
              "      <td>['little', 'dog', 'need', 'food', 'park', 'gro...</td>\n",
              "      <td>[('little', 'JJ'), ('dog', 'NN'), ('need', 'VB...</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16559</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>going to the California beach in the morning  ...</td>\n",
              "      <td>go to the california beach in the morning whic...</td>\n",
              "      <td>[('go', 'VB'), ('to', 'IN'), ('the', 'DT'), ('...</td>\n",
              "      <td>california beach morning cool</td>\n",
              "      <td>['california', 'beach', 'morning', 'cool']</td>\n",
              "      <td>[('california', 'NNP'), ('beach', 'NNP'), ('mo...</td>\n",
              "      <td>[('california', 'GPE')]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16560 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Ethnicity  ...                     Ner_tags\n",
              "0            1.0  ...        [('ethiopia', 'GPE')]\n",
              "1            1.0  ...       [('hispanic', 'NORP')]\n",
              "2            0.0  ...                           []\n",
              "3            1.0  ...       [('tomorrow', 'DATE')]\n",
              "4            1.0  ...        [('russian', 'NORP')]\n",
              "...          ...  ...                          ...\n",
              "16555        0.0  ...                           []\n",
              "16556        0.0  ...  [('jollof rice', 'PERSON')]\n",
              "16557        0.0  ...                           []\n",
              "16558        0.0  ...                           []\n",
              "16559        0.0  ...      [('california', 'GPE')]\n",
              "\n",
              "[16560 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSK17rrrK2ON",
        "outputId": "53dde34f-03c1-43e7-c437-fc9ffce4b524"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range(0,len(df)):\n",
        "  review = re.sub( '[^a-zA-Z]',' ', df['sentence'][i]) # Remove punctuation marks \n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  # ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words('english')\n",
        "  all_stopwords.remove('not') # Remove the 'not' from stopwords\n",
        "  # review = [ ps.stem(word) for word in review if not word in set(all_stopwords)] # stem each word if not in stopwords\n",
        "  review = ' '.join(review) # join with space\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEIseSpnIG25"
      },
      "source": [
        "corpus = df['tokenized_lemmatized_withstopwords'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj1P1r57MRS-",
        "outputId": "d9e4a81d-a5ef-4a66-bfb8-b6a0fae1f2f3"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"['people', 'live', 'ethiopia', 'people', 'thin', 'good', 'distance', 'run']\"\n",
            " \"['professor', 'hispanic', 'man', 'come', 'illegally']\"\n",
            " \"['schoolgirl', 'walk', 'street', 'target', 'child', 'predator']\" ...\n",
            " \"['bike', 'economical', 'energy', 'efficient', 'mode', 'unk', 'transportation']\"\n",
            " \"['little', 'dog', 'need', 'food', 'park', 'grocery', 'store', 'find', 'food', 'treat']\"\n",
            " \"['california', 'beach', 'morning', 'cool']\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdL7hsZ94Vzy"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = None,ngram_range = (1,1), max_df = 1.0) # Further reduce the tokenized words with upper limit\n",
        "X = cv.fit(corpus)\n",
        "bow = cv.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_88d4j7NYRO",
        "outputId": "8a59372a-048b-4b18-bcff-36da66f553e9"
      },
      "source": [
        "print(\"Vocabulary size: {}\".format(len(X.vocabulary_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 9029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPfCPk0bON3-",
        "outputId": "166c2bd6-4ac7-4950-a2bf-d54d1b49aeb9"
      },
      "source": [
        "print(\"Vocabulary content:\\n {}\".format(X.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary content:\n",
            " {'people': 5881, 'live': 4703, 'ethiopia': 2711, 'thin': 8084, 'good': 3403, 'distance': 2323, 'run': 6854, 'professor': 6287, 'hispanic': 3739, 'man': 4873, 'come': 1578, 'illegally': 3926, 'schoolgirl': 6985, 'walk': 8712, 'street': 7725, 'target': 7960, 'child': 1378, 'predator': 6195, 'likely': 4671, 'fly': 3074, 'cape': 1195, 'verde': 8612, 'flight': 3049, 'tomorrow': 8178, 'bet': 781, 'pretty': 6231, 'lazy': 4581, 'know': 4477, 'russian': 6864, 'love': 4775, 'vodka': 8680, 'mark': 4920, 'musician': 5309, 'school': 6983, 'difficulty': 2236, 'drug': 2432, 'complete': 1628, 'high': 3714, 'person': 5909, 'cameroon': 1162, 'dark': 2009, 'moody': 5230, 'ethiopians': 2713, 'sit': 7304, 'huddle': 3855, 'fire': 3003, 'malnourish': 4866, 'savage': 6945, 'afghanistan': 137, 'citizen': 1436, 'middle': 5098, 'east': 2488, 'eastern': 2490, 'terrorist': 8040, 'girl': 3358, 'store': 7699, 'new': 5425, 'york': 8995, 'short': 7213, 'blonde': 856, 'thick': 8081, 'accent': 41, 'find': 2997, 'country': 1827, 'dance': 2000, 'africa': 139, 'like': 4669, 'place': 6015, 'sophisticated': 7472, 'natural': 5360, 'house': 3844, 'mother': 5253, 'kitchen': 4460, 'set': 7105, 'dinner': 2254, 'prepare': 6210, 'table': 7921, 'poor': 6121, 'dirty': 2265, 'crimean': 1894, 'beautiful': 710, 'location': 4727, 'snobbish': 7417, 'spring': 7575, 'time': 8139, 'job': 4316, 'violent': 8657, 'dangerous': 2004, 'student': 7751, 'drink': 2417, 'heavily': 3676, 'share': 7144, 'border': 915, 'pakistan': 5748, 'muslim': 5310, 'introduce': 4163, 'ghanaian': 3343, 'skinny': 7327, 'physicist': 5958, 'brilliant': 1003, 'world': 8917, 'renowned': 6635, 'mathematician': 4972, 'generally': 3303, 'nerdy': 5410, 'lie': 4655, 'ready': 6506, 'big': 799, 'concert': 1653, 'park': 5787, 'talanted': 7933, 'play': 6034, 'thing': 8085, 'exciting': 2767, 'rich': 6741, 'rock': 6787, 'star': 7622, 'twin': 8335, 'kind': 4448, 'care': 1211, 'woman': 8894, 'psychologist': 6352, 'talk': 7939, 'patient': 5820, 'couch': 1815, 'reference': 6560, 'freudian': 3183, 'concept': 1650, 'tailor': 7929, 'meticulous': 5079, 'believe': 751, 'promote': 6302, 'cookbook': 1769, 'culture': 1945, 'integrate': 4118, 'traditional': 8225, 'food': 3093, 'cook': 1768, 'black': 836, 'sister': 7303, 'work': 8907, 'dad': 1984, 'usually': 8550, 'boring': 919, 'meet': 5031, 'group': 3506, 'ethiopian': 2712, 'impoverish': 3965, 'music': 5307, 'graduate': 3433, 'think': 8086, 'rest': 6698, 'veey': 8591, 'diverse': 2336, 'criminal': 1896, 'instrument': 4110, 'creative': 1877, 'express': 2816, 'husband': 3886, 'mover': 5272, 'large': 4547, 'muscular': 5304, 'teach': 7981, 'rape': 6484, 'murder': 5297, 'hate': 3639, 'quiet': 6424, 'civil': 1440, 'servant': 7098, 'building': 1062, 'downtown': 2394, 'help': 3689, 'city': 1439, 'send': 7074, 'bill': 809, 'resident': 6677, 'feel': 2938, 'public': 6356, 'learn': 4593, 'today': 8166, 'steady': 7652, 'lawn': 4576, 'specialist': 7525, 'encourage': 2609, 'old': 5601, 'fashioned': 2906, 'class': 1451, 'white': 8832, 'office': 5587, 'unhelpful': 8435, 'type': 8339, 'latin': 4559, 'america': 278, 'immigrant': 3945, 'belong': 757, 'date': 2013, 'travel': 8256, 'bar': 643, 'hook': 3808, 'groupie': 3507, 'develop': 2201, 'addiction': 90, 'friend': 3188, 'study': 7755, 'research': 6669, 'effect': 2522, 'poverty': 6168, 'malnutrition': 4868, 'development': 2204, 'survey': 7863, 'ask': 457, 'participant': 5793, 'origin': 5655, 'want': 8723, 'single': 7296, 'special': 7524, 'treatment': 8263, 'electric': 2556, 'acoustic': 69, 'guitar': 3530, 'drunk': 2437, 'chess': 1369, 'player': 6036, 'tonight': 8182, 'smart': 7384, 'supermarket': 7829, 'look': 4750, 'untrustworthy': 8512, 'yesterday': 8987, 'listen': 4694, 'quietly': 6425, 'entire': 2649, 'session': 7104, 'hard': 3613, 'understand': 8405, 'arrive': 428, 'unattractive': 8369, 'door': 2374, 'serve': 7099, 'lamb': 4525, 'flatbread': 3037, 'guest': 3522, 'mind': 5121, 'try': 8306, 'control': 1744, 'situation': 7310, 'asian': 453, 'portugese': 6143, 'colony': 1558, 'totally': 8196, 'isolate': 4220, 'island': 4217, 'wear': 8777, 'polka': 6105, 'dot': 2385, 'dress': 2413, 'bake': 609, 'cookie': 1771, 'pie': 5973, 'day': 2021, 'area': 405, 'speak': 7518, 'spanish': 7509, 'enjoy': 2632, 'family': 2880, 'night': 5445, 'stand': 7615, 'outside': 5680, 'skirt': 7329, 'lot': 4764, 'patience': 5819, 'whon': 8838, 'physics': 5960, 'nerd': 5409, 'visit': 8671, 'probably': 6269, 'outrun': 5679, 'easily': 2487, 'lose': 4760, 'match': 4961, 'catch': 1257, 'cheat': 1345, 'congress': 1687, 'jews': 4302, 'boyfriend': 948, 'able': 13, 'perform': 5891, 'home': 3779, 'band': 628, 'start': 7628, 'excite': 2765, 'build': 1060, 'shed': 7158, 'carpenter': 1229, 'household': 3846, 'hear': 3664, 'tell': 8007, 'coffee': 1537, 'ride': 6747, 'elephant': 2566, 'write': 8945, 'report': 6651, 'animal': 314, 'technical': 7990, 'impractical': 3967, 'unnecessary': 8472, 'advice': 127, 'client': 1479, 'base': 670, 'narrow': 5345, 'text': 8052, 'book': 902, 'solution': 7457, 'little': 4702, 'rel': 6592, 'implication': 3955, 'claim': 1445, 'native': 5358, 'mean': 4999, 'rude': 6846, 'hold': 3761, 'meeting': 5032, 'complain': 1626, 'france': 3148, 'spain': 7507, 'combine': 1577, 'starve': 7632, 'runner': 6856, 'disturb': 2332, 'dream': 2411, 'week': 8790, 'lay': 4579, 'nod': 5461, 'head': 3654, 'soccer': 7428, 'team': 7984, 'sport': 7565, 'field': 2973, 'land': 4532, 'gold': 3396, 'chair': 1305, 'emotion': 2592, 'airport': 190, 'temporarily': 8015, 'close': 1495, 'war': 8724, 'safe': 6886, 'plane': 6022, 'near': 5374, 'russia': 6863, 'constant': 1711, 'turmoil': 8321, 'alena': 211, 'consider': 1703, 'fancy': 2888, 'clothe': 1500, 'expensive': 2797, 'interesting': 4136, 'line': 4682, 'specific': 7528, 'scientist': 6992, 'quality': 6411, 'generous': 3307, 'cut': 1974, 'piece': 5974, 'fabric': 2850, 'measure': 5003, 'tape': 7959, 'neck': 5381, 'college': 1553, 'marry': 4930, 'magic': 4833, 'buy': 1128, 'hire': 3738, 'huge': 3858, 'gruff': 3513, 'neighbor': 5397, 'pedro': 5862, 'alot': 248, 'yell': 8978, 'cooking': 1772, 'desk': 2167, 'conservative': 1701, 'hang': 3597, 'mexican': 5084, 'hairstyle': 3562, 'housewife': 3849, 'recently': 6527, 'attend': 509, 'council': 1819, 'hearing': 3665, 'homeless': 3781, 'problem': 6271, 'liberal': 4647, 'politician': 6103, 'knack': 4468, 'waste': 8752, 'taxpayer': 7977, 'money': 5210, 'petty': 5934, 'bum': 1081, 'need': 5384, 'suit': 7805, 'hem': 3693, 'community': 1606, 'typically': 8342, 'male': 4862, 'education': 2516, 'elite': 2571, 'social': 7430, 'lately': 4556, 'breakup': 977, 'english': 2629, 'warehouse': 8728, 'exemplary': 2779, 'fit': 3019, 'business': 1110, 'fact': 2856, 'gay': 3290, 'body': 878, 'especially': 2697, 'pant': 5770, 'science': 6988, 'apply': 377, 'county': 1830, 'pay': 5836, 'peninsula': 5875, 'smile': 7392, 'stranger': 7712, 'arrest': 427, 'begin': 735, 'lift': 4660, 'burly': 1098, 'forearm': 3103, 'glisten': 3374, 'sweat': 7883, 'fix': 3022, 'god': 3390, 'knit': 4474, 'groomsman': 3496, 'wedding': 8785, 'sure': 7849, 'grocery': 3492, 'miniskirt': 5135, 'halter': 3573, 'europe': 2724, 'friendly': 3189, 'homework': 3788, 'teenager': 8000, 'motivated': 5256, 'young': 8998, 'needle': 5385, 'expect': 2794, 'service': 7101, 'receive': 6524, 'horrendous': 3822, 'michelle': 5094, 'local': 4724, 'post': 6155, 'government': 3418, 'holiday': 3764, 'leave': 4598, 'far': 2892, 'usa': 8542, 'orgy': 5652, 'nice': 5432, 'terrorism': 8039, 'wary': 8743, 'analyze': 298, 'word': 8906, 'expression': 2817, 'nigeria': 5439, 'predominately': 6200, 'enter': 2643, 'greet': 3476, 'secretary': 7042, 'welcome': 8800, 'african': 140, 'abroad': 22, 'disease': 2291, 'bring': 1006, 'alcohol': 208, 'party': 5798, 'tequilla': 8026, 'loud': 4767, 'watch': 8754, 'television': 8006, 'reserve': 6673, 'pride': 6242, 'million': 5117, 'year': 8975, 'golden': 3397, 'coast': 1522, 'west': 8809, 'naked': 5333, 'save': 6948, 'heartache': 3667, 'exboyfriend': 2754, 'skin': 7325, 'tom': 8174, 'longer': 4748, 'tall': 7943, 'son': 7466, 'mom': 5201, 'fret': 3181, 'worry': 8922, 'skip': 7328, 'lunch': 4799, 'use': 8544, 'eat': 2495, 'long': 4747, 'period': 5896, 'volunteer': 8691, 'owe': 5716, 'cool': 1774, 'fast': 2907, 'pleasant': 6048, 'intently': 4129, 'great': 3462, 'compassion': 1614, 'promising': 6301, 'sing': 7292, 'security': 7050, 'officer': 5588, 'definitely': 2086, 'employ': 2601, 'super': 7823, 'step': 7661, 'ahead': 167, 'airplane': 188, 'cold': 1542, 'pamper': 5759, 'daily': 1988, 'spend': 7541, 'bath': 684, 'bomb': 886, 'stage': 7607, 'horrible': 3823, 'life': 4656, 'famous': 2883, 'uneducated': 8417, 'sound': 7483, 'dreamy': 2412, 'thief': 8083, 'dishonest': 2301, 'route': 6833, 'kiss': 4458, 'hug': 3857, 'happy': 3608, 'win': 8860, 'tournament': 8206, 'hour': 3842, 'practice': 6180, 'worker': 8911, 'communist': 1605, 'dumb': 2447, 'submissive': 7776, 'protective': 6327, 'kid': 4437, 'road': 6772, 'dusty': 2461, 'late': 4555, 'opportunity': 5622, 'ring': 6757, 'doorbell': 2375, 'sweaty': 7887, 'droopy': 2427, 'read': 6504, 'appointment': 378, 'swing': 7900, 'sky': 7332, 'restaurant': 6699, 'boat': 874, 'warm': 8732, 'remove': 6630, 'shirt': 7192, 'swig': 7895, 'lady': 4520, 'box': 944, 'strong': 7742, 'meal': 4998, 'tasty': 7968, 'engage': 2624, 'tie': 8127, 'activity': 76, 'color': 1559, 'hunter': 3881, 'gatherer': 3286, 'uber': 8347, 'driver': 2421, 'exotic': 2791, 'looking': 4751, 'final': 2990, 'strategic': 7715, 'brother': 1026, 'shy': 7239, 'army': 423, 'secretive': 7043, 'stop': 7698, 'relocate': 6617, 'market': 4923, 'sell': 7068, 'feed': 2937, 'cross': 1914, 'curler': 1956, 'game': 3257, 'nap': 5336, 'rare': 6490, 'spice': 7545, 'harsh': 3630, 'appearance': 368, 'ghanaians': 3344, 'ghana': 3342, 'primitive': 6249, 'tan': 7952, 'change': 1315, 'oil': 5597, 'car': 1205, 'soul': 7482, 'orleans': 5660, 'jazz': 4269, 'widely': 8844, 'respect': 6686, 'genera': 3300, 'bottle': 929, 'morning': 5238, 'stay': 7649, 'mow': 5275, 'mower': 5276, 'missionary': 5158, 'trip': 8281, 'tent': 8022, 'hot': 3837, 'sand': 6916, 'attack': 506, 'scary': 6970, 'afraid': 138, 'artistic': 439, 'different': 2233, 'way': 8767, 'professional': 6286, 'bachelorette': 585, 'hope': 3815, 'instead': 4101, 'thought': 8100, 'idea': 3907, 'hit': 3745, 'tv': 8327, 'end': 2611, 'sew': 7113, 'mitten': 5171, 'relax': 6601, 'total': 8195, 'pervert': 5920, 'sara': 6924, 'diligent': 2244, 'behave': 739, 'teacher': 7982, 'pet': 5925, 'decide': 2048, 'counter': 1824, 'paper': 5771, 'solve': 7458, 'water': 8757, 'rely': 6618, 'port': 6139, 'isaac': 4206, 'exact': 2747, 'measurement': 5004, 'turtle': 8324, 'secret': 7041, 'pirate': 6004, 'hijack': 3721, 'carry': 1238, 'cute': 1975, 'precocious': 6194, 'normal': 5479, 'crochet': 1907, 'slipper': 7366, 'recess': 6529, 'locate': 4726, 'uptight': 8534, 'difficult': 2235, 'language': 4541, 'second': 7039, 'overweight': 5712, 'course': 1835, 'bully': 1079, 'struggle': 7747, 'economically': 2502, 'nearly': 5376, 'incredibly': 3999, 'strategize': 7717, 'upcoming': 8519, 'competition': 1622, 'bookish': 903, 'skill': 7322, 'open': 5612, 'case': 1245, 'roll': 6799, 'joint': 4328, 'smoke': 7395, 'library': 4650, 'plaid': 6018, 'button': 1126, 'low': 4781, 'salary': 6896, 'story': 7705, 'wild': 8849, 'dog': 2356, 'news': 5427, 'controlling': 1745, 'wife': 8846, 'daughter': 2015, 'highly': 3716, 'intelligent': 4123, 'wait': 8708, 'bus': 1108, 'uniform': 8439, 'praise': 6182, 'everyday': 2736, 'differently': 2234, 'cause': 1269, 'issue': 4228, 'trust': 8302, 'mud': 5280, 'taco': 7924, 'truck': 8295, 'logic': 4732, 'reading': 6505, 'passionate': 5804, 'merely': 5061, 'finish': 3001, 'brace': 951, 'stature': 7647, 'slave': 7347, 'provide': 6341, 'important': 3961, 'source': 7487, 'comfort': 1584, 'troubled': 8289, 'jewish': 4299, 'neurotic': 5422, 'stuff': 7756, 'careless': 1215, 'costly': 1810, 'cover': 1842, 'tropical': 8286, 'rain': 6454, 'forest': 3110, 'loin': 4735, 'cloth': 1499, 'termite': 8028, 'che': 1342, 'blank': 840, 'apartment': 359, 'ritual': 6766, 'roommate': 6816, 'nuclear': 5524, 'weapon': 8776, 'alcoholic': 209, 'wasteland': 8753, 'overly': 5698, 'concerned': 1652, 'chris': 1406, 'academic': 40, 'everybody': 2735, 'bad': 597, 'wartorn': 8742, 'polyglot': 6110, 'crude': 1923, 'away': 568, 'canada': 1171, 'homemaker': 3784, 'smell': 7389, 'pick': 5965, 'let': 4637, 'better': 785, 'speaker': 7519, 'space': 7502, 'department': 2139, 'bear': 702, 'death': 2038, 'judgmental': 4362, 'americans': 281, 'soviet': 7497, 'union': 8451, 'fight': 2978, 'soviets': 7498, 'mujahideen': 5287, 'tough': 8199, 'fighter': 2979, 'persecute': 5905, 'crimea': 1893, 'putin': 6401, 'seat': 7035, 'grow': 3508, 'puzzle': 6403, 'drama': 2402, 'reggie': 6578, 'suffer': 7800, 'starvation': 7631, 'relatable': 6593, 'intense': 4125, 'western': 8810, 'sensibility': 7077, 'nation': 5350, 'brave': 965, 'dmv': 2345, 'bureaucracy': 1089, 'quick': 6421, 'precise': 6192, 'proud': 6336, 'fluently': 3068, 'sugar': 7801, 'daddy': 1985, 'tribe': 8275, 'classmate': 1455, 'goat': 3388, 'meat': 5005, 'amy': 292, 'assume': 481, 'connection': 1692, 'renowne': 6634, 'accomplish': 54, 'summer': 7810, 'break': 973, 'shot': 7217, 'prevent': 6234, 'firefighter': 3005, 'represent': 6654, 'interest': 4134, 'answer': 335, 'phone': 5951, 'caller': 1154, 'push': 6398, 'note': 5503, 'bob': 875, 'dude': 2443, 'extra': 2831, 'glass': 3369, 'present': 6217, 'selfish': 7065, 'noise': 5462, 'groove': 3497, 'shaker': 7133, 'slow': 7376, 'individual': 4014, 'cleanliness': 1465, 'standard': 7616, 'clean': 1462, 'campus': 1169, 'exchange': 2764, 'barely': 659, 'sandy': 6920, 'beach': 697, 'hut': 3890, 'wilderness': 8850, 'basement': 672, 'obviously': 5568, 'clumsy': 1510, 'stupid': 7761, 'drop': 2428, 'pm': 6066, 'lab': 4506, 'experiment': 2800, 'concentrate': 1647, 'armed': 419, 'state': 7636, 'extremely': 2837, 'addict': 89, 'vacation': 8557, 'iceland': 3904, 'converse': 1759, 'sea': 7023, 'shoot': 7203, 'nefarious': 5389, 'character': 1319, 'package': 5730, 'refugee': 6571, 'certainly': 1300, 'malnourished': 4867, 'sunday': 7816, 'haul': 3644, 'correctly': 1797, 'thinking': 8088, 'petite': 5930, 'quickly': 6423, 'clothing': 1501, 'drive': 2420, 'bland': 839, 'skinned': 7326, 'south': 7488, 'sudan': 7794, 'scrawny': 7009, 'swollen': 7903, 'belly': 756, 'fun': 3226, 'hair': 3557, 'neat': 5377, 'pigtail': 5982, 'resolve': 6683, 'charge': 1324, 'dollar': 2360, 'behavior': 741, 'effeminate': 2526, 'talented': 7937, 'elderly': 2553, 'apple': 373, 'juice': 4367, 'nostalgic': 5497, 'age': 148, 'museum': 5305, 'gossip': 3412, 'afternoon': 145, 'climate': 1481, 'despite': 2174, 'accustom': 63, 'weather': 8780, 'weekend': 8791, 'romantic': 6805, 'cheap': 1343, 'turn': 8322, 'manage': 4874, 'finance': 2994, 'texas': 8050, 'jean': 4272, 'tshirt': 8307, 'fear': 2926, 'matronly': 4975, 'refer': 6559, 'female': 2947, 'conversation': 1756, 'effortlessly': 2533, 'sexy': 7123, 'elevator': 2567, 'solo': 7456, 'flute': 3072, 'useless': 8546, 'prescribe': 6215, 'medicine': 5021, 'wonder': 8898, 'primary': 6247, 'export': 2814, 'equator': 2666, 'script': 7015, 'famine': 2881, 'podium': 6072, 'nose': 5494, 'audience': 525, 'sleep': 7354, 'right': 6752, 'raise': 6459, 'hand': 3579, 'answering': 336, 'question': 6420, 'promptly': 6303, 'air': 181, 'breast': 978, 'burst': 1106, 'blouse': 862, 'bend': 763, 'notorious': 5508, 'surfer': 7853, 'johnny': 4325, 'utah': 8552, 'polish': 6095, 'nail': 5331, 'comment': 1593, 'jug': 4365, 'boss': 923, 'terrible': 8032, 'temper': 8010, 'dana': 1999, 'beginning': 736, 'career': 1212, 'disillusion': 2303, 'paycheck': 5837, 'northeast': 5486, 'coastline': 1524, 'north': 5485, 'sort': 7478, 'warfare': 8729, 'mirror': 5143, 'insecure': 4086, 'lipstick': 4688, 'shane': 7138, 'rachel': 6438, 'saturday': 6939, 'familiar': 2879, 'laboratory': 4511, 'equipment': 2667, 'procedure': 6274, 'resilient': 6678, 'charity': 1329, 'month': 5226, 'village': 8647, 'illegal': 3925, 'boot': 910, 'bodegas': 877, 'racist': 6443, 'background': 587, 'freeloader': 3165, 'dancer': 2001, 'furniture': 3236, 'central': 1292, 'french': 3174, 'grassroots': 3453, 'campaign': 1165, 'improve': 3973, 'cat': 1256, 'overall': 5687, 'swear': 7882, 'sailor': 6892, 'contain': 1723, 'argue': 410, 'tend': 8017, 'waiter': 8709, 'trouble': 8288, 'notice': 5505, 'talent': 7935, 'crimeans': 1895, 'immigrate': 3946, 'create': 1875, 'insular': 4112, 'sprawl': 7572, 'network': 5418, 'couple': 1831, 'physic': 5955, 'art': 432, 'health': 3661, 'guess': 3520, 'messy': 5071, 'diamond': 2217, 'trade': 8221, 'brutal': 1038, 'coworker': 1850, 'sick': 7244, 'stew': 7672, 'hike': 3722, 'continent': 1730, 'similar': 7280, 'nigerians': 5441, 'majority': 4854, 'airline': 187, 'reliable': 6607, 'tartar': 7963, 'prefer': 6201, 'wide': 8843, 'variety': 8579, 'mental': 5051, 'topic': 8188, 'red': 6551, 'minivan': 5137, 'contract': 1735, 'responsibility': 6695, 'freckle': 3161, 'spectacle': 7531, 'shop': 7206, 'indian': 4007, 'geographical': 3329, 'standpoint': 7619, 'christian': 1408, 'intelligence': 4122, 'cia': 1420, 'power': 6172, 'hack': 3552, 'election': 2555, 'explain': 2804, 'theory': 8074, 'socially': 7434, 'awkward': 574, 'produce': 6280, 'marathon': 4906, 'caring': 1218, 'item': 4233, 'hungry': 3879, 'happen': 3604, 'ago': 159, 'babysitter': 583, 'stellar': 7659, 'athlete': 496, 'track': 8218, 'helpful': 3691, 'flower': 3065, 'birthday': 825, 'anxiety': 349, 'tracy': 8220, 'laugh': 4566, 'clown': 1504, 'giggly': 3354, 'act': 72, 'silly': 7276, 'afterward': 146, 'sweet': 7892, 'tinker': 8151, 'soldier': 7449, 'spy': 7579, 'hostage': 3833, 'horn': 3820, 'adept': 97, 'piano': 5963, 'craft': 1857, 'bore': 916, 'attract': 518, 'opposite': 5624, 'sex': 7117, 'river': 6770, 'deploy': 2145, 'dirt': 2263, 'sad': 6877, 'nurture': 5532, 'lonely': 4745, 'lovely': 4777, 'swim': 7896, 'tear': 7987, 'urge': 8537, 'ton': 8179, 'participate': 5794, 'jack': 4238, 'alteration': 257, 'italian': 4230, 'price': 6239, 'birthplace': 826, 'father': 2913, 'religious': 6615, 'soon': 7471, 'tho': 8094, 'laid': 4521, 'style': 7764, 'baby': 580, 'reunion': 6719, 'dote': 2386, 'lovingly': 4780, 'draw': 2406, 'picture': 5970, 'image': 3931, 'someday': 7463, 'tourism': 8204, 'industry': 4025, 'hole': 3763, 'flamboyant': 3029, 'unhealthy': 8433, 'originally': 5657, 'immature': 3942, 'statement': 7637, 'punish': 6376, 'bit': 830, 'disorganized': 2310, 'plan': 6021, 'sun': 7813, 'ocean': 5576, 'fishing': 3015, 'swimming': 7898, 'frequently': 3177, 'controversy': 1747, 'habit': 3550, 'process': 6276, 'support': 7839, 'united': 8456, 'states': 7638, 'economic': 2500, 'wosh': 8929, 'ignore': 3920, 'fragile': 3143, 'sticker': 7676, 'vase': 8583, 'alma': 244, 'hey': 3708, 'rigid': 6754, 'snobby': 7418, 'room': 6815, 'direction': 2260, 'russians': 6865, 'consist': 1706, 'rice': 6740, 'law': 4574, 'order': 5641, 'consult': 1719, 'opinion': 5618, 'aged': 149, 'photo': 5952, 'lifetime': 4659, 'worried': 8921, 'weak': 8770, 'town': 8211, 'fisherman': 3014, 'explore': 2810, 'terrain': 8029, 'search': 7028, 'confidence': 1671, 'specialize': 7526, 'degree': 2095, 'load': 4718, 'early': 2482, 'crew': 1890, 'noon': 5475, 'lamp': 4529, 'earn': 2483, 'university': 8459, 'bowl': 939, 'hat': 3638, 'pinstripe': 5998, 'resistant': 6681, 'article': 433, 'kidnap': 4438, 'prisoner': 6259, 'information': 4048, 'guy': 3544, 'brighton': 1001, 'blond': 855, 'population': 6130, 'fine': 2999, 'wine': 8867, 'pass': 5800, 'choose': 1398, 'boycott': 947, 'poll': 6106, 'proably': 6268, 'genius': 3313, 'immediately': 3943, 'grossly': 3500, 'overstate': 5710, 'fame': 2878, 'uncertain': 8376, 'map': 4904, 'minority': 5139, 'poorly': 6122, 'educate': 2514, 'prone': 6304, 'tactic': 7925, 'foreign': 3106, 'relation': 6597, 'pack': 5729, 'studio': 7752, 'beard': 703, 'leather': 4597, 'jacket': 4239, 'block': 853, 'boy': 946, 'wake': 8711, 'obtain': 5566, 'cousin': 1841, 'hostile': 3836, 'asia': 452, 'examine': 2752, 'feeling': 2939, 'civilized': 1443, 'hail': 3556, 'backpack': 588, 'video': 8635, 'compete': 1619, 'return': 6717, 'sung': 7817, 'song': 7467, 'recent': 6526, 'violence': 8656, 'gain': 3249, 'international': 4142, 'attention': 512, 'insurgent': 4116, 'savagery': 6946, 'smelly': 7390, 'excellent': 2757, 'design': 2164, 'fashion': 2904, 'stre': 7722, 'simple': 7283, 'analogy': 294, 'loading': 4719, 'unpleasant': 8481, 'wander': 8719, 'lane': 4539, 'uncare': 8373, 'extraordinary': 2833, 'learned': 4594, 'intructor': 4166, 'club': 1505, 'confident': 1672, 'face': 2851, 'bang': 631, 'grope': 3498, 'offer': 5586, 'chance': 1313, 'advancement': 119, 'master': 4956, 'knowledge': 4479, 'knowledgeable': 4480, 'dweebs': 2468, 'dazzle': 2024, 'heart': 3666, 'throb': 8110, 'slowly': 7377, 'knock': 4475, 'opposition': 5625, 'rook': 6813, 'minute': 5142, 'competitor': 1624, 'housebroken': 3845, 'hardworke': 3619, 'singe': 7293, 'radio': 6449, 'busy': 1118, 'jose': 4339, 'champion': 1311, 'board': 870, 'sense': 7076, 'uncle': 8380, 'accompany': 53, 'takeout': 7932, 'easy': 2492, 'coat': 1525, 'cheerleader': 1351, 'historically': 3743, 'basically': 675, 'eritrean': 2681, 'brunette': 1036, 'tea': 7979, 'britain': 1010, 'vein': 8598, 'eriteria': 2679, 'cathedral': 1258, 'italians': 4231, 'nestle': 5415, 'eritrea': 2680, 'mass': 4950, 'email': 2580, 'thousand': 8102, 'scam': 6957, 'researcher': 6670, 'gather': 3285, 'datum': 2014, 'italy': 4232, 'pasta': 5808, 'syria': 7915, 'grandmother': 3443, 'angry': 313, 'refuse': 6573, 'sandwich': 6919, 'shopkeeper': 7207, 'tooth': 8185, 'charles': 1331, 'company': 1609, 'producer': 6281, 'particular': 5795, 'ukrainian': 8355, 'arrange': 426, 'thug': 8115, 'dish': 2298, 'strange': 7710, 'passenger': 5801, 'entrepreneur': 2653, 'ambitious': 275, 'willing': 8856, 'risk': 6764, 'independently': 4004, 'instinct': 4103, 'artist': 438, 'weird': 8799, 'audition': 527, 'greedy': 3468, 'commander': 1591, 'doctor': 2349, 'drinker': 2418, 'iraq': 4195, 'geographically': 3330, 'iran': 4192, 'corrupt': 1799, 'statistically': 7644, 'norway': 5489, 'ski': 7319, 'bossy': 924, 'policeman': 6093, 'profession': 6285, 'grandfather': 3441, 'chemist': 1363, 'japanese': 4264, 'invent': 4171, 'technology': 7996, 'guitarist': 3531, 'fan': 2885, 'fraternity': 3157, 'gentleman': 3321, 'major': 4853, 'manager': 4876, 'employee': 2602, 'lead': 4585, 'rouse': 6832, 'speech': 7534, 'girlfriend': 3359, 'kill': 4441, 'violate': 8655, 'sharia': 7147, 'software': 7444, 'developer': 2203, 'computer': 1643, 'football': 3098, 'throw': 8112, 'ball': 616, 'score': 6998, 'goal': 3386, 'attractive': 519, 'nowadays': 5518, 'columbian': 1567, 'demand': 2111, 'persian': 5907, 'crazy': 1871, 'ecuador': 2504, 'wealthy': 8775, 'bengali': 768, 'filthy': 2989, 'sierra': 7253, 'leon': 4631, 'salone': 6902, 'blunt': 868, 'surprise': 7858, 'dead': 2028, 'saudi': 6942, 'arabian': 393, 'limit': 4677, 'guilty': 3527, 'broad': 1016, 'shoulder': 7218, 'voice': 8681, 'crowded': 1919, 'cuisine': 1940, 'cultural': 1943, 'history': 3744, 'recall': 6523, 'spaghetti': 7506, 'luigi': 4794, 'deal': 2032, 'arab': 391, 'vietnam': 8639, 'revolt': 6730, 'independence': 4002, 'buzz': 1130, 'nurse': 5530, 'eye': 2844, 'colored': 1561, 'winter': 8872, 'assistant': 478, 'reasonable': 6515, 'expectation': 2795, 'glory': 3382, 'maid': 4842, 'amazing': 270, 'center': 1291, 'address': 96, 'subordinate': 7778, 'bark': 661, 'convenience': 1751, 'geek': 3294, 'number': 5527, 'compute': 1642, 'calculator': 1149, 'advanced': 118, 'formula': 3126, 'complex': 1630, 'obvious': 5567, 'neighborhood': 5398, 'pot': 6160, 'rug': 6848, 'influence': 4043, 'result': 6704, 'tendency': 8018, 'colonial': 1556, 'occupation': 5571, 'force': 3101, 'complicate': 1634, 'sociological': 7436, 'phenomenon': 5942, 'statistic': 7642, 'responsibly': 6697, 'previous': 6235, 'unkempt': 8462, 'delivery': 2108, 'somalia': 7460, 'miss': 5155, 'mommy': 5205, 'floral': 3059, 'apron': 389, 'necessary': 5379, 'program': 6292, 'stick': 7675, 'snob': 7416, 'jordan': 4337, 'resource': 6684, 'zone': 9023, 'temperature': 8013, 'barefoote': 658, 'church': 1418, 'jason': 4266, 'butcher': 1119, 'knife': 4473, 'christmas': 1411, 'towel': 8208, 'oversee': 5709, 'forgiveness': 3117, 'movie': 5273, 'actress': 78, 'iranian': 4193, 'traditionally': 8226, 'westerner': 8811, 'gentle': 3320, 'whiskey': 8829, 'politic': 6099, 'fantastic': 2890, 'tourist': 8205, 'destination': 2178, 'bullfighte': 1075, 'hateful': 3640, 'server': 7100, 'hindi': 3728, 'islamic': 4212, 'tradition': 8224, 'bengal': 767, 'tiger': 8128, 'primarily': 6246, 'india': 4006, 'inside': 4090, 'historian': 3740, 'member': 5041, 'nations': 5356, 'schoolboy': 6984, 'rowdy': 6837, 'qualified': 6409, 'test': 8045, 'tube': 8308, 'neighbours': 5401, 'circus': 1432, 'obese': 5547, 'cab': 1133, 'smartly': 7385, 'blooded': 858, 'rome': 6807, 'vatican': 8586, 'ability': 12, 'delicious': 2104, 'prosecutor': 6320, 'argument': 412, 'attorney': 517, 'witness': 8884, 'exercise': 2780, 'nick': 5434, 'lebanon': 4601, 'multiple': 5293, 'past': 5807, 'partner': 5797, 'meek': 5030, 'obscure': 5557, 'real': 6507, 'strict': 7731, 'contact': 1722, 'embassy': 2584, 'embellish': 2586, 'truth': 8304, 'burka': 1095, 'organized': 5649, 'efficient': 2529, 'suppose': 7842, 'absentminde': 26, 'math': 4969, 'architecture': 402, 'theatre': 8068, 'earth': 2485, 'bed': 717, 'breed': 985, 'reward': 6733, 'shape': 7141, 'reconstruct': 6541, 'periodic': 5897, 'memory': 5047, 'tigris': 8132, 'euphrates': 2721, 'illiterate': 3927, 'iranians': 4194, 'headscarfs': 3660, 'morocco': 5241, 'yemen': 8984, 'dessert': 2177, 'collect': 1548, 'washington': 8750, 'womanizer': 8895, 'awkwardly': 575, 'hospitable': 3829, 'columbia': 1566, 'cartel': 1240, 'plumber': 6059, 'repair': 6638, 'toilet': 8169, 'beater': 708, 'outsource': 5683, 'bangladesh': 632, 'squalor': 7583, 'populate': 6129, 'indigenous': 4011, 'non': 5468, 'conversational': 1757, 'flirt': 3051, 'handsome': 3592, 'bartender': 669, 'evening': 2730, 'tip': 8153, 'philosophy': 5948, 'criticize': 1905, 'decent': 2046, 'root': 6818, 'conquer': 1694, 'mayans': 4985, 'instruction': 4108, 'medal': 5014, 'complicated': 1635, 'twain': 8328, 'gray': 3459, 'fat': 2910, 'display': 2314, 'beaker': 699, 'evil': 2743, 'figure': 2981, 'narco': 5342, 'light': 4662, 'breakfast': 974, 'affectionate': 132, 'adore': 114, 'john': 4323, 'unfriendly': 8430, 'check': 1347, 'lover': 4778, 'rarely': 6491, 'exist': 2786, 'anymore': 352, 'republican': 6662, 'leone': 4633, 'chickpea': 1375, 'corner': 1790, 'norweigan': 5492, 'avenger': 555, 'accurate': 61, 'depiction': 2144, 'thor': 8096, 'outgoing': 5674, 'steak': 7653, 'cleaver': 1471, 'vital': 8676, 'escape': 2692, 'pray': 6184, 'treat': 8262, 'fascinate': 2902, 'small': 7383, 'cluster': 1512, 'arm': 418, 'value': 8571, 'largely': 4548, 'islam': 4210, 'kindly': 4451, 'oppressive': 5629, 'sector': 7046, 'diversity': 2337, 'ethnic': 2714, 'mesopotamian': 5067, 'arabs': 396, 'handicraft': 3584, 'carpet': 1231, 'bathroom': 687, 'honeymoon': 3800, 'gesture': 3338, 'wildly': 8853, 'blind': 850, 'foreigner': 3107, 'racially': 6440, 'profile': 6289, 'married': 4929, 'festive': 2960, 'event': 2731, 'jungle': 4377, 'modern': 5187, 'convince': 1764, 'judge': 4358, 'jury': 4381, 'convict': 1762, 'suspect': 7871, 'innocent': 4075, 'regret': 6583, 'barack': 644, 'obama': 5545, 'president': 6222, 'political': 6100, 'frumpy': 3208, 'ugly': 8352, 'unique': 8452, 'british': 1012, 'vs': 8699, 'american': 280, 'noodle': 5474, 'flour': 3063, 'theu': 8079, 'pollution': 6108, 'agenda': 151, 'objective': 5551, 'cuddle': 1937, 'engineer': 2626, 'firm': 3011, 'infant': 4034, 'pretentious': 6230, 'prince': 6250, 'snobs': 7419, 'hospital': 3830, 'motherly': 5254, 'garbage': 3267, 'thirty': 8092, 'catholic': 1259, 'starving': 7633, 'site': 7306, 'condescend': 1659, 'mary': 4943, 'nile': 5451, 'mosquito': 5248, 'patty': 5831, 'chief': 1377, 'marines': 4918, 'sign': 7259, 'jump': 4373, 'stable': 7601, 'economy': 2503, 'bull': 1069, 'involve': 4184, 'newspaper': 5428, 'grandchild': 3438, 'gym': 3545, 'mba': 4990, 'experience': 2798, 'finally': 2992, 'agree': 160, 'cabinet': 1135, 'slide': 7360, 'signature': 7261, 'crack': 1854, 'countryside': 1829, 'rural': 6859, 'training': 8235, 'pale': 5753, 'safari': 6884, 'chat': 1339, 'rule': 6852, 'lavish': 4572, 'colonize': 1557, 'europeans': 2726, 'practical': 6178, 'engineering': 2627, 'film': 2985, 'dishevel': 2299, 'pace': 5726, 'finding': 2998, 'committee': 1599, 'studious': 7753, 'detail': 2188, 'project': 6296, 'loved': 4776, 'floor': 3058, 'stubborn': 7750, 'die': 2228, 'gun': 3536, 'bunch': 1086, 'prissy': 6261, 'nanny': 5334, 'ignorant': 3919, 'bust': 1115, 'scare': 6964, 'shoe': 7201, 'usual': 8549, 'wave': 8764, 'crowd': 1918, 'det': 2186, 'common': 1600, 'self': 7062, 'starter': 7629, 'fish': 3013, 'summit': 7811, 'notable': 5500, 'opera': 5615, 'singer': 7294, 'mafia': 4829, 'pizza': 6012, 'isis': 4209, 'glide': 3373, 'mix': 5173, 'cocktail': 1529, 'patron': 5828, 'awe': 569, 'venous': 8607, 'bicep': 794, 'bulge': 1065, 'mixed': 5174, 'martial': 4933, 'displaced': 2313, 'misery': 5149, 'crime': 1892, 'settle': 7107, 'intrigue': 4161, 'performer': 5893, 'conventional': 1755, 'makeup': 4858, 'shower': 7225, 'approach': 383, 'suspicious': 7874, 'youth': 8999, 'aggression': 154, 'impoverished': 3966, 'increase': 3997, 'linkedin': 4685, 'title': 8161, 'lack': 4515, 'income': 3992, 'excel': 2756, 'mathematic': 4970, 'socialize': 7432, 'relative': 6599, 'reserved': 6674, 'geisha': 3296, 'afterwork': 147, 'organize': 5648, 'task': 7965, 'obsessive': 5563, 'pat': 5814, 'porch': 6133, 'sight': 7256, 'sewer': 7115, 'beth': 783, 'fighting': 2980, 'assist': 476, 'bow': 938, 'nervous': 5412, 'cruise': 1925, 'caribbean': 1217, 'hunch': 3876, 'cane': 1181, 'biology': 822, 'empire': 2600, 'butt': 1121, 'shady': 7130, 'instigate': 4102, 'harbor': 3612, 'anti': 343, 'sentiment': 7081, 'discuss': 2288, 'trick': 8276, 'meathead': 5007, 'douchebag': 2389, 'include': 3990, 'sushi': 7869, 'tale': 7934, 'courteous': 1837, 'manner': 4890, 'rough': 6827, 'mention': 5052, 'traveller': 8258, 'describe': 2157, 'flee': 3043, 'fall': 2875, 'aggresive': 153, 'chemistry': 1364, 'maxium': 4983, 'punishment': 6378, 'overpay': 5700, 'ineffective': 4027, 'tune': 8313, 'endorse': 2615, 'carve': 1244, 'queen': 6418, 'royal': 6838, 'muscle': 5303, 'mic': 5090, 'extroverte': 2843, 'courtroom': 1840, 'earnest': 2484, 'passion': 5803, 'evidence': 2740, 'prove': 6338, 'snow': 7421, 'customer': 1973, 'casino': 1249, 'beggar': 733, 'opulence': 5635, 'sink': 7297, 'peak': 5850, 'stain': 7608, 'grandpa': 3444, 'scowl': 7003, 'chomp': 1395, 'cigar': 1421, 'payment': 5839, 'effort': 2532, 'list': 4693, 'brat': 964, 'touchdown': 8198, 'jail': 4247, 'dogfighting': 2357, 'reputation': 6664, 'masculine': 4947, 'nature': 5363, 'marijuana': 4915, 'blue': 865, 'aunt': 529, 'eloquently': 2576, 'overseas': 5708, 'bathe': 685, 'deodorant': 2138, 'noisy': 5463, 'classroom': 1456, 'vast': 8584, 'desert': 2161, 'landscape': 4536, 'picturesque': 5972, 'coastal': 1523, 'waterway': 8762, 'shame': 7136, 'blow': 863, 'marvelous': 4939, 'scenery': 6973, 'attentive': 513, 'region': 6580, 'associate': 479, 'protect': 6326, 'add': 87, 'diva': 2334, 'bother': 928, 'autograph': 546, 'seeker': 7056, 'creepy': 1886, 'interaction': 4132, 'pain': 5740, 'bridge': 995, 'mit': 5167, 'legislation': 4616, 'geometry': 3331, 'living': 4708, 'quarter': 6416, 'hardly': 3617, 'round': 6829, 'gang': 3262, 'chart': 1337, 'perfect': 5889, 'loving': 4779, 'responsible': 6696, 'inform': 4046, 'surround': 7862, 'superficial': 7824, 'bimbos': 813, 'susie': 7870, 'chef': 1359, 'muslin': 5312, 'beat': 707, 'spoken': 7559, 'shift': 7178, 'strife': 7734, 'touch': 8197, 'flat': 3036, 'straw': 7720, 'delicacy': 2101, 'silent': 7268, 'stride': 7733, 'folk': 3084, 'attendance': 510, 'gaggle': 3248, 'promise': 6300, 'tax': 7975, 'infrastructure': 4049, 'truly': 8298, 'shock': 7197, 'elect': 2554, 'leader': 4586, 'luxurious': 4806, 'pushy': 6399, 'scenic': 6974, 'manufacturing': 4903, 'portion': 6141, 'owner': 5718, 'navy': 5369, 'squint': 7589, 'product': 6282, 'destruction': 2184, 'throne': 8111, 'spirit': 7551, 'katy': 4411, 'perry': 5904, 'upbeat': 8518, 'enthusiastic': 2647, 'comfortable': 1585, 'roam': 6774, 'tire': 8154, 'explode': 2806, 'admit': 108, 'overcharge': 5690, 'shamelessly': 7137, 'index': 4005, 'aggressive': 155, 'winner': 8870, 'lottery': 4765, 'turban': 8315, 'fair': 2868, 'drank': 2404, 'count': 1822, 'coin': 1539, 'terrorize': 8041, 'interview': 4153, 'emotionally': 2594, 'volatile': 8683, 'wail': 8707, 'axe': 577, 'authority': 542, 'junky': 4380, 'textbook': 8053, 'briefly': 997, 'insignificant': 4091, 'mcdonald': 4993, 'jerk': 4284, 'prison': 6258, 'model': 5184, 'distant': 2324, 'victory': 8634, 'keg': 4416, 'bro': 1014, 'curious': 1953, 'variable': 8577, 'success': 7788, 'immoral': 3948, 'personal': 5911, 'hurt': 3884, 'behalf': 738, 'superior': 7826, 'offend': 5582, 'ceo': 1295, 'asshole': 471, 'kingdom': 4454, 'overrun': 5707, 'muslims': 5311, 'retreat': 6715, 'drinking': 2419, 'womanizing': 8896, 'kitten': 4462, 'neuter': 5423, 'hotbed': 3838, 'train': 8233, 'carpentry': 1230, 'status': 7648, 'eyet': 2847, 'sale': 6897, 'peta': 5926, 'bestfriend': 778, 'stink': 7685, 'century': 1294, 'enforce': 2623, 'neo': 5406, 'warmonger': 8736, 'aid': 174, 'injury': 4068, 'movement': 5271, 'military': 5112, 'dictator': 2224, 'abruptly': 23, 'ra': 6432, 'funny': 3234, 'nickname': 5435, 'golf': 3400, 'bent': 770, 'nera': 5408, 'collar': 1546, 'alta': 254, 'attire': 515, 'casual': 1254, 'concussion': 1657, 'remember': 6623, 'eager': 2475, 'season': 7032, 'athletic': 497, 'monument': 5227, 'decendant': 2045, 'vike': 8645, 'nicely': 5433, 'injure': 4066, 'abuse': 34, 'sexually': 7122, 'assault': 466, 'cast': 1250, 'northern': 5488, 'dentist': 2135, 'anger': 309, 'religion': 6613, 'egyptians': 2540, 'pyramid': 6404, 'discovery': 2285, 'youtube': 9000, 'egypt': 2539, 'tight': 8129, 'publish': 6360, 'relate': 6594, 'free': 3163, 'half': 3565, 'dominant': 2364, 'specie': 7527, 'bell': 754, 'expose': 2815, 'closely': 1496, 'print': 6254, 'swan': 7880, 'browse': 1031, 'archive': 403, 'path': 5817, 'successful': 7789, 'straight': 7708, 'flirty': 3053, 'nasa': 5346, 'calculate': 1146, 'rocket': 6789, 'stuffy': 7758, 'monkey': 5219, 'cap': 1192, 'pose': 6145, 'thumb': 8116, 'migrate': 5102, 'bre': 970, 'stereotypical': 7666, 'gender': 3298, 'norm': 5478, 'confine': 1674, 'debate': 2040, 'asmara': 459, 'capital': 1196, 'tigrinya': 8131, 'surprised': 7859, 'chamber': 1310, 'commerce': 1594, 'host': 3832, 'mixer': 5175, 'gung': 3537, 'ho': 3752, 'motivate': 5255, 'mingle': 5129, 'potential': 6162, 'mythological': 5326, 'homogeneous': 3790, 'playground': 6039, 'distract': 2328, 'awhile': 573, 'menacing': 5049, 'dropout': 2430, 'diarrhea': 2219, 'mildly': 5108, 'peace': 5844, 'agreement': 162, 'constantly': 1712, 'endanger': 2612, 'deadly': 2030, 'forget': 3114, 'metal': 5072, 'insane': 4082, 'sweatshop': 7886, 'mutilate': 5316, 'brand': 963, 'harass': 3611, 'ogle': 5594, 'sweater': 7884, 'vest': 8625, 'halfway': 3566, 'china': 1387, 'benefit': 765, 'doubt': 2388, 'materialistic': 4967, 'action': 73, 'smooth': 7400, 'talker': 7941, 'nonathletic': 5469, 'camel': 1159, 'sneaky': 7412, 'fake': 2873, 'auto': 544, 'hooded': 3805, 'sweatshirt': 7885, 'pipe': 6002, 'uncomfortable': 8384, 'favorite': 2918, 'bicker': 795, 'snipe': 7414, 'transport': 8250, 'facility': 2855, 'behaved': 740, 'sentence': 7080, 'waitress': 8710, 'ravage': 6495, 'buckingham': 1051, 'palace': 5751, 'savanna': 6947, 'race': 6437, 'unsavory': 8495, 'flagpole': 3026, 'logo': 4734, 'saying': 6954, 'pull': 6367, 'speeder': 7536, 'ticket': 8125, 'essential': 2699, 'unjust': 8460, 'revolve': 6732, 'cozy': 1853, 'mild': 5107, 'parent': 5784, 'completely': 1629, 'ruin': 6851, 'advertise': 126, 'cuss': 1970, 'staff': 7606, 'tweed': 8331, 'spear': 7521, 'collection': 1549, 'breadth': 972, 'bowtie': 943, 'mustache': 5314, 'suppress': 7844, 'wealth': 8774, 'mining': 5133, 'underground': 8398, 'deserve': 2162, 'exquisite': 2819, 'seven': 7110, 'attitude': 516, 'release': 6604, 'harden': 3615, 'matter': 4977, 'view': 8641, 'stern': 7668, 'subject': 7772, 'future': 3242, 'captain': 1201, 'varsity': 8580, 'baseball': 671, 'rotc': 6824, 'enrol': 2639, 'honor': 3802, 'ap': 355, 'pharmaceutical': 5936, 'kenya': 4424, 'gangster': 3263, 'outdate': 5669, 'cry': 1932, 'cultured': 1946, 'vibrant': 8631, 'demeanor': 2113, 'powerful': 6174, 'poise': 6080, 'escort': 2694, 'respective': 6690, 'valet': 8565, 'bug': 1058, 'snake': 7407, 'rally': 6463, 'scrub': 7018, 'stethoscope': 7669, 'double': 2387, 'popular': 6128, 'inept': 4028, 'colleague': 1547, 'arrogant': 430, 'moment': 5202, 'burkas': 1096, 'deliver': 2107, 'culturally': 1944, 'carbohydrate': 1207, 'occurrence': 5575, 'underdeveloped': 8395, 'compare': 1611, 'dim': 2246, 'lit': 4696, 'litter': 4701, 'bright': 998, 'bulb': 1064, 'mainly': 4848, 'frim': 3192, 'steal': 7654, 'denmark': 2127, 'norwegian': 5490, 'sadistic': 6881, 'gross': 3499, 'arabic': 395, 'caput': 1203, 'rush': 6860, 'peer': 5867, 'spicy': 7546, 'clog': 1493, 'drain': 2401, 'uncaring': 8375, 'shave': 7155, 'station': 7640, 'doughnut': 2391, 'stickler': 7677, 'cotton': 1813, 'merchandise': 5058, 'suspiciously': 7875, 'secretly': 7044, 'supporter': 7840, 'hanish': 3598, 'faith': 2871, 'commit': 1598, 'join': 4327, 'tattoo': 7972, 'excited': 2766, 'outdoor': 5670, 'charmer': 1335, 'irritate': 4204, 'fluent': 3067, 'overzealous': 5715, 'particularly': 5796, 'cocaine': 1528, 'emotional': 2593, 'starbucks': 7624, 'absolutely': 28, 'sesame': 7103, 'allergic': 235, 'israel': 4223, 'seafood': 7024, 'ancient': 303, 'alive': 225, 'taste': 7967, 'shopping': 7211, 'annoy': 328, 'barrack': 665, 'weigh': 8795, 'properly': 6310, 'term': 8027, 'duty': 2464, 'badge': 598, 'newly': 5426, 'shine': 7185, 'massive': 4955, 'following': 3087, 'si': 7240, 'grimey': 3485, 'heated': 3670, 'candidate': 1177, 'throat': 8109, 'samurai': 6914, 'raw': 6499, 'typing': 8343, 'flip': 3050, 'smoky': 7399, 'spell': 7538, 'apart': 358, 'wrong': 8948, 'heat': 3669, 'safety': 6889, 'descent': 2156, 'ass': 464, 'alternative': 259, 'rebellious': 6519, 'shout': 7220, 'jewelry': 4297, 'genealogy': 3299, 'opening': 5613, 'credit': 1882, 'grubhub': 3511, 'mature': 4980, 'stricken': 7730, 'homemade': 3783, 'protractor': 6335, 'pocket': 6069, 'napkin': 5338, 'polite': 6097, 'airy': 192, 'detach': 2187, 'personality': 5912, 'iv': 4234, 'trial': 8273, 'dressed': 2414, 'sincere': 7289, 'heavy': 3677, 'weight': 8796, 'message': 5069, 'ups': 8527, 'mad': 4822, 'retire': 6712, 'terribly': 8033, 'sam': 6907, 'iq': 4191, 'shroom': 7233, 'main': 4845, 'choice': 1393, 'presentation': 6219, 'marketing': 4924, 'consumer': 1720, 'ad': 83, 'proportion': 6316, 'monetary': 5209, 'pope': 6125, 'sue': 7799, 'salsa': 6904, 'lip': 4687, 'sync': 7911, 'leak': 4591, 'tool': 8184, 'liberia': 4649, 'billionaire': 811, 'sponsor': 7561, 'energy': 2622, 'gulf': 3532, 'marriage': 4928, 'rescue': 6668, 'purpose': 6391, 'mess': 5068, 'exam': 2750, 'command': 1590, 'intimidate': 4156, 'civilian': 1441, 'march': 4908, 'troop': 8284, 'dave': 2016, 'schedule': 6976, 'budget': 1055, 'bored': 917, 'coding': 1535, 'ukraine': 8354, 'destitute': 2181, 'intern': 4139, 'underpaid': 8402, 'oatmeal': 5544, 'paint': 5742, 'annoying': 329, 'addition': 93, 'shorthand': 7216, 'weed': 8788, 'wwii': 8953, 'reprimand': 6659, 'homeland': 3780, 'dancing': 2002, 'flamenco': 3031, 'watching': 8755, 'handle': 3587, 'scammer': 6959, 'spouse': 7571, 'satisfy': 6937, 'form': 3120, 'avacado': 553, 'toast': 8163, 'dorky': 2382, 'weakling': 8773, 'actually': 81, 'dictatorship': 2226, 'embattle': 2585, 'borscht': 922, 'nurturing': 5533, 'busk': 1114, 'court': 1836, 'courageous': 1834, 'arid': 415, 'nourish': 5512, 'grateful': 3454, 'extremist': 2840, 'warlord': 8731, 'toy': 8215, 'depression': 2150, 'barren': 667, 'punishing': 6377, 'eke': 2547, 'existence': 2787, 'sparse': 7516, 'hopefully': 3817, 'appropriate': 384, 'restroom': 6703, 'belch': 748, 'fart': 2900, 'available': 554, 'urinal': 8538, 'nearby': 5375, 'disrespectful': 2319, 'goodbye': 3404, 'bag': 602, 'flow': 3064, 'leftover': 4609, 'anxious': 350, 'vision': 8669, 'focus': 3079, 'screen': 7012, 'row': 6836, 'intoxicate': 4160, 'wobble': 8890, 'occupy': 5573, 'undemocratic': 8393, 'dictatorial': 2225, 'intersect': 4149, 'janet': 4260, 'conversationalist': 1758, 'slutty': 7381, 'perky': 5898, 'european': 2725, 'eurpoe': 2727, 'smug': 7403, 'internet': 4144, 'silicon': 7272, 'valley': 8569, 'electricity': 2559, 'endearing': 2613, 'rinse': 6759, 'ruthless': 6868, 'horrific': 3825, 'reek': 6556, 'havoc': 3646, 'globe': 3378, 'opressed': 5630, 'endzone': 2618, 'bookstore': 905, 'articulate': 434, 'inmate': 4072, 'chemical': 1361, 'tidy': 8126, 'card': 1209, 'cracker': 1855, 'peanut': 5851, 'butter': 1122, 'explosive': 2813, 'wipe': 8874, 'stoner': 7696, 'archaeological': 398, 'vietmanese': 8638, 'alex': 213, 'doll': 2359, 'untruthful': 8513, 'dine': 2251, 'follower': 3086, 'jessica': 4289, 'introvert': 4164, 'saddle': 6879, 'freetown': 3168, 'pioneer': 6001, 'scene': 6972, 'donut': 2372, 'reside': 6675, 'sleazy': 7352, 'accept': 42, 'bribe': 992, 'juggle': 4366, 'profit': 6290, 'angrily': 312, 'trigger': 8277, 'theater': 8067, 'mouthy': 5270, 'inconsiderate': 3995, 'geeky': 3295, 'unable': 8364, 'clam': 1447, 'exaggeration': 2749, 'schizophrenia': 6978, 'lesson': 4636, 'vessel': 8624, 'pun': 6371, 'intend': 4124, 'groundbreaking': 3504, 'goggle': 3394, 'remain': 6619, 'conference': 1667, 'busty': 1117, 'patrol': 5827, 'discriminatory': 2287, 'walkie': 8713, 'talkie': 7942, 'capitol': 1199, 'beirut': 746, 'scarf': 6966, 'wonderful': 8899, 'human': 3864, 'traffic': 8227, 'december': 2044, 'celebrate': 1280, 'aside': 455, 'gyp': 3547, 'invite': 4182, 'pound': 6166, 'chuck': 1416, 'roast': 6775, 'mexico': 5086, 'dry': 2439, 'chip': 1390, 'boarder': 871, 'threaten': 8105, 'sovereignty': 7496, 'malicious': 4863, 'memorable': 5044, 'anime': 317, 'brown': 1028, 'someplace': 7464, 'bombing': 891, 'capture': 1202, 'ransom': 6481, 'plot': 6053, 'martha': 4932, 'neighbourhood': 5400, 'sally': 6899, 'loss': 4762, 'overwork': 5714, 'upset': 8529, 'digestion': 2239, 'garment': 3274, 'buddy': 1054, 'smoking': 7398, 'firecracker': 3004, 'un': 8363, 'trustworthy': 8303, 'vote': 8694, 'loser': 4761, 'entertainment': 2645, 'alluring': 242, 'steep': 7657, 'crook': 1911, 'liar': 4645, 'mechanical': 5011, 'burger': 1092, 'mall': 4865, 'classy': 1457, 'syrian': 7916, 'rat': 6492, 'thank': 8062, 'assertive': 468, 'gogurt': 3395, 'slipperiness': 7367, 'oily': 5598, 'acne': 68, 'later': 4557, 'legislator': 4617, 'accuse': 62, 'stripe': 7738, 'earlier': 2480, 'grandson': 3446, 'authoritarian': 540, 'portray': 6142, 'curly': 1957, 'republic': 6661, 'peaceful': 5845, 'accord': 57, 'arabians': 394, 'automobile': 548, 'hagwallah': 3554, 'prevalent': 6233, 'motorsport': 5260, 'automotive': 549, 'related': 6596, 'pasttime': 5812, 'glad': 3364, 'impatient': 3950, 'awful': 571, 'rip': 6761, 'destroy': 2182, 'barber': 654, 'chore': 1404, 'blood': 857, 'honest': 3797, 'cruel': 1924, 'enemy': 2619, 'devout': 2211, 'swiftly': 7894, 'decorative': 2058, 'garage': 3265, 'disheveled': 2300, 'incarcerated': 3982, 'dental': 2134, 'cost': 1809, 'vain': 8562, 'strap': 7714, 'chest': 1370, 'whine': 8827, 'wash': 8745, 'unclog': 8382, 'garden': 3269, 'lush': 4802, 'green': 3471, 'pearl': 5852, 'medical': 5018, 'afro': 144, 'clear': 1469, 'asleep': 458, 'seminar': 7070, 'roman': 6802, 'snore': 7420, 'polished': 6096, 'strum': 7748, 'bearded': 704, 'wwi': 8952, 'expert': 2801, 'bride': 994, 'groom': 3494, 'zoos': 9026, 'michael': 5091, 'forehead': 3105, 'beer': 729, 'nascar': 5348, 'depend': 2141, 'forever': 3111, 'equation': 2665, 'chalkboard': 1308, 'mutter': 5319, 'erase': 2673, 'hustler': 3889, 'restore': 6700, 'national': 5351, 'separation': 7084, 'joy': 4348, 'dual': 2441, 'mike': 5105, 'scientific': 6990, 'journal': 4344, 'norwegians': 5491, 'nasty': 5349, 'pickle': 5966, 'herring': 3702, 'supervise': 7830, 'handyman': 3596, 'thanksgiving': 8065, 'candy': 1180, 'approve': 385, 'aspire': 463, 'district': 2331, 'press': 6225, 'hill': 3724, 'lobbyist': 4723, 'unathletic': 8367, 'olympic': 5605, 'environment': 2656, 'danger': 2003, 'phenominal': 5943, 'rubberband': 6844, 'greasy': 3461, 'suicide': 7804, 'recruit': 6546, 'mid': 5097, 'outsized': 5682, 'mediocre': 5024, 'remind': 6625, 'assess': 469, 'labor': 4510, 'fiance': 2966, 'yard': 8968, 'recreation': 6545, 'foot': 3097, 'handed': 3582, 'barbarian': 647, 'executive': 2777, 'require': 6666, 'avoid': 561, 'moroccan': 5240, 'nomad': 5464, 'trafficking': 8228, 'punctual': 6375, 'sidewalk': 7250, 'patriotic': 5826, 'guard': 3519, 'unrest': 8491, 'affect': 130, 'nationality': 5354, 'sanitation': 6922, 'continue': 1733, 'plague': 6017, 'tirelessly': 8156, 'correct': 1796, 'document': 2352, 'chinese': 1388, 'mug': 5282, 'dealer': 2033, 'refrigerator': 6570, 'poster': 6158, 'wall': 8714, 'bank': 634, 'calendar': 1150, 'perfume': 5895, 'chalk': 1307, 'rapist': 6487, 'wonderwall': 8900, 'oasis': 5543, 'disgusting': 2297, 'sexist': 7120, 'flirtatious': 3052, 'cynical': 1978, 'saloon': 6903, 'approximately': 387, 'mechanic': 5010, 'tour': 8201, 'robe': 6780, 'lebanese': 4600, 'material': 4965, 'possession': 6152, 'technically': 7992, 'doorstep': 2376, 'actresse': 79, 'role': 6797, 'sexual': 7121, 'favor': 2917, 'mountain': 5263, 'waterfall': 8759, 'oahu': 5541, 'camera': 1160, 'chimpanzee': 1385, 'energetic': 2620, 'creature': 1880, 'pressure': 6226, 'grin': 3486, 'imagine': 3935, 'deck': 2053, 'grasp': 3450, 'graceful': 3425, 'upper': 8523, 'tennis': 8020, 'hawaii': 3647, 'stolid': 7692, 'ancestor': 300, 'rob': 6777, 'sven': 7876, 'lecture': 4604, 'standoffish': 7618, 'decade': 2042, 'unhappy': 8432, 'farsi': 2899, 'hairy': 3563, 'freeze': 3171, 'loyalty': 4785, 'respectful': 6689, 'cancel': 1173, 'director': 2262, 'allow': 240, 'dull': 2445, 'humid': 3868, 'valuable': 8570, 'society': 7435, 'assign': 472, 'plea': 6045, 'coax': 1526, 'defendant': 2077, 'eduador': 2513, 'adjust': 103, 'bathing': 686, 'follow': 3085, 'vehicle': 8597, 'joan': 4314, 'alan': 200, 'magazine': 4831, 'overpopulate': 5701, 'overpopulation': 5702, 'bump': 1082, 'cindy': 1424, 'regularly': 6585, 'mesa': 5066, 'historical': 3742, 'plenty': 6051, 'aromatic': 425, 'square': 7584, 'kilometre': 4445, 'mercede': 5056, 'euro': 2722, 'rudely': 6847, 'outsider': 5681, 'glamorous': 3367, 'overwhelming': 5713, 'opinionated': 5620, 'production': 6283, 'imperialist': 3953, 'feudal': 2963, 'vivid': 8679, 'kung': 4498, 'fu': 3214, 'tlc': 8162, 'minimal': 5132, 'interraction': 4147, 'police': 6092, 'crooked': 1912, 'advantage': 120, 'deed': 2065, 'djibouti': 2344, 'cheese': 1353, 'schoolmate': 6986, 'basketball': 679, 'key': 4428, 'shaped': 7142, 'buddhist': 1053, 'monday': 5208, 'tattooed': 7973, 'penis': 5876, 'monarchy': 5207, 'breeding': 986, 'ground': 3503, 'reschedule': 6667, 'valedictorian': 8564, 'summa': 7809, 'cum': 1947, 'laude': 4565, 'bedevil': 718, 'parch': 5783, 'isreal': 4226, 'beef': 724, 'saudia': 6943, 'oppress': 5627, 'medication': 5019, 'venture': 8608, 'construction': 1718, 'extremism': 2839, 'princess': 6251, 'unfortunate': 8427, 'heretic': 3698, 'response': 6694, 'app': 363, 'exspensive': 2820, 'edge': 2508, 'incompetent': 3994, 'snowy': 7422, 'wintry': 8873, 'condition': 1660, 'skiing': 7321, 'pastime': 5809, 'eighty': 2544, 'crumpet': 1927, 'labonese': 4509, 'don': 2367, 'pseudo': 6348, 'intellectual': 4120, 'admire': 107, 'wish': 8880, 'playful': 6037, 'combination': 1576, 'rosy': 6821, 'dawn': 2019, 'hijabs': 3720, 'militant': 5110, 'letter': 4639, 'swedish': 7890, 'physique': 5961, 'courteously': 1838, 'politely': 6098, 'haired': 3560, 'level': 4640, 'attractiveness': 520, 'bias': 791, 'perfectionist': 5890, 'federal': 2932, 'institution': 4105, 'ferrari': 2956, 'olive': 5603, 'heritage': 3699, 'christianity': 1409, 'christians': 1410, 'recliner': 6535, 'eritreans': 2682, 'friday': 3185, 'chatty': 1340, 'amicable': 286, 'yearly': 8976, 'feature': 2930, 'comfy': 1588, 'running': 6857, 'demonstrate': 2121, 'obsess': 5561, 'appear': 367, 'macho': 4819, 'quen': 6419, 'king': 4453, 'leg': 4610, 'bumpy': 1084, 'killer': 4442, 'dozen': 2396, 'soda': 7440, 'idiot': 3913, 'accessory': 45, 'cigarette': 1422, 'decrepit': 2061, 'assistance': 477, 'odd': 5578, 'hygiene': 3894, 'mayor': 4987, 'cliche': 1477, 'fill': 2984, 'mediterranean': 5026, 'pita': 6008, 'bread': 971, 'hezbollah': 3709, 'sideline': 7249, 'abide': 11, 'sway': 7881, 'slightly': 7362, 'breath': 981, 'lightly': 4667, 'general': 3301, 'description': 2158, 'ignorance': 3918, 'speeding': 7537, 'orange': 5637, 'jumpsuit': 4375, 'rub': 6842, 'bloody': 860, 'plant': 6025, 'tree': 8265, 'yield': 8990, 'fentanyl': 2953, 'organic': 5646, 'polymer': 6111, 'vial': 8628, 'fedora': 2934, 'heisenberg': 3684, 'crop': 1913, 'frog': 3196, 'blame': 838, 'heater': 3671, 'derive': 2152, 'fund': 3229, 'ballerina': 619, 'we': 8769, 'haire': 3559, 'hippy': 3736, 'cafeteria': 1140, 'critic': 1902, 'judgemental': 4360, 'throwing': 8113, 'kicking': 4436, 'explorer': 2811, 'obnoxious': 5554, 'harmony': 3626, 'draft': 2398, 'meticulously': 5080, 'scan': 6960, 'error': 2687, 'manhattan': 4882, 'grade': 3427, 'rambunctious': 6467, 'inhabitable': 4054, 'fussy': 3240, 'invole': 4183, 'direct': 2259, 'cuel': 1939, 'theis': 8071, 'era': 2671, 'penny': 5878, 'pincher': 5992, 'bean': 700, 'refined': 6562, 'faithful': 2872, 'housework': 3850, 'sarah': 6925, 'unsociable': 8499, 'multi': 5290, 'tasker': 7966, 'cow': 1845, 'freezer': 3172, 'hotel': 3840, 'misogynistic': 5152, 'intolerant': 4158, 'fierce': 2974, 'rivalry': 6769, 'sweden': 7889, 'sunlight': 7818, 'validate': 8567, 'calculation': 1148, 'grammar': 3435, 'slimy': 7364, 'scone': 6995, 'biologist': 821, 'supportive': 7841, 'laptop': 4542, 'code': 1533, 'boil': 880, 'pond': 6113, 'exhibit': 2784, 'labrador': 4513, 'swoon': 7904, 'seek': 7055, 'asylum': 491, 'tired': 8155, 'salmon': 6900, 'performance': 5892, 'hum': 3863, 'journey': 4346, 'eclectic': 2499, 'flavorful': 3040, 'texture': 8055, 'atlantic': 499, 'distinguish': 2327, 'berber': 772, 'easter': 2489, 'luck': 4788, 'painter': 5743, 'uk': 8353, 'crystal': 1933, 'vigorous': 8643, 'hide': 3711, 'baptize': 642, 'extreme': 2836, 'procrastinate': 6279, 'incarcerate': 3981, 'imprison': 3970, 'invest': 4175, 'sightsee': 7258, 'japan': 4263, 'predatory': 6196, 'squadron': 7581, 'unsanitary': 8493, 'bureaucrat': 1090, 'accomplished': 55, 'innnocent': 4074, 'recital': 6531, 'uninhabite': 8444, 'venue': 8609, 'likwe': 4673, 'hardship': 3618, 'desolate': 2168, 'marcus': 4909, 'childhood': 1379, 'unused': 8514, 'negative': 5390, 'hallmark': 3568, 'wise': 8878, 'imago': 3936, 'dr': 2397, 'observant': 5558, 'empathetic': 2597, 'ronald': 6809, 'stocky': 7691, 'somebody': 7462, 'naive': 5332, 'radical': 6446, 'analytical': 297, 'award': 566, 'brutish': 1042, 'deceitful': 2043, 'undocumente': 8411, 'longerhair': 4749, 'tribal': 8274, 'hardworking': 3621, 'cake': 1143, 'decision': 2051, 'focused': 3080, 'page': 5738, 'dynamic': 2471, 'homophobic': 3793, 'psychology': 6353, 'average': 556, 'adult': 116, 'nosy': 5499, 'pety': 5935, 'stoudious': 7706, 'surprising': 7860, 'intention': 4127, 'method': 5077, 'wackjobs': 8702, 'nonexistent': 5471, 'object': 5550, 'strike': 7735, 'comforting': 1587, 'thinker': 8087, 'bureaucratic': 1091, 'foundation': 3136, 'ethnocentric': 2717, 'characterize': 1321, 'symptom': 7909, 'opinionate': 5619, 'lawlessness': 4575, 'pink': 5995, 'skilled': 7323, 'combative': 1575, 'regard': 6575, 'incapable': 3979, 'polluted': 6107, 'adjustment': 104, 'overbearing': 5689, 'san': 6915, 'francisco': 3151, 'disrespect': 2318, 'artsy': 440, 'typical': 8341, 'physical': 5956, 'emaciate': 2579, 'pin': 5991, 'alter': 256, 'communism': 1604, 'careful': 1213, 'misinform': 5150, 'stress': 7727, 'trudge': 8296, 'hockey': 3756, 'needy': 5388, 'surprisingly': 7861, 'unheard': 8434, 'ponytail': 6117, 'ethnically': 2715, 'undevloped': 8409, 'witty': 8885, 'clever': 1476, 'slavic': 7350, 'string': 7736, 'carefully': 1214, 'construct': 1717, 'strategy': 7718, 'farmworker': 2897, 'hall': 3567, 'jew': 4292, 'stereotype': 7665, 'moneylender': 5211, 'proper': 6309, 'flakey': 3028, 'essentially': 2700, 'stingy': 7684, 'james': 4257, 'controversial': 1746, 'current': 1959, 'fed': 2931, 'dependable': 2142, 'honorable': 3803, 'abundance': 32, 'policy': 6094, 'feminine': 2948, 'protest': 6330, 'enthusiast': 2646, 'girly': 3360, 'compassionate': 1615, 'outspoken': 5684, 'publication': 6357, 'frank': 3152, 'lifelong': 4657, 'lifestyle': 4658, 'harvard': 3632, 'cellphone': 1287, 'embark': 2581, 'educated': 2515, 'competent': 1621, 'portuguese': 6144, 'uncivilized': 8379, 'unreliable': 8489, 'politically': 6101, 'housekeeper': 3847, 'attribute': 521, 'record': 6542, 'shiftless': 7179, 'kick': 4434, 'certain': 1299, 'reckless': 6533, 'decison': 2052, 'maker': 4856, 'snarky': 7409, 'independent': 4003, 'talente': 7936, 'regressive': 6582, 'traveler': 8257, 'volcanic': 8684, 'vert': 8620, 'hardworker': 3620, 'breakout': 976, 'rebel': 6518, 'adorable': 113, 'gameplay': 3259, 'teaching': 7983, 'normally': 5480, 'creation': 1876, 'methodical': 5078, 'opponent': 5621, 'somber': 7461, 'bind': 816, 'calm': 1156, 'proclaim': 6278, 'democrat': 2115, 'chose': 1405, 'warzone': 8744, 'mannerism': 4894, 'inhabit': 4053, 'vibe': 8629, 'ease': 2486, 'blush': 869, 'hopeful': 3816, 'hurricane': 3883, 'georgia': 3333, 'informal': 4047, 'frail': 3145, 'silhouette': 7270, 'illuminate': 3928, 'sensual': 7079, 'candle': 1179, 'kelly': 4417, 'eccentric': 2498, 'writing': 8947, 'shortage': 7214, 'inequality': 4029, 'gardener': 3270, 'unsure': 8503, 'needlepoint': 5386, 'extremelyrich': 2838, 'slap': 7341, 'crafty': 1860, 'broke': 1019, 'afford': 135, 'unintelligent': 8447, 'capable': 1194, 'docile': 2347, 'distinguised': 2326, 'realize': 6510, 'discover': 2284, 'flood': 3056, 'storm': 7702, 'honestly': 3798, 'lifting': 4661, 'judy': 4364, 'instability': 4097, 'presidential': 6223, 'visitor': 8672, 'demanding': 2112, 'unknown': 8465, 'bedrock': 719, 'aloof': 247, 'gig': 3352, 'horny': 3821, 'outfit': 5672, 'flashy': 3034, 'educator': 2518, 'inspire': 4096, 'position': 6148, 'notion': 5507, 'grunt': 3514, 'nag': 5328, 'aboout': 18, 'alpha': 250, 'inventive': 4174, 'mathematicians': 4973, 'algebra': 218, 'frigid': 3191, 'uncouth': 8388, 'replace': 6643, 'faucet': 2915, 'psychic': 6350, 'professorial': 6288, 'convey': 1761, 'rainy': 6458, 'geurilla': 3340, 'effective': 2523, 'nomadic': 5465, 'clueless': 1507, 'pork': 6135, 'chop': 1399, 'coder': 1534, 'overprice': 5703, 'residence': 6676, 'mysterious': 5323, 'lackadaisical': 4516, 'fiddle': 2970, 'selfcentere': 7063, 'contrast': 1741, 'descend': 2154, 'brawny': 967, 'restrict': 6701, 'sassy': 6928, 'author': 539, 'misogynist': 5151, 'presence': 6216, 'teammate': 7985, 'exceptionally': 2760, 'gorgeous': 3410, 'terror': 8038, 'fiery': 2976, 'peole': 5880, 'gentlement': 3322, 'strive': 7740, 'understanding': 8406, 'argumentative': 413, 'pour': 6167, 'broadway': 1017, 'doze': 2395, 'repeatedly': 6641, 'antisocial': 348, 'vindictive': 8652, 'expressive': 2818, 'capability': 1193, 'dedicated': 2063, 'delicately': 2103, 'sharp': 7150, 'aggressively': 156, 'prosecute': 6319, 'elitist': 2572, 'baller': 618, 'turnover': 8323, 'rate': 6494, 'inferior': 4037, 'powerless': 6175, 'agression': 163, 'grave': 3455, 'polygamous': 6109, 'ingredient': 4052, 'attene': 511, 'chopping': 1402, 'shockingly': 7199, 'unusually': 8516, 'workaholic': 8908, 'antiquated': 346, 'resolution': 6682, 'empathy': 2598, 'famished': 2882, 'directly': 2261, 'arabia': 392, 'solitary': 7455, 'hopeless': 3818, 'outlook': 5677, 'farmer': 2895, 'wow': 8933, 'fascist': 2903, 'sociable': 7429, 'cure': 1952, 'cancer': 1174, 'headache': 3655, 'squad': 7580, 'factory': 2858, 'beloved': 758, 'scholar': 6980, 'ukrainians': 8356, 'africian': 143, 'justice': 4383, 'ambition': 274, 'paris': 5786, 'chic': 1371, 'persuasive': 5915, 'musical': 5308, 'repressive': 6658, 'technique': 7993, 'dope': 2379, 'gaming': 3261, 'impression': 3968, 'lookout': 4752, 'inevitable': 4030, 'lisper': 4692, 'pompous': 6112, 'thankfully': 8064, 'belief': 749, 'tiny': 8152, 'ancestry': 301, 'troublemak': 8290, 'backward': 592, 'ally': 243, 'mohammed': 5192, 'official': 5589, 'currency': 1958, 'subjugated': 7773, 'citizenry': 1437, 'agressive': 164, 'examination': 2751, 'leadership': 4587, 'distribute': 2330, 'curropt': 1961, 'strength': 7726, 'mindset': 5124, 'industrious': 4023, 'initial': 4063, 'gregarious': 3478, 'unit': 8454, 'cordial': 1785, 'mischevious': 5145, 'provocatively': 6343, 'adventorous': 122, 'deceptive': 2047, 'winded': 8862, 'lame': 4528, 'adventure': 123, 'ivy': 4235, 'league': 4590, 'beefy': 725, 'discernible': 2279, 'mercedes': 5057, 'defense': 2079, 'onslaught': 5611, 'monsoon': 5224, 'leading': 4588, 'platoon': 6032, 'brute': 1041, 'inner': 4073, 'circle': 1427, 'knee': 4469, 'systemic': 7919, 'racism': 6442, 'timmy': 8146, 'shrewd': 7229, 'timely': 8140, 'fromm': 3197, 'unemotional': 8418, 'coldhearte': 1543, 'stair': 7610, 'workout': 8915, 'intellegant': 4121, 'megalomaniacal': 5033, 'slippery': 7368, 'networking': 5419, 'norweigans': 5493, 'famously': 2884, 'egotistical': 2537, 'timothy': 8147, 'atheltic': 494, 'abusive': 36, 'fjord': 3024, 'gloomy': 3380, 'introverted': 4165, 'brainy': 961, 'teenage': 7999, 'inquire': 4080, 'latinos': 4562, 'historic': 3741, 'mama': 4870, 'mansplaining': 4898, 'grass': 3451, 'yellow': 8981, 'incident': 3987, 'surgery': 7857, 'mans': 4896, 'evolve': 2745, 'murderer': 5298, 'haughty': 3643, 'parliament': 5789, 'discussion': 2289, 'subreddit': 7779, 'dialogue': 2216, 'insatiable': 4083, 'spread': 7574, 'gary': 3275, 'spoil': 7557, 'africans': 142, 'mistreat': 5162, 'operation': 5617, 'screw': 7013, 'publicize': 6358, 'hardcore': 3614, 'lawyer': 4578, 'druggie': 2433, 'perserverent': 5906, 'savvy': 6952, 'launch': 4567, 'dealing': 2034, 'actor': 77, 'personable': 5910, 'clerical': 1473, 'playing': 6040, 'challenge': 1309, 'corruption': 1800, 'governance': 3417, 'visionary': 8670, 'thorough': 8098, 'repressed': 6657, 'depress': 2148, 'catty': 1265, 'achievement': 66, 'cheating': 1346, 'obnoxiously': 5555, 'druglord': 2434, 'stinky': 7686, 'assignment': 473, 'naughty': 5364, 'chaotic': 1318, 'height': 3682, 'uncivilize': 8378, 'bible': 793, 'spending': 7542, 'reality': 6509, 'ballroom': 622, 'install': 4099, 'supply': 7835, 'gas': 3276, 'influential': 4044, 'contentious': 1728, 'obediant': 5546, 'adhere': 99, 'slanteye': 7340, 'outgo': 5673, 'ostentatious': 5665, 'boarish': 872, 'colorful': 1562, 'ham': 3574, 'hero': 3700, 'talkative': 7940, 'plain': 6019, 'sake': 6894, 'celebration': 1281, 'storekeeper': 7700, 'twice': 8333, 'appreciate': 379, 'businesslike': 1111, 'columbians': 1568, 'enduring': 2617, 'obstacle': 5565, 'pair': 5745, 'naan': 5327, 'hummus': 3873, 'principle': 6253, 'respond': 6692, 'consumption': 1721, 'desire': 2166, 'rogue': 6795, 'thesis': 8077, 'inquisitive': 4081, 'label': 4507, 'outcome': 5668, 'ankle': 320, 'skull': 7330, 'deadline': 2029, 'extravagant': 2835, 'mood': 5229, 'update': 8520, 'crisis': 1898, 'acquaintance': 70, 'worth': 8926, 'greenlit': 3475, 'relationship': 6598, 'artwork': 441, 'lively': 4704, 'astounding': 485, 'occur': 5574, 'rider': 6748, 'bugridden': 1059, 'meak': 4997, 'confused': 1682, 'unrelenting': 8488, 'shifty': 7180, 'towelhead': 8209, 'bond': 893, 'sibling': 7243, 'navigate': 5368, 'perpetrator': 5902, 'terriost': 8037, 'unnoticed': 8473, 'workplace': 8916, 'helper': 3690, 'sheltered': 7167, 'underqualifie': 8403, 'bloodthirsty': 859, 'regime': 6579, 'fragrant': 3144, 'unknowable': 8464, 'evident': 2741, 'taxi': 7976, 'infamous': 4033, 'conflict': 1677, 'counselor': 1821, 'percentage': 5886, 'instruct': 4107, 'dan': 1998, 'plumberslooklikesupermario': 6060, 'brillant': 1002, 'california': 1152, 'progressive': 6294, 'pottery': 6165, 'troublemaker': 8291, 'holocaust': 3773, 'piracy': 6003, 'feeble': 2936, 'simply': 7286, 'scared': 6965, 'diet': 2231, 'buff': 1056, 'hulking': 3861, 'frame': 3146, 'moist': 5194, 'vary': 8581, 'wellread': 8806, 'knowledgable': 4478, 'charismatic': 1327, 'sloppy': 7373, 'terrorized': 8042, 'lous': 4772, 'quarell': 6415, 'jane': 4259, 'rival': 6768, 'warlike': 8730, 'true': 8297, 'poeple': 6075, 'kamikaze': 4394, 'pilot': 5989, 'harry': 3629, 'textile': 8054, 'fee': 2935, 'investment': 4178, 'unusual': 8515, 'isolationist': 4221, 'passive': 5805, 'curry': 1962, 'crackhead': 1856, 'deny': 2137, 'parole': 5791, 'sensitive': 7078, 'unstable': 8502, 'authoritative': 541, 'prank': 6183, 'division': 2340, 'uproar': 8526, 'backwards': 593, 'biased': 792, 'laborer': 4512, 'contribution': 1743, 'awfully': 572, 'paula': 5833, 'stone': 7694, 'maintain': 4850, 'sewage': 7114, 'respectable': 6687, 'gambler': 3256, 'lusty': 4804, 'amaze': 268, 'listener': 4695, 'unsustainable': 8504, 'hookah': 3809, 'locker': 4729, 'belligerent': 755, 'possible': 6153, 'mecca': 5009, 'yummy': 9005, 'naggy': 5329, 'relevant': 6606, 'bookworm': 906, 'camp': 1164, 'redneck': 6555, 'walmart': 8718, 'voluptuous': 8692, 'deer': 2071, 'experienced': 2799, 'bike': 805, 'unanimously': 8366, 'lock': 4728, 'pursuit': 6397, 'mannerable': 4891, 'mountainous': 5264, 'festivity': 2961, 'airbnb': 182, 'unclean': 8381, 'province': 6342, 'undeveloped': 8408, 'overbear': 5688, 'comprise': 1640, 'coward': 1846, 'freindly': 3173, 'atmosphere': 501, 'commend': 1592, 'loyal': 4784, 'edwards': 2520, 'rage': 6451, 'immigration': 3947, 'xenophobic': 8957, 'childish': 1380, 'ex': 2746, 'irrogant': 4205, 'verypoor': 8622, 'skater': 7316, 'agreeable': 161, 'moon': 5231, 'mission': 5157, 'caucasian': 1266, 'seriousness': 7096, 'yardwork': 8969, 'considerate': 1705, 'constituent': 1715, 'disingenuine': 2304, 'immensely': 3944, 'critical': 1903, 'unfair': 8423, 'becuase': 716, 'harrowing': 3628, 'troublesome': 8292, 'dedication': 2064, 'random': 6476, 'mistreated': 5163, 'st': 7595, 'thomas': 8095, 'wrist': 8944, 'ponte': 6115, 'cosidere': 1804, 'encounter': 2608, 'tag': 7927, 'culinary': 1941, 'okay': 5600, 'hippie': 3735, 'politican': 6102, 'foolish': 3095, 'stimulating': 7682, 'esteem': 2704, 'aspect': 460, 'bespecale': 776, 'panicked': 5768, 'pediatric': 5856, 'ward': 8726, 'deem': 2066, 'unlawful': 8466, 'indecent': 4000, 'funding': 3231, 'flask': 3035, 'legal': 4612, 'chain': 1303, 'gifted': 3351, 'nourished': 5513, 'fridge': 3186, 'lean': 4592, 'doorway': 2377, 'sip': 7299, 'amenity': 277, 'booming': 908, 'flock': 3055, 'morbidly': 5235, 'deaf': 2031, 'harmonica': 3625, 'unkind': 8463, 'eschew': 2693, 'freud': 3182, 'contemporary': 1725, 'cognitive': 1538, 'behavioral': 742, 'therapy': 8076, 'worldwide': 8919, 'ethnicity': 2716, 'advance': 117, 'comfortably': 1586, 'tourisim': 8203, 'preferred': 6204, 'likable': 4668, 'sanitary': 6921, 'agent': 152, 'legally': 4614, 'addis': 92, 'ababa': 2, 'assimilate': 474, 'classical': 1453, 'warmly': 8735, 'fry': 3212, 'verse': 8618, 'bulky': 1068, 'constitution': 1716, 'handy': 3595, 'rugged': 6850, 'robust': 6786, 'observation': 5559, 'welcoming': 8801, 'productive': 6284, 'access': 44, 'renovate': 6633, 'brush': 1037, 'physically': 5957, 'sloppily': 7372, 'maria': 4911, 'clinic': 1486, 'rise': 6762, 'nobility': 5457, 'fantasy': 2891, 'orientation': 5654, 'outstanding': 5685, 'fitting': 3021, 'sewing': 7116, 'machine': 4818, 'gamble': 3255, 'fulfil': 3222, 'tranquil': 8240, 'hoist': 3759, 'spiritual': 7552, 'kickboxer': 4435, 'styling': 7765, 'cart': 1239, 'reach': 6500, 'shelf': 7163, 'teen': 7998, 'whilst': 8825, 'interested': 4135, 'exceptional': 2759, 'mail': 4843, 'psychoanalyze': 6351, 'depressed': 2149, 'fernando': 2954, 'whatsoever': 8817, 'brazil': 969, 'incredible': 3998, 'prominent': 6298, 'steadfastly': 7651, 'generously': 3308, 'concern': 1651, 'foresight': 3108, 'palm': 5756, 'marvel': 4938, 'fantasize': 2889, 'november': 5516, 'june': 4376, 'alright': 252, 'democracy': 2114, 'enterprise': 2644, 'dominate': 2365, 'responder': 6693, 'beverage': 788, 'mannere': 4892, 'departure': 2140, 'mini': 5130, 'heel': 3679, 'slim': 7363, 'stock': 7689, 'broker': 1021, 'cell': 1286, 'macaroni': 4814, 'hamburger': 3576, 'digger': 2240, 'perception': 5887, 'rethink': 6711, 'memorabilia': 5043, 'daycare': 2022, 'slob': 7370, 'baggy': 604, 'shipping': 7189, 'problom': 6272, 'technological': 7994, 'thread': 8103, 'gigantic': 3353, 'wrestling': 8941, 'prepared': 6211, 'skate': 7314, 'workman': 8914, 'mistake': 5161, 'known': 4481, 'roofer': 6812, 'retail': 6705, 'pharmacy': 5937, 'payer': 5838, 'justified': 4384, 'spa': 7501, 'corps': 1794, 'pretend': 6229, 'entirely': 2650, 'heterosexual': 3705, 'janitor': 4261, 'competitive': 1623, 'alleviate': 237, 'hunger': 3878, 'richness': 6743, 'complexity': 1632, 'flavor': 3039, 'loner': 4746, 'glacial': 3363, 'novelty': 5515, 'showcase': 7224, 'vegetarian': 8595, 'precaution': 6190, 'slight': 7361, 'abstain': 29, 'stationary': 7641, 'pool': 6119, 'housing': 3851, 'quit': 6428, 'acclimate': 48, 'positive': 6150, 'robbery': 6779, 'calmly': 1157, 'relaxed': 6602, 'shore': 7212, 'selection': 7060, 'priest': 6244, 'significantly': 7264, 'hunt': 3880, 'moose': 5232, 'hobby': 3754, 'samaritan': 6909, 'plausible': 6033, 'explanation': 2805, 'absence': 24, 'extraterrestrial': 2834, 'tibet': 8122, 'mississippi': 5159, 'banker': 635, 'masterpiece': 4957, 'pursue': 6396, 'recite': 6532, 'poem': 6074, 'thunderous': 8118, 'applause': 372, 'orient': 5653, 'barbel': 652, 'manicure': 4885, 'plump': 6062, 'cheerful': 1349, 'asians': 454, 'refamiliarize': 6558, 'academia': 39, 'difference': 2232, 'genuinely': 3328, 'hospitality': 3831, 'fortune': 3132, 'pianist': 5962, 'maybe': 4986, 'retirement': 6713, 'jeweler': 4295, 'structure': 7745, 'phd': 5939, 'breastfeed': 980, 'exactly': 2748, 'tower': 8210, 'skyscraper': 7335, 'bountiful': 935, 'harvest': 3633, 'giant': 3348, 'baking': 613, 'aware': 567, 'global': 3376, 'albert': 205, 'lavishly': 4573, 'nutrition': 5535, 'sge': 7124, 'paternal': 5816, 'shake': 7132, 'reunited': 6721, 'nonetheless': 5470, 'safely': 6888, 'injustice': 4069, 'rugby': 6849, 'vegan': 8592, 'atheist': 492, 'prototype': 6334, 'tesla': 8044, 'sowing': 7499, 'dependency': 2143, 'millionaire': 5118, 'glitter': 3375, 'metropolis': 5081, 'flexible': 3047, 'vegetable': 8594, 'karate': 4400, 'transform': 8243, 'damage': 1992, 'wholesome': 8837, 'blare': 842, 'inhabitant': 4055, 'rish': 6763, 'buisness': 1063, 'growth': 3510, 'sedan': 7051, 'sumo': 7812, 'wrestler': 8940, 'pilaw': 5984, 'supper': 7832, 'beauty': 712, 'charm': 1334, 'cheeseburger': 1354, 'electromagnetism': 2560, 'gravity': 3457, 'hourly': 3843, 'saas': 6872, 'management': 4875, 'billion': 810, 'ipo': 4189, 'nyse': 5540, 'naturally': 5362, 'quarterback': 6417, 'luxury': 4807, 'condo': 1663, 'birth': 824, 'andrew': 306, 'mobster': 5181, 'jersey': 4287, 'unconcerned': 8385, 'relatively': 6600, 'varied': 8578, 'scoff': 6994, 'legitimate': 4620, 'notch': 5502, 'cafe': 1139, 'window': 8864, 'input': 4079, 'subtle': 7783, 'hint': 3733, 'cologne': 1554, 'benevolent': 766, 'latino': 4561, 'nobel': 5455, 'prize': 6266, 'welfare': 8802, 'seldom': 7059, 'recognition': 6537, 'smartphone': 7386, 'wrinkle': 8943, 'scruffy': 7019, 'accountant': 60, 'contradict': 1737, 'reveal': 6722, 'mathematical': 4971, 'homely': 3782, 'workforce': 8912, 'mac': 4813, 'staple': 7621, 'sausage': 6944, 'temperate': 8012, 'resourceful': 6685, 'programmer': 6293, 'silverware': 7278, 'plate': 6029, 'badly': 599, 'fraud': 3158, 'healthy': 3663, 'eventual': 2733, 'cosmopolitan': 1807, 'apology': 362, 'pluarlistic': 6056, 'damascus': 1993, 'relic': 6609, 'artifact': 435, 'seemingly': 7057, 'embarrase': 2582, 'refute': 6574, 'incapacitate': 3980, 'breathtaking': 984, 'involved': 4185, 'soft': 7442, 'rehabilitation': 6587, 'sleeveless': 7357, 'ideally': 3909, 'loaf': 4720, 'technicality': 7991, 'mankind': 4888, 'sible': 7242, 'foolishly': 3096, 'phrase': 5954, 'tease': 7988, 'erudite': 2688, 'theology': 8073, 'astrophysic': 489, 'activist': 75, 'fatigue': 2914, 'energetically': 2621, 'dea': 2027, 'generation': 3304, 'accommodating': 50, 'buying': 1129, 'avocados': 560, 'guacamole': 3518, 'urban': 8536, 'businessman': 1112, 'reform': 6565, 'custom': 1971, 'affluent': 134, 'artisan': 437, 'silver': 7277, 'freedom': 3164, 'rampant': 6472, 'gentrification': 3325, 'autonomy': 550, 'teetotaler': 8003, 'warning': 8738, 'agency': 150, 'formal': 3121, 'crass': 1867, 'embrace': 2587, 'modest': 5188, 'penthouse': 5879, 'suite': 7807, 'marine': 4917, 'humility': 3871, 'broken': 1020, 'bitcoin': 832, 'novel': 5514, 'serviceman': 7102, 'nightclub': 5446, 'lamborghini': 4526, 'worship': 8924, 'allah': 226, 'filmmaking': 2986, 'drag': 2399, 'hearted': 3668, 'sronger': 7592, 'doctoral': 2350, 'dissertation': 2322, 'expertise': 2802, 'quantum': 6413, 'potato': 6161, 'prosperous': 6322, 'tech': 7989, 'innovation': 4076, 'entrepreneurship': 2654, 'loan': 4721, 'detain': 2189, 'fruity': 3207, 'environmental': 2657, 'stewardship': 7673, 'climbing': 1484, 'instructor': 4109, 'interact': 4131, 'daytime': 2023, 'sag': 6890, 'builder': 1061, 'phobia': 5949, 'granddaughter': 3439, 'amazed': 269, 'neuroscientist': 5421, 'squeaky': 7586, 'purchase': 6382, 'rv': 6869, 'intent': 4126, 'ussually': 8548, 'reason': 6514, 'mercy': 5059, 'stabilize': 7600, 'delicate': 2102, 'pennsylvania': 5877, 'esl': 2696, 'insurance': 4115, 'conscientious': 1695, 'hierarchical': 3713, 'obey': 5549, 'learning': 4595, 'mouse': 5266, 'bonus': 898, 'karma': 4404, 'currently': 1960, 'donate': 2369, 'combat': 1573, 'veteran': 8627, 'employment': 2604, 'tattere': 7971, 'repress': 6656, 'preserve': 6221, 'wlel': 8886, 'rebuild': 6522, 'forklift': 3119, 'relaxing': 6603, 'futuristic': 3243, 'tolerant': 8172, 'limitiation': 4678, 'stanford': 7620, 'copy': 1782, 'glorious': 3381, 'vibrancy': 8630, 'diligently': 2245, 'trash': 8254, 'sorter': 7479, 'manufacture': 4901, 'olympics': 5606, 'virtual': 8663, 'celebratory': 1282, 'successfully': 7790, 'healthcare': 3662, 'legislature': 4618, 'unfaze': 8426, 'democratic': 2116, 'easygoe': 2493, 'succeed': 7787, 'humor': 3875, 'seriously': 7095, 'animosity': 318, 'exception': 2758, 'fruit': 3205, 'peculiar': 5853, 'oftentime': 5593, 'perspective': 5914, 'beautifully': 711, 'pristine': 6262, 'financial': 2995, 'backer': 586, 'ardent': 404, 'nationalist': 5353, 'forrest': 3127, 'ballad': 617, 'battle': 692, 'accomplishment': 56, 'jewellery': 4296, 'pinnacle': 5996, 'range': 6478, 'scholarship': 6982, 'congratulate': 1684, 'concious': 1654, 'aback': 3, 'meatball': 5006, 'unlike': 8467, 'yoga': 8992, 'crafting': 1858, 'baron': 663, 'goverment': 3415, 'undocumented': 8412, 'wrestle': 8939, 'lesbian': 4634, 'adopt': 111, 'wmd': 8887, 'deft': 2089, 'greatly': 3463, 'farm': 2894, 'cattle': 1264, 'toothpick': 8187, 'charming': 1336, 'wardrobe': 8727, 'khaki': 4431, 'groundwork': 3505, 'inher': 4058, 'freetime': 3167, 'beg': 732, 'humble': 3867, 'supplier': 7834, 'colombia': 1555, 'example': 2753, 'risky': 6765, 'accident': 46, 'blog': 854, 'multilingual': 5292, 'doctorate': 2351, 'designer': 2165, 'ferocious': 2955, 'elementary': 2565, 'remote': 6627, 'curve': 1967, 'melt': 5039, 'redemptive': 6553, 'absolute': 27, 'buck': 1048, 'recipient': 6530, 'ponder': 6114, 'preference': 6203, 'nuance': 5523, 'gamer': 3260, 'curteous': 1966, 'flag': 3025, 'lgbtqia': 4643, 'prior': 6257, 'sneaker': 7411, 'iphone': 4188, 'washboard': 8746, 'ab': 1, 'bulging': 1066, 'victim': 8633, 'backyard': 594, 'moral': 5234, 'ethical': 2710, 'assessment': 470, 'toughen': 8200, 'bubbly': 1047, 'tuxedo': 8326, 'lbgt': 4583, 'plumbing': 6061, 'menu': 5053, 'scalpel': 6956, 'accommodate': 49, 'active': 74, 'mediterranian': 5027, 'utensil': 8553, 'transgend': 8244, 'equality': 2663, 'gpa': 3421, 'firendly': 3008, 'happily': 3606, 'vow': 8695, 'complexion': 1631, 'emily': 2590, 'petra': 5931, 'punch': 6372, 'harder': 3616, 'admittance': 109, 'salad': 6895, 'sunbathe': 7814, 'bikini': 807, 'gift': 3350, 'nephew': 5407, 'publicly': 6359, 'inteligent': 4119, 'buddhism': 1052, 'hinduism': 3730, 'ultra': 8359, 'persians': 5908, 'acknowledge': 67, 'averse': 557, 'grocer': 3491, 'pasture': 5813, 'plainly': 6020, 'disgruntle': 2293, 'fellow': 2945, 'abouit': 19, 'despise': 2173, 'muggy': 5284, 'altitude': 261, 'cowboy': 1848, 'fade': 2860, 'enjoyable': 2633, 'charitable': 1328, 'financially': 2996, 'apparent': 364, 'peruvian': 5917, 'comedy': 1581, 'spare': 7512, 'puppy': 6381, 'impeccably': 3951, 'proceed': 6275, 'migration': 5103, 'achieve': 65, 'rank': 6479, 'corporation': 1793, 'barking': 662, 'cunning': 1948, 'extremly': 2841, 'fitness': 3020, 'cosmetic': 1805, 'sympathetic': 7907, 'mannered': 4893, 'affiliation': 133, 'unorganized': 8477, 'broccoli': 1018, 'environmentally': 2658, 'conscious': 1696, 'presentable': 6218, 'loudly': 4768, 'scrapbook': 7007, 'sparce': 7511, 'wimpy': 8859, 'fond': 3088, 'splendid': 7555, 'compliment': 1637, 'joyful': 4350, 'devoutly': 2212, 'nomination': 5467, 'abusing': 35, 'maniac': 4884, 'hankerchief': 3599, 'bare': 657, 'rave': 6496, 'slam': 7336, 'rap': 6483, 'aim': 178, 'upscale': 8528, 'thrive': 8108, 'fluctuate': 3066, 'pastry': 5811, 'fusion': 3239, 'skim': 7324, 'timid': 8143, 'helmet': 3688, 'clone': 1494, 'dreadlock': 2410, 'sober': 7427, 'motorcycle': 5259, 'fur': 3235, 'trader': 8222, 'homestead': 3786, 'acre': 71, 'ranch': 6474, 'smoothie': 7401, 'platinum': 6031, 'regarded': 6576, 'silcon': 7266, 'aloud': 249, 'midnight': 5100, 'ambidextrous': 273, 'breasted': 979, 'announce': 326, 'bierut': 797, 'curvy': 1968, 'spotlight': 7570, 'likeable': 4670, 'administration': 105, 'fresh': 3178, 'relief': 6611, 'craftsman': 1859, 'bomber': 889, 'foster': 3134, 'jockey': 4318, 'planet': 6023, 'comb': 1572, 'belt': 760, 'dentistry': 2136, 'thrill': 8107, 'sickness': 7248, 'raiser': 6460, 'shelter': 7166, 'audis': 526, 'dimple': 2249, 'appreciation': 380, 'theirs': 8070, 'eventually': 2734, 'gdp': 3292, 'shackle': 7127, 'crinkle': 1897, 'trans': 8241, 'martini': 4936, 'haywire': 3651, 'noble': 5458, 'bullying': 1080, 'franchise': 3149, 'peed': 5864, 'frat': 3156, 'shaven': 7156, 'disorganize': 2309, 'literature': 4700, 'brittany': 1013, 'romance': 6803, 'dainty': 1989, 'standing': 7617, 'nursing': 5531, 'hijab': 3718, 'comic': 1589, 'gourmet': 3414, 'stressful': 7728, 'toe': 8168, 'denim': 2126, 'dedicate': 2062, 'content': 1727, 'impact': 3949, 'awesome': 570, 'taiwan': 7930, 'desperate': 2170, 'equipped': 2668, 'mediate': 5017, 'verbal': 8611, 'disagreement': 2268, 'peacefully': 5846, 'impressive': 3969, 'unimportant': 8443, 'grand': 3437, 'scheme': 6977, 'reinforce': 6590, 'uncultured': 8392, 'whiz': 8835, 'trend': 8269, 'effervescent': 2527, 'lambourghini': 4527, 'habitable': 3551, 'politics': 6104, 'populous': 6131, 'technologically': 7995, 'literate': 4699, 'accepting': 43, 'valid': 8566, 'ideology': 3912, 'decorate': 2057, 'laced': 4514, 'briefcase': 996, 'crush': 1929, 'purse': 6395, 'michelin': 5093, 'inspiration': 4095, 'treasure': 8261, 'densely': 2131, 'gathering': 3287, 'educational': 2517, 'vacant': 8556, 'luau': 4786, 'clipboard': 1488, 'stunning': 7760, 'bronze': 1022, 'gleam': 3371, 'wet': 8813, 'gel': 3297, 'confrontation': 1679, 'similarity': 7281, 'banter': 638, 'cheer': 1348, 'outline': 5676, 'extensive': 2823, 'credential': 1881, 'stylish': 7766, 'kempt': 4419, 'ethic': 2709, 'contrarily': 1739, 'loquacious': 4757, 'vivacious': 8678, 'mariah': 4912, 'carey': 1216, 'karaoke': 4399, 'conviction': 1763, 'spaced': 7503, 'populace': 6127, 'working': 8913, 'seeing': 7054, 'vitamin': 8677, 'studiously': 7754, 'varying': 8582, 'smushe': 7404, 'universe': 8458, 'uncrowded': 8391, 'compromise': 1641, 'storied': 7701, 'manga': 4880, 'plight': 6052, 'warrior': 8740, 'overeat': 5694, 'precious': 6191, 'covet': 1844, 'online': 5610, 'sacrifice': 6875, 'recover': 6544, 'sculpture': 7021, 'scream': 7010, 'reformed': 6566, 'airheade': 186, 'wind': 8861, 'farming': 2896, 'groomed': 3495, 'boastfully': 873, 'subscribe': 7780, 'materialism': 4966, 'rounded': 6830, 'compelling': 1617, 'moderate': 5185, 'original': 5656, 'karen': 4401, 'closet': 1498, 'lgbt': 4641, 'equally': 2664, 'regular': 6584, 'lousy': 4773, 'creator': 1879, 'naval': 5367, 'battalion': 690, 'poet': 6076, 'polar': 6090, 'visual': 8673, 'neutral': 5424, 'encouraging': 2610, 'archaic': 400, 'offense': 5584, 'cannabis': 1183, 'baptist': 641, 'transportation': 8251, 'assortment': 480, 'generalize': 3302, 'bilingual': 808, 'puerto': 6362, 'rico': 6744, 'prom': 6297, 'eyeball': 2845, 'festival': 2959, 'rolex': 6798, 'blend': 846, 'gent': 3318, 'chairlift': 1306, 'shre': 7227, 'elegance': 2563, 'disappoint': 2271, 'writer': 8946, 'negatively': 5391, 'confuse': 1681, 'narcotic': 5343, 'reflect': 6563, 'hygienic': 3895, 'delegate': 2098, 'handcuff': 3581, 'hesitant': 3704, 'bold': 882, 'palatable': 5752, 'booze': 914, 'watering': 8760, 'speed': 7535, 'joke': 4329, 'detect': 2191, 'certification': 1301, 'misunderstanding': 5165, 'sargeant': 6927, 'fairly': 2869, 'withstand': 8883, 'regulated': 6586, 'dyslexic': 2473, 'disorder': 2308, 'undergo': 8396, 'happiness': 3607, 'avid': 558, 'climber': 1483, 'occasion': 5569, 'sturdy': 7763, 'sparkling': 7514, 'defend': 2076, 'procastinator': 6273, 'suburb': 7785, 'private': 6263, 'minded': 5122, 'equal': 2662, 'greatness': 3464, 'punk': 6379, 'sharply': 7153, 'horribly': 3824, 'seasoning': 7034, 'marrakesh': 4927, 'medina': 5023, 'mazelike': 4989, 'medieval': 5022, 'djemaa': 2343, 'el': 2548, 'fna': 3077, 'souks': 7481, 'marketplace': 4925, 'ceramic': 1296, 'latern': 4558, 'electrical': 2557, 'appeal': 366, 'dishonesty': 2302, 'humanitarian': 3865, 'latina': 4560, 'bioethic': 818, 'selfless': 7066, 'hilarious': 3723, 'unfashionable': 8425, 'offspring': 5592, 'hollywood': 3770, 'beekeeper': 727, 'skateboard': 7315, 'unprofessional': 8484, 'bleak': 845, 'businesswoman': 1113, 'warmhearted': 8733, 'capitalist': 1198, 'limitless': 4679, 'philanthropist': 5944, 'impotent': 3964, 'worldly': 8918, 'reject': 6591, 'impolite': 3958, 'workbench': 8909, 'baker': 611, 'calculated': 1147, 'abrasive': 21, 'lethargic': 4638, 'leftist': 4608, 'emotionless': 2595, 'indutrius': 4026, 'malodorous': 4869, 'untouched': 8510, 'amateur': 267, 'existent': 2788, 'unimaginative': 8442, 'zenlike': 9013, 'pacifist': 5728, 'prosporous': 6323, 'fashionable': 2905, 'naturalize': 5361, 'predictable': 6197, 'convivial': 1766, 'oddity': 5579, 'spontaneous': 7562, 'worthy': 8928, 'mansion': 4897, 'sporty': 7566, 'homogenous': 3791, 'capitalism': 1197, 'curl': 1955, 'german': 3334, 'tame': 7948, 'emerge': 2588, 'poetic': 6077, 'chocolate': 1392, 'samuel': 6913, 'uncreative': 8390, 'hygenic': 3893, 'discourage': 2282, 'stimulate': 7681, 'satiate': 6933, 'nonreligious': 5472, 'connect': 1689, 'indifferent': 4010, 'prideful': 6243, 'misanthropic': 5144, 'jade': 4245, 'cautious': 1270, 'liberate': 4648, 'toddler': 8167, 'chubby': 1414, 'socialist': 7431, 'myopic': 5322, 'stiff': 7679, 'register': 6581, 'alien': 221, 'unattentive': 8368, 'forgetful': 3115, 'scholarly': 6981, 'erratic': 2686, 'muslm': 5313, 'boisterous': 881, 'architect': 401, 'annalitical': 323, 'satiete': 6934, 'admirable': 106, 'physicistisnotahighlypayingjob': 5959, 'feminist': 2949, 'irresponsible': 4203, 'eventempere': 2732, 'subdue': 7770, 'postdoctoral': 6157, 'feisty': 2943, 'beta': 782, 'unoriginal': 8478, 'jam': 4251, 'inadequate': 3975, 'combatant': 1574, 'juvenile': 4387, 'chilly': 1383, 'freewheel': 3170, 'sunny': 7820, 'bigamist': 800, 'profitable': 6291, 'joyous': 4351, 'compliant': 1633, 'egalitarian': 2535, 'openminde': 5614, 'turkish': 8319, 'extrovert': 2842, 'clutter': 1514, 'thoughtful': 8101, 'oblivious': 5553, 'idiotic': 3914, 'prude': 6345, 'scarwney': 6969, 'unskille': 8497, 'appliance': 374, 'generic': 3306, 'negligent': 5393, 'monogamous': 5221, 'unbiased': 8372, 'treaty': 8264, 'secular': 7047, 'chill': 1382, 'deliquent': 2106, 'neglecting': 5392, 'greed': 3466, 'unskilled': 8498, 'investor': 4179, 'humidity': 3869, 'innovative': 4077, 'trashy': 8255, 'tonedeaf': 8180, 'boyish': 949, 'easygoing': 2494, 'elegant': 2564, 'unsophisticated': 8501, 'jock': 4317, 'determined': 2195, 'welleducated': 8804, 'basic': 674, 'gentleness': 3323, 'unsocial': 8500, 'conservatively': 1702, 'flatulent': 3038, 'boorish': 909, 'validictorian': 8568, 'selfsufficient': 7067, 'alert': 212, 'shabby': 7125, 'tenacious': 8016, 'softy': 7445, 'joker': 4330, 'magnificent': 4837, 'untimely': 8508, 'greek': 3469, 'wooden': 8903, 'forebearance': 3104, 'spotless': 7569, 'makeover': 4855, 'eloquent': 2575, 'gate': 3284, 'optimistic': 5633, 'compasionate': 1612, 'superhighway': 7825, 'untrue': 8511, 'kindness': 4452, 'toxic': 8213, 'complacent': 1625, 'pictureqsue': 5971, 'savior': 6950, 'fearful': 2927, 'unsystematic': 8505, 'manufacturer': 4902, 'protector': 6328, 'lighthearted': 4665, 'astrophysicist': 490, 'prudish': 6346, 'anorexic': 332, 'median': 5016, 'convoy': 1767, 'sleepy': 7355, 'inexperienced': 4031, 'imprisoned': 3971, 'kurdish': 4499, 'unmindful': 8468, 'uplifting': 8522, 'straightlace': 7709, 'skyline': 7334, 'barbaric': 648, 'curtail': 1964, 'diplomatic': 2258, 'hindu': 3729, 'laidback': 4522, 'villian': 8650, 'demure': 2123, 'upright': 8525, 'dumple': 2452, 'ashamed': 447, 'believable': 750, 'emotive': 2596, 'underhande': 8399, 'athi': 495, 'charleston': 1332, 'amoral': 288, 'australia': 537, 'intriguing': 4162, 'monogamist': 5220, 'downgrade': 2392, 'apprenticeship': 382, 'disconnect': 2281, 'pig': 5977, 'lowbrow': 4782, 'milk': 5114, 'suave': 7767, 'bicycle': 796, 'yacht': 8963, 'overqualifie': 5704, 'spoon': 7564, 'stripper': 7739, 'autistic': 543, 'notallpumbersaremario': 5501, 'uninteresting': 8450, 'lenient': 4628, 'mousy': 5268, 'callous': 1155, 'dispassionate': 2311, 'adventuresome': 124, 'jogger': 4322, 'sissy': 7302, 'comedic': 1580, 'igorant': 3921, 'wellbehave': 8803, 'haphazard': 3603, 'propserous': 6318, 'pepper': 5882, 'atheistic': 493, 'vicious': 8632, 'lion': 4686, 'prosperity': 6321, 'unofficial': 8476, 'spank': 7510, 'unreligious': 8490, 'loose': 4754, 'kindhearte': 4450, 'militaristic': 5111, 'goofy': 3406, 'frugal': 3204, 'defensible': 2080, 'upstande': 8532, 'altruistic': 263, 'mellow': 5038, 'novice': 5517, 'diplomat': 2257, 'stupidity': 7762, 'misunderstood': 5166, 'cowardly': 1847, 'lizard': 4710, 'uncertantie': 8377, 'untalented': 8506, 'imaginative': 3934, 'germanic': 3335, 'performingartistcanmakealotofmoney': 5894, 'rockstar': 6790, 'playfulness': 6038, 'laundry': 4569, 'genuine': 3327, 'asexual': 445, 'disgust': 2295, 'mime': 5120, 'discouraging': 2283, 'dextrous': 2213, 'smartwatch': 7387, 'truthful': 8305, 'comrade': 1644, 'sucessful': 7791, 'peacful': 5847, 'helpless': 3692, 'bong': 897, 'democratically': 2117, 'rope': 6819, 'forward': 3133, 'slant': 7338, 'grab': 3423, 'watermelon': 8761, 'bite': 833, 'cad': 1136, 'yolanda': 8994, 'prostitute': 6324, 'alley': 238, 'discard': 2278, 'uncared': 8374, 'jamal': 4254, 'rapper': 6488, 'vince': 8651, 'dirtbike': 2264, 'melissa': 5037, 'imbibe': 3939, 'siesta': 7254, 'cheryl': 1368, 'resentful': 6671, 'jenny': 4281, 'astonishment': 484, 'interpret': 4145, 'squinty': 7590, 'respected': 6688, 'jerome': 4285, 'sunburn': 7815, 'cazar': 1272, 'abandon': 4, 'squeal': 7587, 'narc': 5341, 'yorker': 8996, 'canyon': 1190, 'sigh': 7255, 'shaniqua': 7139, 'caddy': 1137, 'handout': 3591, 'ravens': 6498, 'shareholder': 7146, 'burglar': 1093, 'mattie': 4978, 'englishman': 2630, 'joe': 4319, 'suddenly': 7796, 'hip': 3734, 'grimace': 3484, 'carrie': 1236, 'gullible': 3533, 'canvas': 1189, 'cough': 1816, 'sharon': 7149, 'yes': 8986, 'marginalize': 4910, 'brayden': 968, 'suprise': 7848, 'jerry': 4286, 'hostess': 3835, 'sir': 7300, 'sergeant': 7092, 'insist': 4092, 'cadet': 1138, 'sidney': 7251, 'prey': 6237, 'lucy': 4791, 'ice': 3902, 'skating': 7317, 'adam': 84, 'dismay': 2306, 'robert': 6781, 'jet': 4291, 'eric': 2677, 'ben': 761, 'jeremy': 4283, 'ruby': 6845, 'cheery': 1352, 'carl': 1219, 'considerable': 1704, 'hunting': 3882, 'excursion': 2773, 'lumber': 4796, 'ax': 576, 'fell': 2944, 'swoop': 7905, 'cathy': 1262, 'oddly': 5580, 'attach': 505, 'faced': 2853, 'val': 8563, 'nappy': 5339, 'beyonce': 790, 'gentry': 3326, 'decipher': 2050, 'chang': 1314, 'lee': 4606, 'theft': 8069, 'plow': 6055, 'chong': 1396, 'ling': 4683, 'thorn': 8097, 'lena': 4625, 'avon': 562, 'cream': 1873, 'carton': 1242, 'shannon': 7140, 'sweep': 7891, 'tyrone': 8346, 'shadow': 7129, 'pole': 6091, 'ghetto': 3345, 'barbershop': 655, 'treyvone': 8272, 'aspiration': 462, 'useful': 8545, 'bracelet': 952, 'needlessly': 5387, 'confrontational': 1680, 'mexicans': 5085, 'electronics': 2562, 'agriculture': 165, 'jake': 4249, 'bodybuilder': 879, 'bearish': 705, 'disappointed': 2272, 'greuze': 3481, 'greta': 3480, 'sammy': 6910, 'denny': 2128, 'seedy': 7053, 'pornographic': 6138, 'frightened': 3190, 'grit': 3489, 'gallery': 3252, 'slash': 7342, 'unfortunately': 8428, 'damn': 1995, 'hoe': 3757, 'tuppence': 8314, 'paul': 5832, 'basket': 678, 'indians': 4009, 'horse': 3827, 'jaleel': 4250, 'commonly': 1601, 'juan': 4354, 'wallet': 8716, 'creep': 1884, 'regardless': 6577, 'sadness': 6883, 'silas': 7265, 'flashback': 3033, 'susan': 7868, 'dig': 2238, 'ditch': 2333, 'kurt': 4500, 'carla': 1220, 'scamme': 6958, 'realtor': 6511, 'ragesh': 6452, 'manual': 4899, 'kia': 4432, 'zumba': 9027, 'kariem': 4402, 'armenian': 420, 'ninja': 5452, 'plead': 6046, 'penalty': 5871, 'account': 58, 'jorge': 4338, 'chan': 1312, 'slanted': 7339, 'pop': 6123, 'hood': 3804, 'tick': 8124, 'terrance': 8030, 'orchestrate': 5639, 'nigerian': 5440, 'enraged': 2637, 'joseph': 4340, 'williams': 8855, 'pit': 6007, 'sinewy': 7291, 'manly': 4889, 'beast': 706, 'eskimo': 2695, 'kwame': 4502, 'droplet': 2429, 'awake': 564, 'induce': 4019, 'florida': 3060, 'micheal': 5092, 'fred': 3162, 'voodoo': 8693, 'mumbo': 5296, 'jumbo': 4372, 'nonsense': 5473, 'cashier': 1248, 'gypsy': 3548, 'rot': 6822, 'jenna': 4279, 'weekly': 8792, 'basis': 677, 'website': 8784, 'gladly': 3365, 'mistress': 5164, 'alike': 224, 'lowly': 4783, 'cower': 1849, 'koreans': 4491, 'terrell': 8031, 'tim': 8138, 'anita': 319, 'trailer': 8232, 'workday': 8910, 'joanne': 4315, 'jafari': 4246, 'cal': 1144, 'villager': 8648, 'shoplift': 7208, 'cheeto': 1357, 'steve': 7670, 'duel': 2444, 'jimmy': 4313, 'bench': 762, 'strongly': 7743, 'lance': 4531, 'shoppe': 7209, 'david': 2017, 'unavailable': 8370, 'kirby': 4456, 'option': 5634, 'railroad': 6453, 'craig': 1861, 'zoo': 9024, 'inherent': 4059, 'sitter': 7307, 'sitting': 7308, 'bawl': 693, 'unhappily': 8431, 'confront': 1678, 'eddie': 2506, 'cash': 1246, 'astronomy': 488, 'heist': 3685, 'marie': 4914, 'spider': 7547, 'shriek': 7231, 'expertly': 2803, 'tractor': 8219, 'wright': 8942, 'roof': 6811, 'manuel': 4900, 'haircut': 3558, 'trim': 8279, 'landscaper': 4537, 'globetrotter': 3379, 'prescott': 6214, 'iii': 3923, 'neuroradiologist': 5420, 'scent': 6975, 'desperately': 2171, 'carol': 1225, 'shelly': 7165, 'rhythm': 6737, 'grace': 3424, 'socializing': 7433, 'inspector': 4094, 'julie': 4368, 'tow': 8207, 'li': 4644, 'adrin': 115, 'horror': 3826, 'chicken': 1374, 'wing': 8869, 'multiplicative': 5294, 'megan': 5034, 'laquisha': 4544, 'patrick': 5824, 'mexians': 5083, 'townspeople': 8212, 'apprehend': 381, 'tackle': 7923, 'kyle': 4504, 'chase': 1338, 'burr': 1103, 'crawl': 1869, 'sheng': 7169, 'chen': 1366, 'wu': 8950, 'astronomer': 487, 'galaxy': 3250, 'veronica': 8616, 'shirk': 7191, 'alfred': 216, 'wielder': 8845, 'deshawn': 2163, 'sympathy': 7908, 'barbed': 651, 'wire': 8875, 'fence': 2950, 'joxer': 4347, 'magical': 4834, 'phillip': 5946, 'ultimately': 8358, 'dictate': 2223, 'operate': 5616, 'rear': 6513, 'clutch': 1513, 'reign': 6589, 'kkk': 4465, 'sob': 7426, 'becasue': 714, 'kentucky': 4423, 'biff': 798, 'fearless': 2928, 'hell': 3686, 'russ': 6861, 'josh': 4341, 'grandparent': 3445, 'goalie': 3387, 'burn': 1099, 'charlie': 1333, 'kelsi': 4418, 'section': 7045, 'ming': 5128, 'lisa': 4691, 'salon': 6901, 'cop': 1779, 'umbrella': 8362, 'camille': 1163, 'terrify': 8036, 'clap': 1448, 'thankful': 8063, 'cleanse': 1466, 'tamp': 7951, 'recognize': 6538, 'recommendation': 6540, 'contempt': 1726, 'photography': 5953, 'killing': 4443, 'tarnisha': 7962, 'lurk': 4801, 'england': 2628, 'pub': 6355, 'funeral': 3233, 'tanning': 7957, 'tammy': 7950, 'suck': 7792, 'leonard': 4632, 'annual': 330, 'magician': 4835, 'convention': 1754, 'aggressiveness': 157, 'prestigious': 6228, 'strawberry': 7721, 'arguing': 411, 'yelling': 8979, 'suitable': 7806, 'jones': 4334, 'silence': 7267, 'bootlegger': 913, 'faster': 2909, 'sentimental': 7082, 'dumpster': 2453, 'dive': 2335, 'uninsured': 8446, 'ed': 2505, 'toss': 8193, 'ebony': 2497, 'mitchell': 5168, 'decore': 2059, 'charlene': 1330, 'paddle': 5734, 'clue': 1506, 'george': 3332, 'landslide': 4538, 'darkness': 2010, 'disappointment': 2273, 'insensitivity': 4089, 'da': 1982, 'sergey': 7093, 'spearman': 7522, 'routine': 6834, 'martinez': 4935, 'destine': 2179, 'lebron': 4602, 'collaborate': 1545, 'principal': 6252, 'tanisha': 7955, 'jeff': 4273, 'elastic': 2549, 'ellen': 2573, 'pregnant': 6206, 'andy': 307, 'subway': 7786, 'pickpockete': 5967, 'jim': 4312, 'aeronautical': 128, 'commercial': 1595, 'hitchhike': 3747, 'hitch': 3746, 'cloak': 1491, 'courthouse': 1839, 'addictive': 91, 'phoebe': 5950, 'uneasy': 8415, 'mack': 4820, 'slaughter': 7345, 'benjamin': 769, 'ban': 625, 'platform': 6030, 'rotate': 6823, 'satisfied': 6936, 'edith': 2511, 'edward': 2519, 'menial': 5050, 'keeper': 4415, 'investigation': 4177, 'unamerican': 8365, 'vrenna': 8698, 'nia': 5431, 'fervor': 2958, 'shrimp': 7232, 'shawn': 7157, 'fulfill': 3223, 'request': 6665, 'kentuckian': 4422, 'marty': 4937, 'trophy': 8285, 'thrift': 8106, 'lea': 4584, 'motel': 5251, 'terry': 8043, 'excessive': 2762, 'irishman': 4198, 'gutte': 3542, 'coal': 1519, 'bargain': 660, 'nell': 5404, 'fist': 3017, 'morgan': 5236, 'frown': 3202, 'knelt': 4472, 'gravestone': 3456, 'hometown': 3787, 'mile': 5109, 'survival': 7864, 'overnight': 5699, 'fruition': 3206, 'embarrassed': 2583, 'flame': 3030, 'greedily': 3467, 'richard': 6742, 'winery': 8868, 'napa': 5337, 'marianne': 4913, 'cherish': 1367, 'heirloom': 3683, 'drawer': 2407, 'wage': 8704, 'ryan': 6870, 'ship': 7188, 'lost': 4763, 'shipwreck': 7190, 'homeowner': 3785, 'bourgeois': 936, 'powered': 6173, 'cynthia': 1979, 'unreasonable': 8486, 'trunk': 8301, 'peddle': 5855, 'matt': 4976, 'landlord': 4534, 'suspicion': 7873, 'wad': 8703, 'guinea': 3528, 'julius': 4370, 'gabriel': 3246, 'distress': 2329, 'curse': 1963, 'census': 1289, 'mate': 4963, 'steel': 7656, 'earl': 2479, 'tommy': 8177, 'invincible': 4180, 'ronnie': 6810, 'absent': 25, 'bowman': 942, 'matching': 4962, 'fearlessly': 2929, 'evict': 2739, 'interracial': 4146, 'fool': 3094, 'genetic': 3310, 'kate': 4407, 'arson': 431, 'alibi': 220, 'greg': 3477, 'insistence': 4093, 'point': 6079, 'erotic': 2684, 'unsatisfactory': 8494, 'cease': 1278, 'trainer': 8234, 'faint': 2866, 'heather': 3673, 'brad': 953, 'blackmail': 837, 'meghan': 5035, 'indulge': 4020, 'wildest': 8851, 'whim': 8826, 'miguel': 5104, 'hanson': 3600, 'swallow': 7877, 'spit': 7553, 'cheaply': 1344, 'korean': 4490, 'jacob': 4242, 'tony': 8183, 'confidently': 1673, 'employer': 2603, 'stalk': 7612, 'ron': 6808, 'obscene': 5556, 'pier': 5975, 'albeit': 204, 'scar': 6963, 'stark': 7626, 'reminder': 6626, 'dear': 2036, 'toxin': 8214, 'yakuza': 8964, 'betty': 786, 'cookout': 1773, 'classic': 1452, 'impossible': 3963, 'spill': 7548, 'snap': 7408, 'pronounce': 6305, 'consonant': 1709, 'obligation': 5552, 'relied': 6610, 'stephen': 7663, 'pokemon': 6087, 'pokeball': 6085, 'cerveza': 1302, 'tequila': 8025, 'sherry': 7173, 'atrocity': 504, 'dough': 2390, 'corey': 1787, 'tightly': 8130, 'clung': 1511, 'meager': 4996, 'darn': 2012, 'estate': 2703, 'instantly': 4100, 'gaudy': 3288, 'rim': 6756, 'crappy': 1865, 'cis': 1433, 'detective': 2192, 'mugger': 5283, 'clarinet': 1449, 'gradebook': 3428, 'kim': 4446, 'drew': 2416, 'anse': 334, 'handiwork': 3585, 'brian': 991, 'bat': 683, 'felon': 2946, 'discrimination': 2286, 'carlos': 1222, 'pérez': 6405, 'devastating': 2200, 'fastball': 2908, 'davis': 2018, 'javier': 4267, 'martin': 4934, 'disappearance': 2270, 'amish': 287, 'wood': 8902, 'system': 7917, 'instal': 4098, 'kathy': 4409, 'maternal': 4968, 'shiloh': 7184, 'misrepresent': 5154, 'rican': 6739, 'maintenance': 4851, 'scuba': 7020, 'fortunately': 3131, 'unemployment': 8419, 'dutch': 2462, 'pleading': 6047, 'soap': 7424, 'wisely': 8879, 'inject': 4065, 'insulin': 4113, 'inglethorp': 4051, 'sorry': 7477, 'creatively': 1878, 'swimmer': 7897, 'congregate': 1685, 'sean': 7027, 'fundraiser': 3232, 'southern': 7491, 'stitch': 7688, 'gunshot': 3538, 'wound': 8932, 'erik': 2678, 'brain': 958, 'core': 1786, 'pill': 5986, 'aligned': 222, 'deandre': 2035, 'timidly': 8144, 'headline': 3656, 'parade': 5774, 'nema': 5405, 'roma': 6801, 'billy': 812, 'handler': 3588, 'molest': 5197, 'roberta': 6782, 'thunder': 8117, 'shack': 7126, 'loom': 4753, 'bedroom': 720, 'infest': 4038, 'roach': 6771, 'humiliate': 3870, 'patricia': 5823, 'ordeal': 5640, 'behead': 744, 'fail': 2865, 'trot': 8287, 'reservation': 6672, 'hysterical': 3900, 'trevon': 8271, 'feng': 2952, 'wendy': 8807, 'finger': 3000, 'chin': 1386, 'pang': 5767, 'dorothy': 2383, 'loot': 4755, 'pitbull': 6009, 'raincoat': 6456, 'propose': 6317, 'dump': 2450, 'anyways': 354, 'stability': 7599, 'jackson': 4241, 'kayak': 4413, 'militia': 5113, 'shelby': 7162, 'electronic': 2561, 'rent': 6636, 'singing': 7295, 'preaching': 6189, 'scottish': 7001, 'chink': 1389, 'aisha': 193, 'sucker': 7793, 'sudeep': 7798, 'rennie': 6632, 'bravely': 966, 'riding': 6750, 'marquis': 4926, 'hoodie': 3806, 'titanic': 8159, 'jailer': 4248, 'bristle': 1008, 'tenth': 8024, 'jog': 4321, 'contractor': 1736, 'burrito': 1105, 'donald': 2368, 'stab': 7597, 'imigrant': 3940, 'investigate': 4176, 'supervisor': 7831, 'stare': 7625, 'ramirez': 6469, 'dense': 2130, 'animalistic': 315, 'nostalgia': 5496, 'sneak': 7410, 'lamar': 4524, 'significant': 7263, 'chunk': 1417, 'tae': 7926, 'kwon': 4503, 'essex': 2701, 'mule': 5289, 'zoey': 9020, 'chide': 1376, 'fifth': 2977, 'shouting': 7221, 'communicate': 1602, 'kassandra': 4405, 'airhead': 185, 'boom': 907, 'defdant': 2072, 'ramshackle': 6473, 'ritz': 6767, 'picnic': 5969, 'salesman': 6898, 'pastor': 5810, 'kristy': 4494, 'hysteric': 3899, 'thai': 8059, 'automatically': 547, 'contribute': 1742, 'heavyset': 3678, 'fireman': 3007, 'plantation': 6026, 'crow': 1917, 'hover': 3852, 'bird': 823, 'abyss': 37, 'scowled': 7004, 'amusement': 291, 'mooch': 5228, 'wang': 8720, 'chopstick': 1403, 'miserable': 5148, 'gorilla': 3411, 'womans': 8897, 'undisciplined': 8410, 'burglarize': 1094, 'brag': 956, 'competence': 1620, 'nasally': 5347, 'eugene': 2720, 'reenactment': 6557, 'sudden': 7795, 'aids': 176, 'confirm': 1675, 'industriously': 4024, 'streak': 7723, 'brenda': 988, 'shocked': 7198, 'hillbilly': 3725, 'squalid': 7582, 'anne': 325, 'crocheting': 1908, 'sheriff': 7172, 'delighted': 2105, 'celebrity': 1283, 'april': 388, 'katie': 4410, 'contest': 1729, 'highlight': 3715, 'texans': 8049, 'seth': 7106, 'cloud': 1502, 'graciously': 3426, 'vet': 8626, 'afaid': 129, 'hassle': 3636, 'leshawn': 4635, 'boris': 920, 'kevin': 4427, 'enlist': 2635, 'rancher': 6475, 'dawson': 2020, 'inscrutable': 4084, 'plotter': 6054, 'decline': 2056, 'hitler': 3749, 'cd': 1275, 'robber': 6778, 'spin': 7549, 'visible': 8667, 'lynn': 4809, 'devastate': 2198, 'patiently': 5821, 'ramon': 6470, 'inherently': 4060, 'lloyd': 4716, 'tissue': 8157, 'defer': 2082, 'shoddy': 7200, 'bai': 606, 'bandido': 630, 'apache': 357, 'freak': 3160, 'backstage': 590, 'supremacist': 7845, 'devious': 2209, 'soren': 7475, 'notoriously': 5509, 'jasmine': 4265, 'cheesecake': 1355, 'ashley': 449, 'shaquille': 7143, 'candace': 1175, 'liz': 4709, 'vegas': 8593, 'easterner': 2491, 'empowerment': 2605, 'lawrence': 4577, 'grandma': 3442, 'dryer': 2440, 'repairman': 6639, 'hooky': 3812, 'homicide': 3789, 'hustle': 3888, 'slaught': 7344, 'mob': 5180, 'shevon': 7174, 'weep': 8794, 'pregnancy': 6205, 'unpatriotic': 8479, 'driving': 2423, 'unsafe': 8492, 'zero': 9014, 'shilling': 7183, 'merge': 5062, 'driveway': 2422, 'remarkable': 6621, 'defiant': 2083, 'hoodoo': 3807, 'oracle': 5636, 'laura': 4570, 'spook': 7563, 'heaven': 3675, 'austin': 536, 'looter': 4756, 'loiter': 4736, 'ethan': 2708, 'hatred': 3642, 'rajeev': 6462, 'chardonnay': 1323, 'qualify': 6410, 'relinquish': 6616, 'anna': 322, 'incompatible': 3993, 'hoard': 3753, 'joyce': 4349, 'candice': 1176, 'gardening': 3271, 'kolton': 4486, 'exorcist': 2790, 'conclusion': 1655, 'hastily': 3637, 'dylan': 2470, 'kenny': 4421, 'slip': 7365, 'tobacco': 8165, 'dale': 1991, 'breeze': 987, 'livestock': 4707, 'visibly': 8668, 'malik': 4864, 'vengeful': 8603, 'xbox': 8955, 'disarray': 2274, 'lara': 4546, 'frustrate': 3209, 'massacre': 4952, 'weave': 8781, 'orchard': 5638, 'seasonally': 7033, 'radar': 6445, 'jen': 4277, 'norse': 5484, 'lily': 4675, 'scandanavian': 6961, 'alexander': 214, 'texan': 8048, 'madam': 4824, 'counseling': 1820, 'daniel': 2006, 'brent': 989, 'smith': 7394, 'harold': 3627, 'suddently': 7797, 'scott': 7000, 'jill': 4311, 'jamie': 4258, 'tyler': 8338, 'celia': 1285, 'californian': 1153, 'consciousness': 1697, 'olga': 5602, 'yang': 8966, 'canadian': 1172, 'floridian': 3061, 'william': 8854, 'greeks': 3470, 'amhad': 285, 'exit': 2789, 'monique': 5216, 'jeffrey': 4274, 'vietnamese': 8640, 'irish': 4197, 'garcia': 3268, 'samantha': 6908, 'johnson': 4326, 'lang': 4540, 'alfre': 215, 'jon': 4332, 'raja': 6461, 'sniper': 7415, 'mathew': 4974, 'winston': 8871, 'jacques': 4243, 'stewart': 7674, 'cisfemale': 1434, 'wesley': 8808, 'katherine': 4408, 'senior': 7075, 'pampered': 5760, 'overeducate': 5695, 'spearwoman': 7523, 'caucasians': 1267, 'kassidy': 4406, 'dutchman': 2463, 'bel': 747, 'monica': 5215, 'dweller': 2469, 'juliette': 4369, 'spence': 7540, 'anglo': 311, 'mongolian': 5213, 'tran': 8238, 'begger': 734, 'jennifer': 4280, 'muhammad': 5286, 'ample': 290, 'bradford': 954, 'worthington': 8927, 'claire': 1446, 'molly': 5198, 'ok': 5599, 'privileged': 6264, 'co': 1517, 'whibte': 8823, 'tyresse': 8345, 'cisgender': 1435, 'swede': 7888, 'colorado': 1560, 'bougie': 932, 'pakistani': 5749, 'peter': 5928, 'henderson': 3694, 'brady': 955, 'joo': 4336, 'lin': 4681, 'connor': 1693, 'naomi': 5335, 'surgeon': 7856, 'joey': 4320, 'dustin': 2459, 'barbara': 646, 'ny': 5539, 'collector': 1552, 'cody': 1536, 'offender': 5583, 'posh': 6146, 'cleric': 1472, 'zack': 9007, 'lou': 4766, 'disgusted': 2296, 'degenerate': 2093, 'corporate': 1792, 'unfairly': 8424, 'lashawnda': 4554, 'butler': 1120, 'wee': 8787, 'uneducate': 8416, 'becky': 715, 'australian': 538, 'maya': 4984, 'englishwoman': 2631, 'suspend': 7872, 'wong': 8901, 'ramericans': 6468, 'priya': 6265, 'malcolm': 4861, 'johnathan': 4324, 'laquan': 4543, 'spoiled': 7558, 'missy': 5160, 'maggie': 4832, 'aristocratic': 416, 'lar': 4545, 'jacqui': 4244, 'fanatic': 2886, 'nightstand': 5448, 'gild': 3355, 'brahmin': 957, 'vedas': 8589, 'recommend': 6539, 'contradiction': 1738, 'degrade': 2094, 'sacred': 6874, 'caste': 1251, 'garb': 3266, 'holy': 3778, 'possesion': 6151, 'exhort': 2785, 'mosque': 5247, 'preacher': 6188, 'congregation': 1686, 'irrelevant': 4202, 'guide': 3524, 'suggestion': 7803, 'scripture': 7016, 'prayer': 6185, 'dearly': 2037, 'guilt': 3526, 'handbag': 3580, 'convert': 1760, 'abaya': 6, 'scrolled': 7017, 'perceive': 5885, 'coach': 1518, 'august': 528, 'dresser': 2415, 'handwritten': 3594, 'entry': 2655, 'license': 4652, 'threat': 8104, 'bodega': 876, 'madrasas': 4827, 'brainwash': 959, 'sinner': 7298, 'lord': 4758, 'rebecca': 6517, 'jihad': 4307, 'catholicism': 1260, 'maim': 4844, 'oppression': 5628, 'nobleman': 5459, 'importance': 3960, 'clearly': 1470, 'version': 8619, 'boston': 925, 'fiction': 2968, 'deity': 2096, 'adherent': 100, 'witchdoctor': 8882, 'prejudice': 6209, 'quote': 6429, 'selfcontradicte': 7064, 'philosophical': 5947, 'eve': 2729, 'prestigeous': 6227, 'theme': 8072, 'intollerant': 4159, 'temple': 8014, 'quaran': 6414, 'believer': 752, 'legalistic': 4613, 'disbelief': 2277, 'unprecedented': 8483, 'previously': 6236, 'warn': 8737, 'despair': 2169, 'mohammad': 5190, 'tense': 8021, 'mr': 5278, 'carter': 1241, 'ned': 5382, 'tasha': 7964, 'luring': 4800, 'disdain': 2290, 'spiteful': 7554, 'wiccan': 8842, 'trap': 8252, 'coupon': 1833, 'bee': 723, 'removal': 6629, 'kosher': 4492, 'costume': 1811, 'nosedly': 5495, 'judgment': 4361, 'torah': 8189, 'fle': 3041, 'humongous': 3874, 'nostril': 5498, 'entrance': 2652, 'rotten': 6826, 'pile': 5985, 'donation': 2370, 'piggy': 5979, 'size': 7312, 'eiffel': 2543, 'poison': 6081, 'scarlet': 6968, 'vishas': 8666, 'sacrilegious': 6876, 'perth': 5916, 'exploit': 2808, 'entitle': 2651, 'navajo': 5366, 'prophecy': 6313, 'manipulate': 4887, 'weary': 8778, 'conspire': 1710, 'cole': 1544, 'hostel': 3834, 'hex': 3707, 'motive': 5258, 'anybody': 351, 'quran': 6430, 'satanist': 6931, 'steer': 7658, 'revealing': 6723, 'glare': 3368, 'yay': 8971, 'amp': 289, 'pathetic': 5818, 'sickening': 7246, 'fuck': 3215, 'pigfucke': 5978, 'troll': 8283, 'nazis': 5371, 'al': 199, 'aqsa': 390, 'jihadi': 4309, 'pjnet': 6014, 'tcot': 7978, 'tgdn': 8056, 'ccot': 1274, 'rednationrising': 6554, 'teapay': 7986, 'nra': 5520, 'nigger': 5444, 'enteaine': 2642, 'knockdown': 4476, 'tweet': 8332, 'lol': 4737, 'jihadist': 4310, 'bitch': 831, 'crosshair': 1915, 'ahhh': 168, 'teabagger': 7980, 'morningjoe': 5239, 'fucking': 3218, 'qaeda': 6406, 'rhyme': 6736, 'civilization': 1442, 'mikepence': 5106, 'etc': 2706, 'ironic': 4200, 'prisonplanet': 6260, 'strangle': 7713, 'conve': 1748, 'yells': 8983, 'allahu': 227, 'akbar': 197, 'hi': 3710, 'intervention': 4152, 'shiism': 7181, 'permanent': 5899, 'palestinians': 5755, 'africanfellowdog': 141, 'protestant': 6331, 'koran': 4488, 'saintpetersburg': 6893, 'ha': 3549, 'smirk': 7393, 'jealous': 4271, 'inventer': 4172, 'shit': 7193, 'ur': 8535, 'hea': 3652, 'signifcantly': 7262, 'indisputable': 4013, 'petersburg': 5929, 'attacker': 507, 'wtf': 8949, 'defeatist': 2074, 'shepard': 7170, 'supreme': 7847, 'cou': 1814, 'uninspired': 8445, 'repetition': 6642, 'sunnis': 7819, 'imaamat': 3930, 'guidance': 3523, 'shias': 7177, 'retard': 6708, 'isphahaan': 4222, 'imaam': 3929, 'reappear': 6512, 'literally': 4698, 'genocide': 3315, 'humanity': 3866, 'prayforsyria': 6186, 'chemicalattack': 1362, 'ya': 8961, 'inhuman': 4062, 'tht': 8114, 'kanchiong': 4396, 'fr': 3142, 'beh': 737, 'tetibe': 8047, 'ckp': 1444, 'malay': 4860, 'cuz': 1976, 'bruh': 1034, 'twat': 8329, 'xeno': 8956, 'boner': 895, 'paranoia': 5779, 'hysteria': 3898, 'cce': 1273, 'judicial': 4363, 'merit': 5063, 'sindh': 7290, 'collective': 1550, 'gon': 3402, 'jesus': 4290, 'dick': 2222, 'devil': 2208, 'separete': 7085, 'nd': 5373, 'forgave': 3112, 'sin': 7288, 'mtcheeeeeeeeew': 5279, 'enrichment': 2638, 'whichever': 8824, 'utterly': 8555, 'halo': 3572, 'religionofpeace': 6614, 'yesthisissarcasm': 8988, 'tomahawk': 8175, 'missile': 5156, 'airstrip': 191, 'assad': 465, 'firework': 3010, 'godbot': 3391, 'sepparate': 7088, 'meditteranean': 5028, 'crispy': 1900, 'hooknose': 3811, 'chimney': 1384, 'bestow': 780, 'confession': 1669, 'mol': 5195, 'auschwitz': 532, 'ashtray': 450, 'impoant': 3957, 'hanukkah': 3601, 'blast': 843, 'pa': 5724, 'oven': 5686, 'synagogue': 7910, 'carload': 1221, 'slogan': 7371, 'daed': 1986, 'wheeler': 8821, 'intersection': 4151, 'sniff': 7413, 'overdose': 5692, 'bacon': 595, 'usain': 8543, 'bolt': 885, 'interrogator': 4148, 'lactose': 4517, 'paedophile': 5736, 'dumbfuck': 2448, 'crap': 1864, 'excuse': 2774, 'semitic': 7071, 'unhinged': 8436, 'perve': 5918, 'germany': 3337, 'sulfur': 7808, 'ash': 446, 'nazi': 5370, 'overheat': 5696, 'iz': 4236, 'repoer': 6650, 'abdul': 8, 'rhazim': 6735, 'sheep': 7159, 'isn': 4219, 'oh': 5595, 'switch': 7902, 'germans': 3336, 'circumcise': 1429, 'virginity': 8662, 'crispie': 1899, 'hotdog': 3839, 'rid': 6745, 'timing': 8145, 'wednesday': 8786, 'shooter': 7204, 'climb': 1482, 'auctually': 524, 'inhale': 4056, 'inheil': 4057, 'sleeve': 7356, 'meme': 5042, 'deep': 2067, 'vermin': 8614, 'exterminator': 2826, 'corpse': 1795, 'didn': 2227, 'bergen': 773, 'belsen': 759, 'mum': 5295, 'violet': 8659, 'bihday': 804, 'parcel': 5782, 'doesn': 2355, 'counting': 1825, 'botch': 927, 'circumsision': 1431, 'canablism': 1170, 'balloon': 621, 'rabbis': 6434, 'circumcision': 1430, 'borrow': 921, 'foy': 3141, 'thiy': 8093, 'dummy': 2449, 'aren': 406, 'ill': 3924, 'nun': 5529, 'chainsaw': 1304, 'santa': 6923, 'clause': 1459, 'bullet': 1073, 'trendy': 8270, 'eater': 2496, 'guten': 3541, 'wasn': 8751, 'virgin': 8661, 'ceificate': 1279, 'shovel': 7223, 'ak': 194, 'randomly': 6477, 'blowjob': 864, 'albe': 203, 'einstein': 2545, 'paicular': 5739, 'erection': 2676, 'sho': 7196, 'fuse': 3238, 'pulp': 6368, 'cord': 1784, 'timer': 8141, 'shooting': 7205, 'pittsburgh': 6010, 'haram': 3609, 've': 8588, 'convee': 1749, 'documentary': 2353, 'comedian': 1579, 'curiously': 1954, 'stream': 7724, 'inter': 4130, 'racial': 6439, 'midget': 5099, 'porn': 6136, 'holla': 3765, 'scout': 7002, 'wrap': 8934, 'review': 6728, 'christ': 1407, 'adolf': 110, 'ann': 321, 'ly': 4808, 'semtex': 7073, 'staing': 7609, 'coincidince': 1540, 'killstreak': 4444, 'woulf': 8931, 'chem': 1360, 'kroger': 4495, 'sperm': 7543, 'urn': 8541, 'exterminate': 2824, 'squi': 7588, 'spritz': 7577, 'baked': 610, 'flying': 3075, 'insect': 4085, 'jihaddy': 4308, 'sexiest': 7118, 'baptism': 640, 'en': 2606, 'masse': 4954, 'ashy': 451, 'grotesque': 3501, 'faze': 2923, 'prick': 6241, 'preist': 6208, 'favourite': 2920, 'minor': 5138, 'bomberman': 890, 'ain': 179, 'adapt': 85, 'lmao': 4717, 'shouldn': 7219, 'phelps': 5941, 'staer': 7605, 'survive': 7865, 'execution': 2776, 'executor': 2778, 'norris': 5483, 'reborn': 6521, 'revengesaveshumanity': 6725, 'memorial': 5045, 'countryman': 1828, 'ahmed': 171, 'expel': 2796, 'kahoot': 4391, 'diffrence': 2237, 'staed': 7604, 'flie': 3048, 'paedofile': 5735, 'rabbi': 6433, 'apparently': 365, 'scope': 6997, 'walma': 8717, 'pok': 6083, 'mon': 5206, 'toyota': 8216, 'corallah': 1783, 'egg': 2536, 'feility': 2941, 'infeile': 4036, 'claus': 1458, 'ashe': 448, 'piss': 6005, 'concentration': 1649, 'stale': 7611, 'barb': 645, 'masturbation': 4959, 'penance': 5872, 'brainwasher': 960, 'lightbulb': 4663, 'zyklon': 9028, 'baloon': 623, 'thano': 8066, 'dust': 2458, 'span': 7508, 'sma': 7382, 'obesity': 5548, 'pandemic': 5764, 'fictional': 2969, 'jamaican': 4253, 'happend': 3605, 'album': 207, 'deficit': 2085, 'trek': 8266, 'edit': 2510, 'limp': 4680, 'gassy': 3282, 'justify': 4385, 'mo': 5179, 'farrah': 2898, 'journalist': 4345, 'survivor': 7867, 'repo': 6647, 'hotline': 3841, 'genitalia': 3312, 'nein': 5403, 'mouth': 5269, 'couldn': 1818, 'resist': 6679, 'pe': 5842, 'stumble': 7759, 'reply': 6646, 'moses': 5245, 'trough': 8293, 'scum': 7022, 'diary': 2220, 'kinky': 4455, 'relateable': 6595, 'furry': 3237, 'adic': 101, 'execute': 2775, 'plagiarism': 6016, 'inch': 3986, 'shove': 7222, 'motivation': 5257, 'calorie': 1158, 'typo': 8344, 'canoe': 1187, 'seal': 7025, 'vede': 8590, 'slot': 7374, 'simulator': 7287, 'vr': 8697, 'concentrated': 1648, 'bury': 1107, 'brim': 1005, 'undress': 8414, 'choppable': 1400, 'firewood': 3009, 'airforce': 184, 'bowler': 940, 'cricket': 1891, 'accommodation': 51, 'lingerie': 4684, 'socks': 7439, 'bachelor': 584, 'whore': 8840, 'separate': 7083, 'bih': 803, 'reunite': 6720, 'defeat': 2073, 'morgue': 5237, 'fa': 2849, 'hu': 3854, 'boxer': 945, 'reich': 6588, 'krispie': 4493, 'zaknelson': 9009, 'goldenpiggiez': 3398, 'suggest': 7802, 'luftwaffle': 4792, 'cunt': 1949, 'rampage': 6471, 'manchester': 4877, 'toll': 8173, 'velocity': 8599, 'cooper': 1778, 'lgbtq': 4642, 'condom': 1664, 'intestine': 4155, 'innoveyte': 4078, 'mourn': 5265, 'sweetie': 7893, 'honey': 3799, 'dwarf': 2467, 'wanna': 8722, 'explosion': 2812, 'asif': 456, 'solider': 7454, 'pow': 6169, 'pedophilia': 5861, 'volkswagen': 8686, 'pussy': 6400, 'hawking': 3649, 'paralysed': 5778, 'nigga': 5442, 'burner': 1100, 'th': 8058, 'gi': 3347, 'trinity': 8280, 'best': 777, 'bishop': 828, 'ans': 333, 'brothel': 1025, 'bathtub': 688, 'submerge': 7775, 'rainbow': 6455, 'sta': 7596, 'gass': 3281, 'venomous': 8606, 'satan': 6929, 'ting': 8150, 'boote': 911, 'void': 8682, 'bootes': 912, 'imaginary': 3933, 'chuch': 1415, 'vendor': 8601, 'soar': 7425, 'ww': 8951, 'jewsual': 4305, 'scarface': 6967, 'islamophobia': 4216, 'unfounately': 8429, 'reaction': 6502, 'excess': 2761, 'chick': 1373, 'tray': 8260, 'ish': 4207, 'jenga': 4278, 'pedophile': 5860, 'taboo': 7922, 'goblet': 3389, 'drum': 2436, 'fleshlight': 3046, 'confusion': 1683, 'potter': 6164, 'faed': 2862, 'hus': 3885, 'handjob': 3586, 'goddamn': 3392, 'react': 6501, 'endothermically': 2616, 'disappear': 2269, 'cleansed': 1467, 'burkha': 1097, 'accidentally': 47, 'steam': 7655, 'tab': 7920, 'heating': 3674, 'cod': 1532, 'gamemode': 3258, 'balcony': 615, 'siamese': 7241, 'veggie': 8596, 'pinocchio': 5997, 'pleasure': 6049, 'tautology': 7974, 'quaerback': 6408, 'hipster': 3737, 'niece': 5436, 'disprove': 2316, 'transgender': 8245, 'adhd': 98, 'shwitz': 7238, 'paywall': 5840, 'goy': 3419, 'orgasm': 5651, 'screech': 7011, 'disperse': 2312, 'conveed': 1750, 'judaism': 4356, 'conjecture': 1688, 'nah': 5330, 'genre': 3317, 'trapbar': 8253, 'hitla': 3748, 'dindu': 2250, 'nuffin': 5525, 'ejaculate': 2546, 'palestenian': 5754, 'realise': 6508, 'outright': 5678, 'extinction': 2829, 'crematorium': 1888, 'microwave': 5096, 'snack': 7405, 'inf': 4032, 'hrer': 3853, 'disciple': 2280, 'gta': 3516, 'breathe': 982, 'ahhhh': 169, 'infidel': 4039, 'divide': 2338, 'crash': 1866, 'inappropriate': 3977, 'frankly': 3153, 'guessing': 3521, 'grape': 3449, 'wildfire': 8852, 'whale': 8815, 'dip': 2256, 'hogwas': 3758, 'offensive': 5585, 'jewsish': 4304, 'auschwitzersehen': 533, 'di': 2214, 'fevorite': 2964, 'bless': 848, 'detanotor': 2190, 'rentable': 6637, 'trump': 8299, 'pinterest': 6000, 'burrie': 1104, 'barbeque': 653, 'catholics': 1261, 'dustpan': 2460, 'broom': 1024, 'autumn': 552, 'coughing': 1817, 'wheezing': 8822, 'disagree': 2267, 'cre': 1872, 'idk': 3915, 'minesweeper': 5127, 'facial': 2854, 'anticipate': 344, 'rpg': 6840, 'shi': 7175, 'cannon': 1186, 'handgun': 3583, 'tpose': 8217, 'postcard': 6156, 'argentina': 408, 'bakery': 612, 'dragon': 2400, 'feilizer': 2942, 'envy': 2659, 'bunk': 1088, 'stae': 7603, 'sleve': 7358, 'minuet': 5141, 'messiah': 5070, 'testament': 8046, 'afghani': 136, 'drone': 2424, 'ali': 219, 'turks': 8320, 'eld': 2551, 'grill': 3483, 'franks': 3154, 'inflate': 4042, 'conductor': 1665, 'fae': 2861, 'niggas': 5443, 'coz': 1852, 'astronaut': 486, 'wolf': 8893, 'overrate': 5705, 'columbine': 1569, 'slaveholder': 7348, 'negro': 5394, 'uppity': 8524, 'ku': 4497, 'kikes': 4440, 'klan': 4466, 'repost': 6653, 'bashar': 673, 'ghouta': 3346, 'bungee': 1087, 'gook': 3408, 'detector': 2193, 'rapey': 6485, 'minecraft': 5125, 'creeper': 1885, 'petting': 5933, 'breaking': 975, 'propey': 6312, 'cleaning': 1464, 'kneel': 4470, 'zach': 9006, 'galifianaki': 3251, 'repoedly': 6649, 'radicalise': 6447, 'airpo': 189, 'kaaba': 4388, 'altar': 255, 'akhbar': 198, 'preschool': 6212, 'drones': 2425, 'disabled': 2266, 'zebra': 9011, 'cannibal': 1184, 'bucket': 1049, 'kaboom': 4390, 'bloom': 861, 'taliban': 7938, 'years': 8977, 'flesh': 3045, 'clock': 1492, 'arnt': 424, 'yu': 9002, 'dose': 2384, 'approx': 386, 'unravel': 8485, 'paradox': 5776, 'cremate': 1887, 'telephone': 8004, 'mode': 5183, 'compensate': 1618, 'sauce': 6941, 'orphanage': 5662, 'favour': 2919, 'dna': 2346, 'wahog': 8706, 'domination': 2366, 'whitespace': 8833, 'dachau': 1983, 'desse': 2176, 'strudel': 7746, 'giveaway': 3361, 'receptionist': 6528, 'haha': 3555, 'sword': 7906, 'shall': 7134, 'cloudy': 1503, 'retarded': 6709, 'auschvistic': 530, 'split': 7556, 'baender': 601, 'painting': 5744, 'ketchum': 4425, 'mmm': 5178, 'eah': 2477, 'auchwitz': 523, 'gaurd': 3289, 'foo': 3092, 'cannibalism': 1185, 'ft': 3213, 'israeli': 4224, 'kaijew': 4392, 'mcu': 4994, 'wouldn': 8930, 'mas': 4945, 'camping': 1167, 'dent': 2133, 'gastank': 3283, 'attempt': 508, 'choir': 1394, 'jaw': 4268, 'wank': 8721, 'abel': 9, 'definitive': 2087, 'proof': 6306, 'ace': 64, 'au': 522, 'jus': 4382, 'llama': 4712, 'llamaphobia': 4713, 'aboion': 14, 'defuse': 2091, 'dumpsterfire': 2454, 'chaos': 1317, 'hue': 3856, 'atleast': 500, 'visualise': 8674, 'stretch': 7729, 'horizontally': 3819, 'soundproof': 7484, 'shemale': 7168, 'umbilical': 8361, 'foreskin': 3109, 'pizzeria': 6013, 'advent': 121, 'wreath': 8937, 'aharon': 166, 'elijah': 2569, 'shmuel': 7195, 'vw': 8701, 'specifically': 7529, 'insensitive': 4088, 'barbecue': 650, 'obsession': 5562, 'mustard': 5315, 'exhausting': 2783, 'kneeling': 4471, 'posistion': 6147, 'kite': 4461, 'kike': 4439, 'poke': 6084, 'defenition': 2078, 'hasn': 3635, 'legit': 4619, 'punching': 6373, 'cliff': 1480, 'aushwitz': 534, 'candie': 1178, 'loli': 4739, 'poland': 6089, 'exhausted': 2782, 'grenade': 3479, 'biography': 819, 'till': 8137, 'sunset': 7822, 'yamakas': 8965, 'spos': 7567, 'dime': 2248, 'charcoal': 1322, 'preach': 6187, 'brit': 1009, 'refine': 6561, 'alternatively': 260, 'ignite': 3917, 'mein': 5036, 'kampf': 4395, 'allahver': 228, 'edgy': 2509, 'ausschwitz': 535, 'spray': 7573, 'machete': 4817, 'louvre': 4774, 'announcement': 327, 'cheetah': 1356, 'fistful': 3018, 'groud': 3502, 'lyric': 4810, 'owowowowowowow': 5719, 'revenge': 6724, 'madandwohless': 4825, 'speculation': 7533, 'ferraris': 2957, 'ultimate': 8357, 'anal': 293, 'sis': 7301, 'landmine': 4535, 'disguise': 2294, 'mat': 4960, 'prophet': 6314, 'copper': 1781, 'maccas': 4816, 'bun': 1085, 'pedal': 5854, 'porno': 6137, 'hooker': 3810, 'kayla': 4414, 'mueller': 5281, 'islamist': 4214, 'demonstration': 2122, 'shia': 7176, 'peshmerga': 5922, 'times': 8142, 'ceainly': 1277, 'fanatical': 2887, 'idol': 3916, 'smash': 7388, 'assyrian': 482, 'statue': 7646, 'circumambulate': 1428, 'kabba': 4389, 'trench': 8268, 'imitate': 3941, 'mosul': 5250, 'bigotry': 802, 'sexism': 7119, 'hatre': 3641, 'mohamed': 5189, 'cornerstone': 1791, 'crushing': 1930, 'outlet': 5675, 'brutalizing': 1040, 'tikrit': 8134, 'daesh': 1987, 'expe': 2793, 'mohamme': 5191, 'xmas': 8958, 'insufferably': 4111, 'blight': 849, 'slavery': 7349, 'filth': 2988, 'intersecting': 4150, 'turkey': 8318, 'erdogan': 2674, 'unbelievable': 8371, 'barbarity': 649, 'madrassa': 4828, 'cage': 1142, 'bs': 1043, 'organ': 5645, 'reflection': 6564, 'perversion': 5919, 'shopper': 7210, 'ezidis': 2848, 'cult': 1942, 'caoon': 1191, 'satire': 6935, 'incitement': 3989, 'inane': 3976, 'hollande': 3766, 'raqqa': 6489, 'aberration': 10, 'asad': 444, 'autocratic': 545, 'carbon': 1208, 'dilemma': 2243, 'pest': 5924, 'float': 3054, 'butterfly': 1123, 'jab': 4237, 'allcap': 229, 'brick': 993, 'voluntarily': 8690, 'fatal': 2911, 'repeat': 6640, 'freshman': 3180, 'highschool': 3717, 'sry': 7593, 'meanjokes': 5001, 'sub': 7768, 'lube': 4787, 'ste': 7650, 'alternate': 258, 'allcaps': 230, 'atlanta': 498, 'falcon': 2874, 'destruct': 2183, 'transparent': 8249, 'gutter': 3543, 'frenchman': 3175, 'gadget': 3247, 'pedometer': 5859, 'abattoir': 5, 'structural': 7744, 'neighbour': 5399, 'mahid': 4840, 'nope': 5476, 'wo': 8888, 'limb': 4676, 'mus': 5302, 'carr': 1233, 'beucase': 787, 'everytime': 2737, 'setup': 7109, 'vagina': 8561, 'passer': 5802, 'xs': 8960, 'stroke': 7741, 'reverse': 6727, 'grader': 3429, 'videotape': 8637, 'smokin': 7397, 'babe': 579, 'ai': 172, 'adderall': 88, 'genocider': 3316, 'fuhrer': 3220, 'deathly': 2039, 'gallow': 3253, 'harambe': 3610, 'anta': 338, 'mm': 5177, 'plus': 6063, 'gingerbread': 3356, 'jea': 4270, 'mushroom': 5306, 'meaning': 5000, 'usurer': 8551, 'sadly': 6882, 'demon': 2118, 'riddance': 6746, 'enslave': 2640, 'earlock': 2481, 'ale': 210, 'romania': 6804, 'woh': 8891, 'la': 4505, 'fontaine': 3091, 'exclaim': 2768, 'alt': 253, 'protagonist': 6325, 'tank': 7956, 'sting': 7683, 'spot': 7568, 'thailand': 8060, 'gaza': 3291, 'strip': 7737, 'pint': 5999, 'stas': 7635, 'mayr': 4988, 'aificial': 177, 'luckily': 4789, 'hiding': 3712, 'attic': 514, 'stereo': 7664, 'yaaaay': 8962, 'hanz': 3602, 'ze': 9010, 'showerhead': 7226, 'plug': 6057, 'nero': 5411, 'emperor': 2599, 'granduncle': 3447, 'fbi': 2924, 'omar': 5607, 'mateen': 4964, 'carnage': 1223, 'westminster': 8812, 'renew': 6631, 'warming': 8734, 'myth': 5325, 'ariana': 414, 'grande': 3440, 'conce': 1645, 'imam': 3937, 'url': 8540, 'lucky': 4790, 'specify': 7530, 'drawing': 2408, 'colour': 1563, 'rectum': 6547, 'islamaphobic': 4211, 'scissor': 6993, 'defusal': 2090, 'haggle': 3553, 'wht': 8841, 'vape': 8575, 'pokejew': 6086, 'pikajew': 5983, 'meth': 5075, 'methhead': 5076, 'ramadan': 6464, 'diy': 2342, 'heaattack': 3653, 'gasbill': 3278, 'monk': 5218, 'parachute': 5773, 'cmon': 1515, 'bouncer': 934, 'orlando': 5659, 'llah': 4711, 'hashtag': 3634, 'pimp': 5990, 'emergency': 2589, 'istanbul': 4229, 'auschwitistic': 531, 'genital': 3311, 'mutilation': 5318, 'clitoris': 1490, 'scrabble': 7005, 'bin': 814, 'somali': 7459, 'drunken': 2438, 'mohammere': 5193, 'centre': 1293, 'choo': 1397, 'til': 8135, 'pac': 5725, 'pissed': 6006, 'nightmare': 5447, 'cent': 1290, 'zit': 9019, 'cardinal': 1210, 'domestic': 2362, 'dora': 2380, 'doda': 2354, 'exploder': 2807, 'lend': 4626, 'jap': 4262, 'invention': 4173, 'holo': 3771, 'clerk': 1474, 'shwit': 7237, 'cot': 1812, 'paradise': 5775, 'elong': 2574, 'xpost': 8959, 'sickipedia': 7247, 'format': 3124, 'despondent': 2175, 'jerusalem': 4288, 'recognise': 6536, 'london': 4742, 'tragic': 8230, 'reckon': 6534, 'roastme': 6776, 'consent': 1698, 'watchtower': 8756, 'incidentally': 3988, 'unpopular': 8482, 'cruiser': 1926, 'barry': 668, 'mitzvah': 5172, 'yeah': 8974, 'windpipe': 8865, 'zeke': 9012, 'deutschland': 2197, 'ss': 7594, 'subtract': 7784, 'ps': 6347, 'basilica': 676, 'punchline': 6374, 'bw': 1131, 'beetle': 730, 'fromt': 3198, 'exclude': 2769, 'aryan': 442, 'allowance': 241, 'ipod': 4190, 'hysterically': 3901, 'covered': 1843, 'weightlift': 8797, 'hijabi': 3719, 'jeopardy': 4282, 'patronized': 5829, 'wisconsin': 8877, 'rs': 6841, 'um': 8360, 'pas': 5799, 'ma': 4812, 'fundamental': 3230, 'haven': 3645, 'undergraduate': 8397, 'frustrated': 3210, 'denounce': 2129, 'yiddishkeit': 8989, 'spades': 7504, 'stabbing': 7598, 'mainstream': 4849, 'jq': 4352, 'torba': 8190, 'sle': 7351, 'goliath': 3401, 'poxgay': 6176, 'grudge': 3512, 'commute': 1607, 'exploitative': 2809, 'swindle': 7899, 'goyim': 3420, 'packing': 5731, 'moslem': 5246, 'recount': 6543, 'bailout': 607, 'castration': 1253, 'poraye': 6132, 'medium': 5029, 'heuristic': 3706, 'absurd': 30, 'jewsdidthis': 4303, 'bitter': 834, 'cling': 1485, 'dynamite': 2472, 'antipathy': 345, 'cristian': 1901, 'assimilation': 475, 'frustration': 3211, 'otto': 5667, 'warburg': 8725, 'developt': 2206, 'semitism': 7072, 'eachother': 2474, 'mighty': 5101, 'invader': 4168, 'ely': 2578, 'judeo': 4357, 'actual': 80, 'shekel': 7160, 'undermine': 8400, 'pervy': 5921, 'weimerica': 8798, 'duper': 2456, 'backup': 591, 'hollyjewsteinwood': 3769, 'destructive': 2185, 'cuck': 1936, 'confide': 1670, 'holofakes': 3775, 'series': 7094, 'holofake': 3774, 'gab': 3245, 'searchbar': 7029, 'conservacuck': 1700, 'unconditional': 8386, 'suppo': 7836, 'impoance': 3956, 'muh': 5285, 'sole': 7450, 'visa': 8665, 'shummer': 7234, 'traitor': 8236, 'americas': 283, 'israhell': 4225, 'acually': 82, 'bestiality': 779, 'urine': 8539, 'misogyny': 5153, 'burqa': 1102, 'zog': 9021, 'verge': 8613, 'britfam': 1011, 'govern': 3416, 'censor': 1288, 'uptick': 8533, 'purge': 6385, 'violently': 8658, 'depo': 2146, 'bulldoze': 1072, 'headquaer': 3658, 'smokescreen': 7396, 'commie': 1596, 'propaganda': 6307, 'wham': 8816, 'quisling': 6427, 'accountable': 59, 'ideal': 3908, 'whistle': 8831, 'bullshtte': 1078, 'contrary': 1740, 'foueen': 3135, 'underage': 8394, 'browner': 1029, 'worser': 8923, 'worstest': 8925, 'lobby': 4722, 'sovereign': 7495, 'fcn': 2925, 'cos': 1803, 'defect': 2075, 'fatality': 2912, 'nhs': 5430, 'surviving': 7866, 'memree': 5048, 'founders': 3138, 'texaschurchmassacre': 8051, 'wifi': 8847, 'invisible': 4181, 'banislam': 633, 'speakfreely': 7520, 'jewry': 4301, 'marxism': 4941, 'feelsgoodman': 2940, 'png': 6067, 'ville': 8649, 'ppl': 6177, 'suppoer': 7837, 'dissaray': 2321, 'hallow': 3569, 'flutter': 3073, 'pro': 6267, 'leaveiranalone': 4599, 'mere': 5060, 'formality': 3122, 'identify': 3910, 'narrative': 5344, 'fleece': 3044, 'indefinitely': 4001, 'bullshit': 1077, 'satanic': 6930, 'pedofile': 5858, 'soros': 7476, 'twatter': 8330, 'wetback': 8814, 'opsec': 5631, 'nationalism': 5352, 'multicultural': 5291, 'melting': 5040, 'utopia': 8554, 'agitate': 158, 'stir': 7687, 'bannon': 637, 'henry': 3695, 'kissinger': 4459, 'kushner': 4501, 'altright': 262, 'supremacy': 7846, 'nigel': 5438, 'farage': 2893, 'kokesh': 4485, 'molyneux': 5200, 'preheat': 6207, 'minion': 5134, 'shut': 7235, 'enable': 2607, 'invasion': 4170, 'bigot': 801, 'tolerance': 8171, 'inclusion': 3991, 'swampwhite': 7879, 'equivalent': 2669, 'nobilitation': 5456, 'americafirst': 279, 'americansfirst': 282, 'maga': 4830, 'systematically': 7918, 'dethrone': 2196, 'bottomless': 931, 'merkel': 5064, 'grip': 3488, 'berlin': 774, 'transition': 8247, 'cnn': 1516, 'translation': 8248, 'jong': 4335, 'warp': 8739, 'londoner': 4743, 'invade': 4167, 'degeneracy': 2092, 'seperate': 7086, 'peg': 5868, 'incest': 3985, 'trannie': 8239, 'insee': 4087, 'fahkin': 2864, 'depoation': 2147, 'gentile': 3319, 'retake': 6706, 'zionism': 9016, 'behaviour': 743, 'planned': 6024, 'parenthood': 5785, 'guideline': 3525, 'determine': 2194, 'mutilated': 5317, 'rothschilds': 6825, 'twitter': 8336, 'facebook': 2852, 'shill': 7182, 'islamization': 4215, 'gasajewforjesus': 3277, 'nee': 5383, 'google': 3407, 'bastards': 682, 'molotov': 5199, 'amerike': 284, 'rodinu': 6793, 'wellfare': 8805, 'shitistan': 7194, 'trillion': 8278, 'saviour': 6951, 'halal': 3564, 'yrs': 9001, 'tij': 8133, 'simplify': 7285, 'normie': 5482, 'tgsnt': 8057, 'beholden': 745, 'aipac': 180, 'stephanie': 7662, 'schriock': 6987, 'strategist': 7716, 'lampshade': 4530, 'tutorial': 8325, 'uncomfoable': 8383, 'muzziepedos': 5320, 'reactionary': 6503, 'apa': 356, 'donkey': 2371, 'francis': 3150, 'founder': 3137, 'banu': 639, 'qurayza': 6431, 'indiana': 4008, 'whip': 8828, 'synonymous': 7913, 'moron': 5242, 'loudmouth': 4769, 'screwer': 7014, 'herod': 3701, 'beeyoch': 731, 'babydick': 581, 'babyfa': 582, 'establishment': 2702, 'harm': 3623, 'purpoe': 6387, 'defensive': 2081, 'criticism': 1904, 'restriction': 6702, 'extend': 2821, 'branch': 962, 'coalition': 1520, 'eradicate': 2672, 'rumor': 6853, 'nominate': 5466, 'powell': 6171, 'slaughterhouse': 7346, 'tuesday': 8310, 'lust': 4803, 'hehe': 3681, 'mongrel': 5214, 'aryans': 443, 'media': 5015, 'holohoax': 3777, 'homosexuality': 3795, 'propoion': 6315, 'representative': 6655, 'jewess': 4298, 'albright': 206, 'pagan': 5737, 'stormfront': 7703, 'gradualy': 3432, 'repoe': 6648, 'cleansing': 1468, 'capitulate': 1200, 'galway': 3254, 'suppoive': 7838, 'katz': 4412, 'oppose': 5623, 'extermination': 2825, 'js': 4353, 'effos': 2534, 'fold': 3082, 'talmudivision': 7947, 'maial': 4841, 'mossad': 5249, 'groid': 3493, 'dice': 2221, 'labeling': 4508, 'exclusive': 2770, 'soo': 7470, 'dissaprove': 2320, 'ratajczak': 6493, 'dese': 2159, 'hmm': 3751, 'rant': 6482, 'ns': 5521, 'bbc': 695, 'marxist': 4942, 'mill': 5116, 'libtard': 4651, 'purpoed': 6388, 'vile': 8646, 'despicable': 2172, 'talmud': 7946, 'goodness': 3405, 'convinced': 1765, 'superiority': 7827, 'fidelity': 2971, 'intimmidate': 4157, 'cronulla': 1910, 'saver': 6949, 'thi': 8080, 'medicene': 5020, 'silja': 7273, 'europa': 2723, 'stockholm': 7690, 'cossack': 1808, 'effo': 2531, 'demonize': 2120, 'propagate': 6308, 'holocau': 3772, 'analysis': 295, 'btw': 1044, 'accompaigne': 52, 'clumpsy': 1508, 'yep': 8985, 'vulnerable': 8700, 'glamorise': 3366, 'fox': 3140, 'unwatchable': 8517, 'greece': 3465, 'zionist': 9017, 'fishman': 3016, 'brooklyn': 1023, 'ledven': 4605, 'cresskill': 1889, 'latvian': 4564, 'prague': 6181, 'negroid': 5395, 'eahly': 2478, 'jewtube': 4306, 'selve': 7069, 'oppounity': 5626, 'guise': 3529, 'condensene': 1658, 'thou': 8099, 'holdomor': 3762, 'willingly': 8857, 'lentin': 4630, 'www': 8954, 'normanfinkelstein': 5481, 'com': 1571, 'arrogance': 429, 'mask': 4948, 'klike': 4467, 'nig': 5437, 'tralee': 8237, 'disgrace': 2292, 'nowthe': 5519, 'ireland': 4196, 'desecrate': 2160, 'sos': 7480, 'aicle': 173, 'fag': 2863, 'anderson': 304, 'abraham': 20, 'falsely': 2877, 'ishmael': 4208, 'bolsheviks': 884, 'commissars': 1597, 'homosexual': 3794, 'coupling': 1832, 'masquerade': 4949, 'dumping': 2451, 'printing': 6256, 'purpoise': 6389, 'literal': 4697, 'demonic': 2119, 'yea': 8973, 'resistance': 6680, 'stomach': 7693, 'parody': 5790, 'nato': 5359, 'paly': 5758, 'fault': 2916, 'imperial': 3952, 'nordic': 5477, 'deviant': 2207, 'unnatural': 8470, 'abomination': 17, 'islamisation': 4213, 'licker': 4653, 'dup': 2455, 'travesty': 8259, 'fawning': 2921, 'thetruthseeker': 8078, 'reproduce': 6660, 'patriot': 5825, 'endgame': 2614, 'bullsh': 1076, 'solely': 7451, 'ted': 7997, 'er': 2670, 'bankroll': 636, 'making': 4859, 'gut': 3540, 'insult': 4114, 'awaken': 565, 'ple': 6044, 'murders': 5300, 'entice': 2648, 'compatible': 1616, 'isle': 4218, 'ff': 2965, 'complicit': 1636, 'betrayal': 784, 'tragedy': 8229, 'clip': 1487, 'tail': 7928, 'forskin': 3128, 'hyperinflation': 3896, 'scandinavian': 6962, 'hershey': 3703, 'mop': 5233, 'escalator': 2691, 'poisonous': 6082, 'minister': 5136, 'fri': 3184, 'mar': 4905, 'shatter': 7154, 'wrecker': 8938, 'rag': 6450, 'kruger': 4496, 'reverence': 6726, 'holahoax': 3760, 'pact': 5733, 'jewdo': 4294, 'putinistan': 6402, 'aka': 195, 'federation': 2933, 'sicken': 7245, 'sitcom': 7305, 'lololo': 4741, 'mongering': 5212, 'deprivation': 2151, 'wohy': 8892, 'protesting': 6333, 'institutionalized': 4106, 'indoctrinate': 4016, 'jewlord': 4300, 'apelantic': 361, 'films': 2987, 'revisionist': 6729, 'ceain': 1276, 'republicans': 6663, 'whitey': 8834, 'inceldom': 3984, 'stub': 7749, 'stance': 7614, 'tallcel': 7944, 'incel': 3983, 'hypersexuality': 3897, 'promiscuity': 6299, 'eid': 2542, 'manhood': 4883, 'murderous': 5299, 'hollow': 3767, 'forgive': 3116, 'scenario': 6971, 'retweete': 6718, 'mccain': 4992, 'lickspittle': 4654, 'naqvi': 5340, 'heptullah': 3696, 'shahnawaz': 7131, 'herd': 3697, 'communion': 1603, 'submarine': 7774, 'seaman': 7026, 'confess': 1668, 'masturbate': 4958, 'pancake': 5762, 'lazytown': 4582, 'erect': 2675, 'halloween': 3570, 'pacman': 5732, 'solid': 7453, 'officially': 5590, 'fuckin': 3217, 'overdone': 5691, 'complement': 1627, 'powder': 6170, 'joule': 4343, 'proverbs': 6340, 'denial': 2125, 'shepherd': 7171, 'uni': 8437, 'holographic': 3776, 'gt': 3515, 'planted': 6027, 'psychotic': 6354, 'thirst': 8089, 'waterboarde': 8758, 'stratosphere': 7719, 'countdown': 1823, 'vacuum': 8559, 'cleaner': 1463, 'snuck': 7423, 'finnish': 3002, 'faxe': 2922, 'takeaway': 7931, 'boob': 900, 'tit': 8158, 'roundhouse': 6831, 'bombdefuse': 888, 'simplified': 7284, 'spelling': 7539, 'override': 5706, 'allegiance': 232, 'royale': 6839, 'flash': 3032, 'jewdism': 4293, 'bisexual': 827, 'coarse': 1521, 'himler': 3726, 'blitzkrieg': 851, 'locomotive': 4730, 'carriage': 1235, 'infinity': 4041, 'netherlands': 5417, 'ahme': 170, 'bombay': 887, 'boyscout': 950, 'pssssssst': 6349, 'lil': 4674, 'pump': 6369, 'lodging': 4731, 'headquaers': 3659, 'remotely': 6628, 'broach': 1015, 'pc': 5841, 'hamas': 3575, 'schizophrenic': 6979, 'nipple': 5454, 'engine': 2625, 'os': 5663, 'samsung': 6912, 'charger': 1325, 'ugh': 8351, 'orphan': 5661, 'drool': 2426, 'overdue': 5693, 'essay': 2698, 'comfoable': 1583, 'headmaster': 3657, 'assembly': 467, 'bombshell': 892, 'enought': 2636, 'paki': 5747, 'remembrance': 6624, 'wil': 8848, 'nt': 5522, 'belive': 753, 'lad': 4518, 'overlook': 5697, 'bastard': 681, 'luger': 4793, 'tourette': 8202, 'glee': 3372, 'hocaust': 3755, 'coon': 1776, 'proudly': 6337, 'gua': 3517, 'lolface': 4738, 'tinder': 8149, 'crucify': 1922, 'whoa': 8836, 'osama': 5664, 'organizer': 5650, 'poc': 6068, 'oxymoron': 5721, 'robot': 6785, 'bot': 926, 'yazidis': 8972, 'hindus': 3731, 'pattern': 5830, 'muzzrat': 5321, 'mah': 4839, 'username': 8547, 'hollyjewd': 3768, 'wit': 8881, 'mouf': 5261, 'allegiant': 233, 'wahhabis': 8705, 'impose': 3962, 'diktat': 2242, 'joshua': 4342, 'avraham': 563, 'shekelberg': 7161, 'bloc': 852, 'anarchist': 299, 'sjw': 7313, 'ep': 2660, 'anncoulter': 324, 'delta': 2109, 'additional': 94, 'abt': 31, 'antisemitism': 347, 'evolution': 2744, 'nfl': 5429, 'boo': 899, 'fucken': 3216, 'playa': 6035, 'soy': 7500, 'dinosaur': 2255, 'syndicate': 7912, 'anglian': 310, 'ing': 4050, 'pricipal': 6240, 'binder': 817, 'genocidal': 3314, 'inherit': 4061, 'slovenia': 7375, 'subhuman': 7771, 'thirsty': 8090, 'imbecile': 3938, 'stooge': 7697, 'tragically': 8231, 'claw': 1460, 'bruise': 1035, 'mph': 5277, 'applique': 376, 'banana': 626, 'carrot': 1237, 'lighting': 4666, 'sour': 7486, 'patch': 5815, 'gps': 3422, 'incorrect': 3996, 'pierre': 5976, 'unrelated': 8487, 'baroque': 664, 'romanticist': 6806, 'crust': 1931, 'marble': 4907, 'meow': 5054, 'chopper': 1401, 'slump': 7379, 'moldy': 5196, 'brutality': 1039, 'seashell': 7030, 'poker': 6088, 'sofa': 7441, 'garfield': 3272, 'lasagna': 4551, 'southeastern': 7490, 'tile': 8136, 'metamorphose': 5073, 'larvae': 4549, 'raccoon': 6436, 'nocturnal': 5460, 'toothbrush': 8186, 'compass': 1613, 'silica': 7271, 'hamster': 3578, 'smelt': 7391, 'elderberry': 2552, 'keystroke': 4430, 'pupil': 6380, 'alphabet': 251, 'shade': 7128, 'refrie': 6567, 'shallow': 7135, 'turk': 8317, 'supplemental': 7833, 'disney': 2307, 'koala': 4483, 'eucalyptus': 2719, 'coconut': 1531, 'unicorn': 8438, 'salt': 6905, 'sparkle': 7513, 'ribbon': 6738, 'silk': 7274, 'masaai': 4946, 'nest': 5414, 'purple': 6386, 'closer': 1497, 'electrician': 2558, 'printer': 6255, 'ink': 4070, 'digital': 2241, 'battery': 691, 'secure': 7048, 'mattress': 4979, 'tibia': 8123, 'bone': 894, 'lighten': 4664, 'liquid': 4689, 'gaseous': 3279, 'grey': 3482, 'vermont': 8615, 'russell': 6862, 'terrier': 8034, 'generator': 3305, 'diesel': 2230, 'fuel': 3219, 'rigatoni': 6751, 'washer': 8748, 'curtain': 1965, 'poodle': 6118, 'rio': 6760, 'connected': 1690, 'sock': 7438, 'barbie': 656, 'pudding': 6361, 'appetite': 371, 'monopoly': 5222, 'soup': 7485, 'cocoa': 1530, 'puff': 6363, 'indonesia': 4017, 'pen': 5870, 'retractable': 6714, 'mammal': 4871, 'parasite': 5781, 'contamination': 1724, 'thunderstorm': 8119, 'descendant': 2155, 'iowa': 4187, 'cabbage': 1134, 'edm': 2512, 'starbuck': 7623, 'hoop': 3813, 'underway': 8407, 'sandal': 6917, 'durable': 2457, 'seinfeld': 7058, 'finale': 2991, 'viewer': 8642, 'copier': 1780, 'severythe': 7112, 'ufo': 8350, 'glaze': 3370, 'sprinkler': 7576, 'pogo': 6078, 'pixel': 6011, 'corn': 1789, 'magnet': 4836, 'notebook': 5504, 'bookshelf': 904, 'bounce': 933, 'sharpie': 7152, 'wheel': 8819, 'judgement': 4359, 'batman': 689, 'bruce': 1033, 'wayne': 8768, 'bert': 775, 'ernie': 2683, 'foam': 3078, 'stadium': 7602, 'disintegrate': 2305, 'rapidly': 6486, 'pickup': 5968, 'blanket': 841, 'bubble': 1045, 'tangerine': 7953, 'seashore': 7031, 'satisfying': 6938, 'gasoline': 3280, 'southernmost': 7492, 'peacock': 5849, 'plumage': 6058, 'typhoon': 8340, 'bamboo': 624, 'sacramento': 6873, 'handmaid': 3590, 'uno': 8474, 'tater': 7969, 'tot': 8194, 'stonehenge': 7695, 'cats': 1263, 'purr': 6394, 'llhasa': 4715, 'ac': 38, 'dc': 2025, 'scratch': 7008, 'aardvark': 0, 'woah': 8889, 'andes': 305, 'giza': 3362, 'paranormal': 5780, 'positioning': 6149, 'constellation': 1713, 'orion': 5658, 'recycling': 6550, 'thursdays': 8121, 'murdock': 5301, 'rambo': 6466, 'debt': 2041, 'castle': 1252, 'situ': 7309, 'archaeologist': 399, 'amazon': 272, 'pacific': 5727, 'tango': 7954, 'vespa': 8623, 'beak': 698, 'serena': 7091, 'safelite': 6887, 'windshield': 8866, 'arizona': 417, 'sunrise': 7821, 'skid': 7320, 'pave': 5834, 'moth': 5252, 'coonskin': 1777, 'issac': 4227, 'jello': 4275, 'skunk': 7331, 'campsite': 1168, 'lecarre': 4603, 'pulitzer': 6366, 'shellfish': 7164, 'grizzly': 3490, 'kodiak': 4484, 'carousal': 1227, 'rabbit': 6435, 'flea': 3042, 'september': 7090, 'protein': 6329, 'cruncy': 1928, 'cereal': 1297, 'soggy': 7446, 'beatle': 709, 'las': 4550, 'seatbelt': 7036, 'incans': 3978, 'grower': 3509, 'cup': 1950, 'zombie': 9022, 'leash': 4596, 'untie': 8507, 'obsolete': 5564, 'fog': 3081, 'appendage': 369, 'leaf': 4589, 'giraffe': 3357, 'tornado': 8191, 'amendment': 276, 'bush': 1109, 'fulfilling': 3224, 'becalm': 713, 'latitude': 4563, 'wheelchair': 8820, 'panda': 5763, 'footprint': 3099, 'desalination': 2153, 'seawater': 7037, 'kennedy': 4420, 'nava': 5365, 'teethe': 8001, 'nut': 5534, 'ef': 2521, 'scale': 6955, 'weaken': 8771, 'mainland': 4847, 'refund': 6572, 'piccolo': 5964, 'racket': 6444, 'chicago': 1372, 'secede': 7038, 'false': 2876, 'misconception': 5146, 'antebellum': 342, 'dorian': 2381, 'siege': 7252, 'bahama': 605, 'frito': 3193, 'houseplant': 3848, 'peach': 5848, 'cupboard': 1951, 'toaster': 8164, 'futile': 3241, 'swam': 7878, 'antarctica': 340, 'popsicle': 6126, 'int': 4117, 'fonda': 3089, 'ape': 360, 'length': 4627, 'uneven': 8421, 'abominable': 16, 'trumpet': 8300, 'mos': 5244, 'submit': 7777, 'compose': 1639, 'tortilla': 8192, 'backstab': 589, 'pee': 5863, 'duck': 2442, 'egret': 2538, 'sail': 6891, 'tylenol': 8337, 'bovine': 937, 'means': 5002, 'popcorn': 6124, 'cushion': 1969, 'fluffy': 3069, 'tariff': 7961, 'retaliate': 6707, 'cheerio': 1350, 'cranjus': 1863, 'mcbasketlball': 4991, 'zip': 9018, 'skydive': 7333, 'yara': 8967, 'abolitionist': 15, 'lemur': 4624, 'madagascar': 4823, 'slander': 7337, 'libel': 4646, 'runny': 6858, 'petroleum': 5932, 'fre': 3159, 'volt': 8688, 'pigment': 5981, 'greener': 3472, 'rooster': 6817, 'appendix': 370, 'bagel': 603, 'improper': 3972, 'alignment': 223, 'buckeye': 1050, 'lysol': 4811, 'bacteria': 596, 'cuddly': 1938, 'coke': 1541, 'mitochondria': 5169, 'majestic': 4852, 'snail': 7406, 'oxygen': 5720, 'enlightenment': 2634, 'frosted': 3201, 'flake': 3027, 'weasel': 8779, 'surfing': 7854, 'vine': 8653, 'slumber': 7378, 'pajama': 5746, 'unite': 8455, 'venison': 8604, 'corgi': 1788, 'mocking': 5182, 'peen': 5866, 'hammer': 3577, 'nba': 5372, 'phase': 5938, 'fied': 2972, 'chrome': 1412, 'firefox': 3006, 'web': 8782, 'browser': 1032, 'countless': 1826, 'swiss': 7901, 'fondue': 3090, 'ninjas': 5453, 'logitech': 4733, 'cycle': 1977, 'rebirth': 6520, 'frost': 3200, 'granny': 3448, 'deeply': 2070, 'panorama': 5769, 'extinguisher': 2830, 'oak': 5542, 'stove': 7707, 'bubblegum': 1046, 'peel': 5865, 'asphalt': 461, 'cone': 1666, 'juul': 4386, 'mango': 4881, 'pod': 6070, 'tomato': 8176, 'provoke': 6344, 'eagle': 2476, 'spagetti': 7505, 'gravy': 3458, 'aide': 175, 'teller': 8008, 'macbook': 4815, 'surface': 7852, 'goldfish': 3399, 'kimchi': 4447, 'probiotic': 6270, 'diabete': 2215, 'washing': 8749, 'convenient': 1752, 'evanovich': 2728, 'mystery': 5324, 'predispose': 6198, 'pepsi': 5884, 'extinct': 2828, 'asteroid': 483, 'yucatan': 9003, 'greenland': 3474, 'alligator': 239, 'hula': 3860, 'relieve': 6612, 'boredom': 918, 'worm': 8920, 'shiny': 7187, 'dogs': 2358, 'avocado': 559, 'pretzel': 6232, 'dolphin': 2361, 'mousse': 5267, 'file': 2983, 'monitor': 5217, 'misdirection': 5147, 'plastic': 6028, 'breathing': 983, 'penguin': 5874, 'antartica': 341, 'czechoslovakia': 1981, 'safaris': 6885, 'july': 4371, 'parking': 5788, 'otter': 5666, 'miami': 5089, 'dining': 2253, 'volcano': 8685, 'eruption': 2690, 'outdoors': 5671, 'mammoth': 4872, 'carowind': 1228, 'carolina': 1226, 'getaway': 3339, 'piglet': 5980, 'cauliflower': 1268, 'ketchup': 4426, 'jumper': 4374, 'croak': 1906, 'jonas': 4333, 'brothers': 1027, 'chemotherapy': 1365, 'cilantro': 1423, 'laundromat': 4568, 'cashew': 1247, 'pineapple': 5994, 'fried': 3187, 'pan': 5761, 'sopranos': 7473, 'releasing': 6605, 'conditioning': 1662, 'southwest': 7493, 'domesticate': 2363, 'den': 2124, 'fortnite': 3130, 'videogame': 8636, 'coyote': 1851, 'numb': 5526, 'blueberry': 866, 'simba': 7279, 'demagnetized': 2110, 'eveyone': 2738, 'meatloaf': 5008, 'pheasant': 5940, 'bulletin': 1074, 'capybara': 1204, 'rodent': 6792, 'beep': 728, 'weeding': 8789, 'pumpkin': 6370, 'mulan': 5288, 'caffeine': 1141, 'cinematic': 1426, 'dispute': 2317, 'sony': 7469, 'flooding': 3057, 'alarm': 201, 'violin': 8660, 'mint': 5140, 'lava': 4571, 'crown': 1920, 'genesis': 3309, 'crossword': 1916, 'brownie': 1030, 'patio': 5822, 'artificial': 436, 'stormy': 7704, 'factor': 2857, 'ply': 6065, 'vampire': 8572, 'dairy': 1990, 'pencil': 5873, 'fetch': 2962, 'shark': 7148, 'blac': 835, 'chyna': 1419, 'bleach': 844, 'keyboard': 4429, 'med': 5013, 'frequent': 3176, 'bumper': 1083, 'marionberry': 4919, 'pillow': 5987, 'stiletto': 7680, 'surf': 7851, 'decker': 2054, 'refrigeration': 6569, 'conditioner': 1661, 'bait': 608, 'nike': 5450, 'kibble': 4433, 'erupt': 2689, 'spew': 7544, 'pony': 6116, 'buttery': 1124, 'yarn': 8970, 'heeled': 3680, 'diaper': 2218, 'duvet': 2465, 'pillowcase': 5988, 'shortcake': 7215, 'connecticut': 1691, 'hartford': 3631, 'efficiency': 2528, 'autopsy': 551, 'whopper': 8839, 'paneer': 5765, 'superman': 7828, 'bulldog': 1070, 'tuna': 8312, 'rose': 6820, 'manifest': 4886, 'destiny': 2180, 'ideological': 3911, 'framework': 3147, 'expansion': 2792, 'jackie': 4240, 'fuzzy': 3244, 'noun': 5511, 'butts': 1127, 'drown': 2431, 'implement': 3954, 'ordinance': 5642, 'scooter': 6996, 'jollof': 4331, 'jambalaya': 4255, 'mermaid': 5065, 'fad': 2859, 'crocodile': 1909, 'van': 8573, 'gecko': 3293, 'chromosome': 1413, 'yello': 8980, 'biking': 806, 'meyer': 5087, 'lemon': 4622, 'ant': 337, 'gummy': 3535, 'divorce': 2341, 'sandwhich': 6918, 'shoelace': 7202, 'sliver': 7369, 'brightness': 1000, 'nighttime': 5449, 'arena': 407, 'axis': 578, 'affection': 131, 'graze': 3460, 'meadow': 4995, 'deficiency': 2084, 'sardine': 6926, 'palpitation': 5757, 'notre': 5510, 'dame': 1994, 'louisville': 4771, 'sept': 7089, 'oregon': 5643, 'hulu': 3862, 'dvd': 2466, 'squirrel': 7591, 'shinnig': 7186, 'brightly': 999, 'venezuela': 8602, 'hugo': 3859, 'chavez': 1341, 'ramble': 6465, 'landfall': 4533, 'dock': 2348, 'miner': 5126, 'chili': 1381, 'stem': 7660, 'eu': 2718, 'october': 5577, 'jambalya': 4256, 'milkshake': 5115, 'satellite': 6932, 'jelly': 4276, 'massage': 4953, 'therapist': 8075, 'sore': 7474, 'floss': 3062, 'import': 3959, 'sharpener': 7151, 'bonfire': 896, 'devour': 2210, 'korea': 4489, 'ryugyong': 6871, 'doom': 2373, 'tallest': 7945, 'unoccupied': 8475, 'frozen': 3203, 'pea': 5843, 'cinema': 1425, 'protester': 6332, 'hon': 3796, 'kong': 4487, 'lemonade': 4623, 'unintentional': 8448, 'anchor': 302, 'rustle': 6867, 'errand': 2685, 'bass': 680, 'shuttle': 7236, 'wool': 8905, 'drastic': 2405, 'owl': 5717, 'rollercoaster': 6800, 'iced': 3903, 'flush': 3071, 'robins': 6783, 'brexit': 990, 'ladder': 4519, 'rihanna': 6755, 'hairspray': 3561, 'butthole': 1125, 'comet': 1582, 'gymnastic': 3546, 'forge': 3113, 'component': 1638, 'grassland': 3452, 'pete': 5927, 'freely': 3166, 'poop': 6120, 'pug': 6364, 'diving': 2339, 'saxophone': 6953, 'woodwind': 8904, 'binary': 815, 'tennessee': 8019, 'primate': 6248, 'gore': 3409, 'lunatic': 4798, 'lake': 4523, 'starwar': 7634, 'preschooler': 6213, 'fil': 2982, 'badminton': 600, 'ford': 3102, 'adoptable': 112, 'tummy': 8311, 'carpeting': 1232, 'memorize': 5046, 'almond': 245, 'wrapping': 8936, 'benz': 771, 'beanie': 701, 'puke': 6365, 'hallway': 3571, 'playroom': 6042, 'stamp': 7613, 'walled': 8715, 'hose': 3828, 'quack': 6407, 'statistical': 7643, 'analyst': 296, 'statistician': 7645, 'fixture': 3023, 'clarity': 1450, 'carat': 1206, 'crayon': 1870, 'momentous': 5204, 'llamas': 4714, 'yum': 9004, 'webcam': 8783, 'pine': 5993, 'dingo': 2252, 'classify': 1454, 'porcupine': 6134, 'salty': 6906, 'sterile': 7667, 'sadio': 6880, 'mane': 4879, 'liverpool': 4706, 'giants': 3349, 'eli': 2568, 'manning': 4895, 'bedtime': 722, 'campfire': 1166, 'ringing': 6758, 'udon': 8349, 'alaska': 202, 'density': 2132, 'sightly': 7257, 'scorpion': 6999, 'venom': 8605, 'maketh': 4857, 'lasagne': 4552, 'chirp': 1391, 'saddam': 6878, 'hussein': 3887, 'sparkly': 7515, 'gouge': 3413, 'heathen': 3672, 'temperament': 8011, 'oreo': 5644, 'hydration': 3891, 'pavilion': 5835, 'slush': 7380, 'kangaroos': 4397, 'fm': 3076, 'druid': 2435, 'carrey': 1234, 'rainfall': 6457, 'hello': 3687, 'clumsey': 1509, 'pelt': 5869, 'citrus': 1438, 'lollipop': 4740, 'mount': 5262, 'fuji': 3221, 'unplanned': 8480, 'pluto': 6064, 'solar': 7448, 'akashi': 196, 'kaikyo': 4393, 'wax': 8766, 'beehive': 726, 'handwoven': 3593, 'bluegrass': 867, 'calcium': 1145, 'tator': 7970, 'folger': 3083, 'allergy': 236, 'hinge': 3732, 'fend': 2951, 'burp': 1101, 'frizzy': 3194, 'whisper': 8830, 'los': 4759, 'angeles': 308, 'clipper': 1489, 'magpie': 4838, 'corvidae': 1802, 'cranberry': 1862, 'cartoon': 1243, 'podcast': 6071, 'hive': 3750, 'meteor': 5074, 'telescope': 8005, 'yogurt': 8993, 'ehler': 2541, 'danlos': 2007, 'painful': 5741, 'exhaust': 2781, 'glove': 3384, 'additive': 95, 'unsinkable': 8496, 'windmill': 8863, 'grind': 3487, 'wheat': 8818, 'vanilla': 8574, 'volume': 8689, 'laser': 4553, 'crave': 1868, 'velvet': 8600, 'elvis': 2577, 'presley': 6224, 'retriever': 6716, 'papercut': 5772, 'effectiveness': 2525, 'cubs': 1935, 'spongebob': 7560, 'dare': 2008, 'dopamine': 2378, 'serotonin': 7097, 'delectable': 2097, 'necessity': 5380, 'ozone': 5723, 'iguana': 3922, 'mackinac': 4821, 'alwaays': 266, 'karl': 4403, 'marx': 4940, 'sociology': 7437, 'cyrodil': 1980, 'thalmor': 8061, 'cob': 1527, 'neil': 5402, 'armstrong': 422, 'mandela': 4878, 'soil': 7447, 'ohio': 5596, 'columbus': 1570, 'garlic': 3273, 'legend': 4615, 'zig': 9015, 'zag': 9008, 'starseed': 7627, 'bowling': 941, 'pepperonis': 5883, 'mlb': 5176, 'rookie': 6814, 'vaunted': 8587, 'playstation': 6043, 'bandicoot': 629, 'madison': 4826, 'trout': 8294, 'jamaica': 4252, 'roku': 6796, 'volley': 8687, 'refrigerate': 6568, 'odor': 5581, 'hummingbird': 3872, 'idaho': 3906, 'bottled': 930, 'shrew': 7228, 'buffalo': 1057, 'bison': 829, 'pedicure': 5857, 'roadway': 6773, 'toni': 8181, 'morrison': 5243, 'redcoat': 6552, 'revolutionary': 6731, 'lumbar': 4795, 'elbow': 2550, 'weenus': 8793, 'playoff': 6041, 'vineyard': 8654, 'bedspread': 721, 'ver': 8610, 'junk': 4379, 'eyesight': 2846, 'indigestion': 4012, 'constipation': 1714, 'hong': 3801, 'milton': 5119, 'epic': 2661, 'liquor': 4690, 'godzilla': 3393, 'cub': 1934, 'carnivore': 1224, 'metropolitan': 5082, 'vaccine': 8558, 'byproduct': 1132, 'underrated': 8404, 'remake': 6620, 'wavy': 8765, 'alfredo': 217, 'wrapper': 8935, 'edgar': 2507, 'allen': 234, 'poe': 6073, 'functional': 3227, 'sephora': 7087, 'password': 5806, 'quilt': 6426, 'handmade': 3589, 'moviw': 5274, 'rockefeller': 6788, 'diego': 2229, 'zookeeper': 9025, 'marilyn': 4916, 'monroe': 5223, 'dimaggio': 2247, 'silky': 7275, 'upside': 8530, 'deliberately': 2100, 'thursday': 8120, 'disastrous': 2276, 'monster': 5225, 'exclusively': 2771, 'balance': 614, 'cheetos': 1358, 'supposedly': 7843, 'unconscious': 8387, 'subsequently': 7781, 'similarly': 7282, 'damned': 1996, 'deeper': 2069, 'unk': 8461, 'characteristic': 1320, 'shrews': 7230, 'undoubtedly': 8413, 'respectively': 6691, 'infinitely': 4040, 'continual': 1731, 'roughly': 6828, 'greenhouse': 3473, 'reportedly': 6652, 'somewhat': 7465, 'practically': 6179, 'ranking': 6480, 'freeway': 3169, 'eliminate': 2570, 'unnamed': 8469, 'iron': 4199, 'de': 2026, 'runaway': 6855, 'routinely': 6835, 'buster': 1116, 'alwaay': 265, 'occasionally': 5570, 'overwatch': 5711, 'coloured': 1564, 'creamy': 1874, 'yellowstone': 8982, 'racing': 6441, 'marker': 4922, 'reliably': 6608, 'versatile': 8617, 'downstairs': 2393, 'canopy': 1188, 'evidently': 2742, 'unintentionally': 8449, 'sled': 7353, 'abdoman': 7, 'remarkably': 6622, 'dreadful': 2409, 'layer': 4580, 'atom': 502, 'internal': 4140, 'casually': 1255, 'purposely': 6393, 'emoji': 2591, 'pure': 6383, 'substitute': 7782, 'amazingly': 271, 'cooler': 1775, 'effectively': 2524, 'meditation': 5025, 'luna': 4797, 'underneath': 8401, 'possibly': 6154, 'strictly': 7732, 'southeast': 7489, 'developmental': 2205, 'correspond': 1798, 'kinda': 4449, 'organizational': 5647, 'rubber': 6843, 'intentionally': 4128, 'individually': 4015, 'temp': 8009, 'harmless': 3624, 'hind': 3727, 'beneath': 764, 'intestinal': 4154, 'purport': 6390, 'antarctic': 339, 'graft': 3434, 'nutritional': 5536, 'arbitrarily': 397, 'ridiculously': 6749, 'irrational': 4201, 'fiercely': 2975, 'permanently': 5900, 'excruciating': 2772, 'partially': 5792, 'glowing': 3385, 'inland': 4071, 'personally': 5913, 'retention': 6710, 'console': 1708, 'sketch': 7318, 'globally': 3377, 'decrease': 2060, 'fully': 3225, 'abundant': 33, 'net': 5416, 'statewide': 7639, 'gum': 3534, 'continuously': 1734, 'frontal': 3199, 'luther': 4805, 'marshall': 4931, 'brilliantly': 1004, 'unilaterally': 8441, 'vigorously': 8644, 'universally': 8457, 'info': 4045, 'bananas': 627, 'unexpectedly': 8422, 'fortified': 3129, 'trading': 8223, 'ensure': 2641, 'cleveland': 1475, 'homophobia': 3792, 'scientifically': 6991, 'calf': 1151, 'yorkshire': 8997, 'formally': 3123, 'recurrent': 6548, 'likewise': 4672, 'mindfulness': 5123, 'tentative': 8023, 'silently': 7269, 'cameron': 1161, 'optical': 5632, 'adjoin': 102, 'thirteen': 8091, 'annually': 331, 'bulldogs': 1071, 'fairy': 2870, 'gently': 3324, 'external': 2827, 'kirk': 4457, 'liver': 4705, 'max': 4981, 'gran': 3436, 'strangely': 7711, 'customary': 1972, 'severely': 7111, 'consequently': 1699, 'blended': 847, 'tremendously': 8267, 'meowing': 5055, 'secondary': 7040, 'charging': 1326, 'kitty': 4464, 'scramble': 7006, 'brisk': 1007, 'hay': 3650, 'progressively': 6295, 'processing': 6277, 'saturn': 6940, 'northeastern': 5487, 'neatly': 5378, 'glossy': 3383, 'formation': 3125, 'instinctively': 4104, 'spur': 7578, 'sixth': 7311, 'kittens': 4463, 'solicitation': 7452, 'cerebral': 1298, 'animation': 316, 'distinctly': 2325, 'continually': 1732, 'pal': 5750, 'dangerously': 2005, 'cortex': 1801, 'upfront': 8521, 'purposefully': 6392, 'declare': 2055, 'weakening': 8772, 'sample': 6911, 'disposable': 2315, 'shared': 7145, 'concrete': 1656, 'uncover': 8389, 'dramatically': 2403, 'boldly': 883, 'virtually': 8664, 'efficiently': 2530, 'precisely': 6193, 'arguably': 409, 'triple': 8282, 'kansas': 4398, 'watson': 8763, 'unnecessarily': 8471, 'internationally': 4143, 'louis': 4770, 'radically': 6448, 'etched': 2707, 'creek': 1883, 'definitively': 2088, 'smoothly': 7402, 'harlem': 3622, 'allegedly': 231, 'spinal': 7550, 'industrial': 4021, 'invariably': 4169, 'maximum': 4982, 'potentially': 6163, 'signal': 7260, 'atop': 503, 'darle': 2011, 'extraordinarily': 2832, 'permission': 5901, 'crucial': 1921, 'interior': 4138, 'transfer': 8242, 'adapter': 86, 'rightly': 6753, 'subconscious': 7769, 'securely': 7049, 'olivia': 5604, 'faintly': 2867, 'legacy': 4611, 'preferably': 6202, 'hoover': 3814, 'purely': 6384, 'consistently': 1707, 'gus': 3539, 'reasonably': 6516, 'biological': 820, 'rust': 6866, 'disaster': 2275, 'pesky': 5923, 'portfolio': 6140, 'momentarily': 5203, 'selectively': 7061, 'slice': 7359, 'fourth': 3139, 'icy': 3905, 'terrified': 8035, 'uniquely': 8453, 'boobs': 901, 'airborne': 183, 'hawaiian': 3648, 'tue': 8309, 'bbq': 696, 'application': 375, 'sonic': 7468, 'interconnected': 4133, 'damp': 1997, 'slate': 7343, 'pri': 6238, 'wartime': 8741, 'bay': 694, 'settling': 7108, 'estimate': 2705, 'nationwide': 5357, 'quantity': 6412, 'vastly': 8585, 'improvement': 3974, 'wilson': 8858, 'yo': 8991, 'spectacular': 7532, 'exceedingly': 2755, 'fiber': 2967, 'uniformly': 8440, 'devastated': 2199, 'nutritious': 5537, 'gap': 3264, 'sticky': 7678, 'farther': 2901, 'perennial': 5888, 'panel': 5766, 'miniature': 5131, 'observe': 5560, 'onion': 5609, 'synthetic': 7914, 'armor': 421, 'roger': 6794, 'neigh': 5396, 'unto': 8509, 'spawn': 7517, 'tap': 7958, 'steven': 7671, 'aluminum': 264, 'mitochondrial': 5170, 'functionally': 3228, 'rex': 6734, 'perpetually': 5903, 'nationally': 5355, 'surely': 7850, 'anytime': 353, 'squash': 7585, 'cosmic': 1806, 'initially': 4064, 'freshly': 3179, 'companion': 1608, 'quicker': 6422, 'mi': 5088, 'gf': 3341, 'fro': 3195, 'paragraph': 5777, 'turbulent': 8316, 'indoor': 4018, 'frantically': 3155, 'channel': 1316, 'unequivocally': 8420, 'twilight': 8334, 'mechanically': 5012, 'vacuumed': 8560, 'lone': 4744, 'deliberate': 2099, 'developed': 2202, 'washed': 8747, 'industrialize': 4022, 'ballet': 620, 'numerous': 5528, 'recycle': 6549, 'nw': 5538, 'vox': 8696, 'thicker': 8082, 'wireless': 8876, 'footstep': 3100, 'celestial': 1284, 'softly': 7443, 'hydrogen': 3892, 'injured': 4067, 'infection': 4035, 'postseason': 6159, 'internally': 4141, 'click': 1478, 'titanium': 8160, 'seed': 7052, 'cave': 1271, 'nervously': 5413, 'offshore': 5591, 'maryland': 4944, 'locally': 4725, 'proverbial': 6339, 'visually': 8675, 'philippines': 5945, 'interface': 4137, 'robinson': 6784, 'maine': 4846, 'moderately': 5186, 'receiver': 6525, 'fluorescent': 3070, 'oz': 5722, 'junior': 4378, 'barrel': 666, 'ongoing': 5608, 'markedly': 4921, 'adventurous': 125, 'southwestern': 7494, 'noticeably': 5506, 'knox': 4482, 'sciences': 6989, 'comparatively': 1610, 'decidedly': 2049, 'left': 4607, 'presently': 6220, 'gradually': 3431, 'teething': 8002, 'extension': 2822, 'gradient': 3430, 'property': 6311, 'imaginable': 3932, 'startup': 7630, 'canine': 1182, 'claws': 1461, 'lens': 4629, 'replacement': 6644, 'conveniently': 1753, 'collectively': 1551, 'surge': 7855, 'ucla': 8348, 'confirmation': 1676, 'vapor': 8576, 'tokyo': 8170, 'rocky': 6791, 'duly': 2446, 'finals': 2993, 'alongside': 246, 'upstairs': 8531, 'hungarian': 3877, 'tamil': 7949, 'predominantly': 6199, 'replica': 6645, 'primal': 6245, 'leigh': 4621, 'beverly': 789, 'micro': 5095, 'transistor': 8246, 'conceal': 1646, 'colourful': 1565, 'vertical': 8621, 'raven': 6497, 'bulk': 1067, 'stuffed': 7757, 'judah': 4355, 'occupied': 5572, 'massachusetts': 4951, 'tin': 8148, 'cooked': 1770, 'fork': 3118, 'pledge': 6050, 'excessively': 2763, 'fischer': 3012, 'ion': 4186, 'deepen': 2068, 'economical': 2501}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csV6SaqwBs4n"
      },
      "source": [
        "y = df.iloc[:,:7].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP6rEXBy1bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0607755-ecf9-4093-8a1b-9fcb5c880837"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 1., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eXIk-uwCzzo"
      },
      "source": [
        "LABEL_COLUMN = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82HlauMuSEl8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(bow,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6B3OVKJDJE1",
        "outputId": "74f434da-5601-4c04-e2ef-714bb12c1920"
      },
      "source": [
        "train_df_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11592, 9029)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_oJ9bqwPpHx",
        "outputId": "92b8a3fb-cf50-45d4-c851-1fa850a956cd"
      },
      "source": [
        "train_df_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11592, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryy3Vts37t-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b664a6c8-d57d-4f6e-facc-53916c1100ca"
      },
      "source": [
        "val_df_text.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2484, 9029)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNs1dyq77x2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5269af27-f86f-4173-d7f5-35e98c4c793e"
      },
      "source": [
        "val_df_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2484, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA67-V-3C9hT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "e3598343-8411-436d-d0e8-c75acb96f9c3"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "multilabel_classifier = MultiOutputClassifier(classifier, n_jobs=-1)\n",
        "# multilabel_classifier = multilabel_classifier.fit(train_df_text,train_df_labels)\n",
        "multilabel_classifier = multilabel_classifier.fit(X_train, train_df_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/multioutput.py\", line 40, in _fit_estimator\n    estimator.fit(X, y)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 635, in fit\n    self._count(X, Y)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\", line 756, in _count\n    check_non_negative(X, \"MultinomialNB (input X)\")\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 994, in check_non_negative\n    raise ValueError(\"Negative values in data passed to %s\" % whom)\nValueError: Negative values in data passed to MultinomialNB (input X)\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-12541d4fd218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmultilabel_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# multilabel_classifier = multilabel_classifier.fit(train_df_text,train_df_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmultilabel_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilabel_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    168\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    169\u001b[0m                 self.estimator, X, y[:, i], sample_weight)\n\u001b[0;32m--> 170\u001b[0;31m             for i in range(y.shape[1]))\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sZjNqQSWnJ"
      },
      "source": [
        "y_test_pred = multilabel_classifier.predict(test_df_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULGQP2I4SWnJ"
      },
      "source": [
        "labels = test_df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvvYaSGSSWnK",
        "outputId": "6e95fbec-37c6-4d3b-ff2d-0142db18304b"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"MultinomialNB_tfidf\",0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.700864 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.322866 \n",
            "\n",
            "\n",
            " hamming_loss : 0.154072 \n",
            "\n",
            "\n",
            " hamming score : 0.478630 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.620035 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.492754 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.534890 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_MultinomialNB_tfidf_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pVkuZovmYuA"
      },
      "source": [
        "### KNN with countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixa4xBnRKK9P"
      },
      "source": [
        "feature_vector_tfidf = pd.concat([df,tf_idf_feature], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "fhyY6llWKK9Q",
        "outputId": "caf1f826-3622-48ee-fe68-30b47c959dc1"
      },
      "source": [
        "feature_vector_tfidf.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "      <th>sentence</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>lemmatized_withStopwords</th>\n",
              "      <th>tokenized_lemmatized_withstopwords</th>\n",
              "      <th>lemma_pos</th>\n",
              "      <th>Ner_tags</th>\n",
              "      <th>tfIdf_aardvark</th>\n",
              "      <th>tfIdf_ab</th>\n",
              "      <th>tfIdf_ababa</th>\n",
              "      <th>tfIdf_aback</th>\n",
              "      <th>tfIdf_abandon</th>\n",
              "      <th>tfIdf_abattoir</th>\n",
              "      <th>tfIdf_abaya</th>\n",
              "      <th>tfIdf_abdoman</th>\n",
              "      <th>tfIdf_abdul</th>\n",
              "      <th>tfIdf_abel</th>\n",
              "      <th>tfIdf_aberration</th>\n",
              "      <th>tfIdf_abide</th>\n",
              "      <th>tfIdf_ability</th>\n",
              "      <th>tfIdf_able</th>\n",
              "      <th>tfIdf_aboion</th>\n",
              "      <th>tfIdf_abolitionist</th>\n",
              "      <th>tfIdf_abominable</th>\n",
              "      <th>tfIdf_abomination</th>\n",
              "      <th>tfIdf_aboout</th>\n",
              "      <th>tfIdf_abouit</th>\n",
              "      <th>tfIdf_abraham</th>\n",
              "      <th>tfIdf_abrasive</th>\n",
              "      <th>tfIdf_abroad</th>\n",
              "      <th>tfIdf_abruptly</th>\n",
              "      <th>tfIdf_absence</th>\n",
              "      <th>tfIdf_absent</th>\n",
              "      <th>...</th>\n",
              "      <th>tfIdf_yiddishkeit</th>\n",
              "      <th>tfIdf_yield</th>\n",
              "      <th>tfIdf_yo</th>\n",
              "      <th>tfIdf_yoga</th>\n",
              "      <th>tfIdf_yogurt</th>\n",
              "      <th>tfIdf_yolanda</th>\n",
              "      <th>tfIdf_york</th>\n",
              "      <th>tfIdf_yorker</th>\n",
              "      <th>tfIdf_yorkshire</th>\n",
              "      <th>tfIdf_young</th>\n",
              "      <th>tfIdf_youth</th>\n",
              "      <th>tfIdf_youtube</th>\n",
              "      <th>tfIdf_yrs</th>\n",
              "      <th>tfIdf_yu</th>\n",
              "      <th>tfIdf_yucatan</th>\n",
              "      <th>tfIdf_yum</th>\n",
              "      <th>tfIdf_yummy</th>\n",
              "      <th>tfIdf_zach</th>\n",
              "      <th>tfIdf_zack</th>\n",
              "      <th>tfIdf_zag</th>\n",
              "      <th>tfIdf_zaknelson</th>\n",
              "      <th>tfIdf_ze</th>\n",
              "      <th>tfIdf_zebra</th>\n",
              "      <th>tfIdf_zeke</th>\n",
              "      <th>tfIdf_zenlike</th>\n",
              "      <th>tfIdf_zero</th>\n",
              "      <th>tfIdf_zig</th>\n",
              "      <th>tfIdf_zionism</th>\n",
              "      <th>tfIdf_zionist</th>\n",
              "      <th>tfIdf_zip</th>\n",
              "      <th>tfIdf_zit</th>\n",
              "      <th>tfIdf_zoey</th>\n",
              "      <th>tfIdf_zog</th>\n",
              "      <th>tfIdf_zombie</th>\n",
              "      <th>tfIdf_zone</th>\n",
              "      <th>tfIdf_zoo</th>\n",
              "      <th>tfIdf_zookeeper</th>\n",
              "      <th>tfIdf_zoos</th>\n",
              "      <th>tfIdf_zumba</th>\n",
              "      <th>tfIdf_zyklon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Many people live in Ethiopia. The people are v...</td>\n",
              "      <td>many people live in ethiopia the people be ver...</td>\n",
              "      <td>[('many', 'JJ'), ('people', 'NNS'), ('live', '...</td>\n",
              "      <td>people live ethiopia people thin good distance...</td>\n",
              "      <td>['people', 'live', 'ethiopia', 'people', 'thin...</td>\n",
              "      <td>[('people', 'NNS'), ('live', 'VBP'), ('ethiopi...</td>\n",
              "      <td>[('ethiopia', 'GPE')]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>My professor is a hispanic man He came here il...</td>\n",
              "      <td>professor be a hispanic man come here illegally</td>\n",
              "      <td>[('professor', 'NNP'), ('be', 'VB'), ('a', 'DT...</td>\n",
              "      <td>professor hispanic man come illegally</td>\n",
              "      <td>['professor', 'hispanic', 'man', 'come', 'ille...</td>\n",
              "      <td>[('professor', 'NNP'), ('hispanic', 'NNP'), ('...</td>\n",
              "      <td>[('hispanic', 'NORP')]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The schoolgirl is walking down the street. She...</td>\n",
              "      <td>the schoolgirl be walk down the street be a ta...</td>\n",
              "      <td>[('the', 'DT'), ('schoolgirl', 'NN'), ('be', '...</td>\n",
              "      <td>schoolgirl walk street target child predator</td>\n",
              "      <td>['schoolgirl', 'walk', 'street', 'target', 'ch...</td>\n",
              "      <td>[('schoolgirl', 'NNP'), ('walk', 'NNP'), ('str...</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>We'll likely fly over Cape Verde on our flight...</td>\n",
              "      <td>will likely fly over cape verde on flight tomo...</td>\n",
              "      <td>[('will', 'MD'), ('likely', 'RB'), ('fly', 'VB...</td>\n",
              "      <td>likely fly cape verde flight tomorrow bet peop...</td>\n",
              "      <td>['likely', 'fly', 'cape', 'verde', 'flight', '...</td>\n",
              "      <td>[('likely', 'RB'), ('fly', 'VB'), ('cape', 'NN...</td>\n",
              "      <td>[('tomorrow', 'DATE')]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I know many people who are Russian. They love ...</td>\n",
              "      <td>know many people who be russian love vodka</td>\n",
              "      <td>[('know', 'VBP'), ('many', 'JJ'), ('people', '...</td>\n",
              "      <td>know people russian love vodka</td>\n",
              "      <td>['know', 'people', 'russian', 'love', 'vodka']</td>\n",
              "      <td>[('know', 'VBP'), ('people', 'NNS'), ('russian...</td>\n",
              "      <td>[('russian', 'NORP')]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 9043 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ethnicity  gender  profession  ...  tfIdf_zoos  tfIdf_zumba  tfIdf_zyklon\n",
              "0        1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "1        1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "2        0.0     1.0         0.0  ...         0.0          0.0           0.0\n",
              "3        1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "4        1.0     0.0         0.0  ...         0.0          0.0           0.0\n",
              "\n",
              "[5 rows x 9043 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ0hwUJCJGSu"
      },
      "source": [
        "y = feature_vector_tfidf.iloc[:,:7].values\n",
        "X = feature_vector_tfidf.iloc[:,15:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvZ9bqZMJOz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604692e1-7b17-4b72-b624-5fc1b287bc27"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 1., 0.],\n",
              "       [1., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 1., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq_Z03xeKpIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7ac677-90bd-4df0-90e4-4711494e18ec"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obt0FjxlIln1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SLG7XYbS8Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046a51c6-2c1c-49c2-ba1b-3bfa59667c1b"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "classifier.fit(train_df_text,train_df_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NbTUw7FEcK5"
      },
      "source": [
        "y_test_pred = classifier.predict(test_df_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKyyICucEcK5"
      },
      "source": [
        "labels = test_df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cedxn9z9EcK5",
        "outputId": "71311599-c375-42ac-af33-0bbd1b222542"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"KNN_countVectorizer\",0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.579531 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.260870 \n",
            "\n",
            "\n",
            " hamming_loss : 0.257189 \n",
            "\n",
            "\n",
            " hamming score : 0.299852 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.330515 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.308172 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.315620 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_KNN_countVectorizer_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVZ73QiQEu-y"
      },
      "source": [
        "### Decision Tree with CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ17849TEkg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44cb7f7-32e1-4cca-9d53-e7ad9a6cb320"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(train_df_text,train_df_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=0, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WS9gV60E-lX"
      },
      "source": [
        "y_test_pred = classifier.predict(test_df_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryFfysgVE-lY"
      },
      "source": [
        "labels = test_df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRG7l7UGE-lY",
        "outputId": "051326a3-5954-41ed-f5b0-78008df0a2a0"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"DecisionTree_tfidf\",0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.821131 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.583736 \n",
            "\n",
            "\n",
            " hamming_loss : 0.137911 \n",
            "\n",
            "\n",
            " hamming score : 0.691291 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.745169 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.744968 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.745035 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_DecisionTree_tfidf_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Kt4CwOF03V"
      },
      "source": [
        "### Random_forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9C8qCUCFBL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b199b0-552f-46fa-84da-de27d28068c9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(train_df_text,train_df_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etWvIOLVGNxJ"
      },
      "source": [
        "y_test_pred = classifier.predict(test_df_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4qUqDe4GNxK"
      },
      "source": [
        "labels = test_df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q1LeL37GNxL",
        "outputId": "d25900a3-c217-4738-b55c-67088c93add1"
      },
      "source": [
        "write_to_file = True\n",
        "classification_metrics(y_test_pred,labels,\"RandomForest_tfidf\",0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n",
            "\n",
            " ROC-AUC score: 0.791064 \n",
            "\n",
            "\n",
            " Subset accuracy : 0.479871 \n",
            "\n",
            "\n",
            " hamming_loss : 0.137624 \n",
            "\n",
            "\n",
            " hamming score : 0.627281 \n",
            "\n",
            "\n",
            " sample average  precision_sample_average : 0.736514 \n",
            "\n",
            "\n",
            " sample average  recall_sample_average : 0.665459 \n",
            "\n",
            "\n",
            " sample average  f1_sample_average : 0.689144 \n",
            "\n",
            "  Saving the metrics into a file: eval_results_RandomForest_tfidf_0.5_.json with threshold :0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c_x9bekUAxT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}