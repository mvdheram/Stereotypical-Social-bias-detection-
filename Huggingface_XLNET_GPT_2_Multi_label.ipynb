{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Huggingface XLNET GPT-2 Multi-label.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Huggingface_XLNET_GPT_2_Multi_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_l-enkMLVXe",
        "outputId": "1ece355d-52d8-4171-b854-6989c5947a36"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  4 13:01:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHvFW0K8Qf9g",
        "outputId": "e508c362-ea0f-495c-de48-c98dc3e22b48"
      },
      "source": [
        "!pip install transformers==4.5.1 --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 25.6 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPQDIfhHGP57"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
        "                          PreTrainedModel,\n",
        "                          TrainingArguments, Trainer)\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2ForSequenceClassification\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import AdamW\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
        "import logging\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from torch import nn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latBo6gOXV_6"
      },
      "source": [
        "# Loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbi59jjRy-G"
      },
      "source": [
        "df_ohe = pd.read_csv('/content/ohe_multilabel.csv', index_col = 0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfBi2dX95LrJ"
      },
      "source": [
        "y = df_ohe.iloc[:,:-1].values\n",
        "X = df_ohe.iloc[:,-1].values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHbSsoPyH2-"
      },
      "source": [
        "MAX_LEN = 50\n",
        "RANDOM_SEED = 47"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAMi3VQ5yzOy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(X,y, test_size=0.3, random_state=RANDOM_SEED, stratify = y)\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOlETKwG51gQ"
      },
      "source": [
        "LABEL_COLUMNS = ['Ethnicity',\t'gender'\t,'profession'\t,'religion',\t'Anti-stereotype',\t'stereotype',\t'unrelated']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i83d58mP5tBG"
      },
      "source": [
        "train_df_labels = pd.DataFrame(train_df_labels, columns= LABEL_COLUMNS)\n",
        "val_df_labels = pd.DataFrame(val_df_labels, columns= LABEL_COLUMNS)\n",
        "test_df_labels = pd.DataFrame(test_df_labels, columns= LABEL_COLUMNS)\n",
        "train_df_text = pd.DataFrame(train_df_text, columns = ['sentence'])\n",
        "val_df_text = pd.DataFrame(val_df_text, columns = ['sentence'])\n",
        "test_df_text = pd.DataFrame(test_df_text, columns = ['sentence'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZXdFymNjYQ4"
      },
      "source": [
        "train_df = pd.concat([train_df_text,train_df_labels],axis=1)\n",
        "val_df = pd.concat([val_df_text,val_df_labels], axis = 1)\n",
        "test_df = pd.concat([test_df_text,test_df_labels], axis = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJKNm_r2TCDz",
        "outputId": "9d4d3914-c59b-477f-c7ce-539e21dae932"
      },
      "source": [
        "train_df.shape, val_df.shape, test_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11580, 8), (2482, 8), (2482, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "QYqI8rg2eEeN",
        "outputId": "296c40eb-6453-48b3-dd27-41616bf202ab"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The grandfather dined at the restaurant. He en...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Albeit unattractive, her scars served as a sta...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there is a new middle eastern ice cream compan...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>while they are in prison the kikes should get ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Historian was a teacher He loved history a...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11575</th>\n",
              "      <td>Most butchers are window.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11576</th>\n",
              "      <td>Muslim households are very polyester places.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11577</th>\n",
              "      <td>Yemen is near away from california</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11578</th>\n",
              "      <td>The shirt is too wet to wear.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11579</th>\n",
              "      <td>The telephone's constant ringing is annoying.</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11580 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  ...  unrelated\n",
              "0      The grandfather dined at the restaurant. He en...  ...        0.0\n",
              "1      Albeit unattractive, her scars served as a sta...  ...        0.0\n",
              "2      there is a new middle eastern ice cream compan...  ...        0.0\n",
              "3      while they are in prison the kikes should get ...  ...        0.0\n",
              "4      The Historian was a teacher He loved history a...  ...        0.0\n",
              "...                                                  ...  ...        ...\n",
              "11575                          Most butchers are window.  ...        1.0\n",
              "11576       Muslim households are very polyester places.  ...        1.0\n",
              "11577                 Yemen is near away from california  ...        0.0\n",
              "11578                      The shirt is too wet to wear.  ...        1.0\n",
              "11579      The telephone's constant ringing is annoying.  ...        1.0\n",
              "\n",
              "[11580 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmh6bYApXUhR"
      },
      "source": [
        "# Tokenize and encode using pytorch datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYoVLpnSdGmS"
      },
      "source": [
        "class ExplicitStereotypeDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data: pd.DataFrame, tokenizer,max_token_len: int = 50):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.max_token_len = max_token_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "    text = data_row[0]\n",
        "    # labels = data_row[2]\n",
        "    labels = list(data_row.iloc[1:].to_dict().values()) # To handle one-hot encoded categorical values [0-8] \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_token_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding= True,\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return dict(\n",
        "      # text=text,\n",
        "      input_ids=encoding[\"input_ids\"].flatten(),\n",
        "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "      labels= torch.FloatTensor(labels)\n",
        "    )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4X5MKSAc9Ga"
      },
      "source": [
        "def create_train_val_datasets(tokenizer):\n",
        "\n",
        "  train_dataset = ExplicitStereotypeDataset(train_df, tokenizer, max_token_len=MAX_LEN)\n",
        "  val_dataset = ExplicitStereotypeDataset(val_df, tokenizer, max_token_len=MAX_LEN)\n",
        "  test_dataset = ExplicitStereotypeDataset(test_df, tokenizer, max_token_len=MAX_LEN)\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldeHdDQZde-P"
      },
      "source": [
        "# Fine-tuning \n",
        "\n",
        "Fine-tuning for Multi-Label classification can be done by either \n",
        "\n",
        "1. Creating a model that overrides the `forward` method of huggingface transformers with \n",
        "  * Appropriate pooling\n",
        "  * Loss function : `torch.nn.BCEWithLogitsLoss()`\n",
        "2. Creating a custom `trainer` that overrides `compute_loss`\n",
        "\n",
        "Reference:\n",
        "\n",
        "1. Huggingface : https://colab.research.google.com/drive/1X7l8pM6t4VLqxQVJ23ssIxmrsc4Kpc5q?usp=sharing#scrollTo=XZEN8MhaL54M\n",
        "2. https://github.com/gkebe/mlmc/blob/master/mlmc_class.py\n",
        "\n",
        "\n",
        "Methodology :\n",
        "\n",
        "* Using the pooling used in huggingface transformers for sequence classification \n",
        "* Overriding the `compute_loss` of `trainer` class. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z76Gk9XHPEZc"
      },
      "source": [
        "## XLnet, GPT-2\n",
        "\n",
        "Method:\n",
        "  * Creating a custom `trainer` that overrides `compute_loss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjp4jyI7dCtl"
      },
      "source": [
        "class MultilabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
        "                        labels.float().view(-1, self.model.config.num_labels))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa-kncSoPTfR"
      },
      "source": [
        "## GPT2\n",
        "\n",
        "Method:\n",
        "  * Creating method that overrides `forward` method \n",
        "\n",
        "  Look into : https://github.com/huggingface/transformers/issues/3168"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjJ2qHL9Pf-M"
      },
      "source": [
        "class GPT2ForMultiLabelSequenceClassification(GPT2ForSequenceClassification):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = transformer_outputs[0]\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size, sequence_length = input_ids.shape[:2]\n",
        "        else:\n",
        "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
        "\n",
        "        assert (\n",
        "            self.config.pad_token_id is not None or batch_size == 1\n",
        "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "                logger.warning(\n",
        "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
        "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
        "                )\n",
        "\n",
        "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = BCEWithLogitsLoss()\n",
        "        #Changes: labels vector is extended to the number labels instead of 1\n",
        "            loss = loss_fct(pooled_logits.view(-1, self.num_labels),\n",
        "                            pooled_logits.view(-1, self.num_labels).type_as(logits.view(-1, self.num_labels)))\n",
        "            \n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzhy5JdFTH5-"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luNhFIDfUxxU"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3paH1VqU8aL"
      },
      "source": [
        "LABELS = ['Ethnicity','gender','profession','religion','Anti-stereotype','stereotype','unrelated']"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHt7W1ljoZu6"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report,hamming_loss, roc_auc_score, accuracy_score,multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "upper, lower = 1, 0\n",
        "\n",
        "def classification_metrics(test_pred,labels,model_name,threshold,label_names = LABELS):\n",
        "\n",
        "  print(\"Evaluation metrics for test set:\")\n",
        "  y_pred = np.where(test_pred > threshold, upper, lower)\n",
        "  ROC_AUC_score = roc_auc_score(test_df_labels, test_pred)\n",
        "  accuracy = accuracy_score(labels, y_pred)\n",
        "  hloss = hamming_loss(labels, y_pred)\n",
        "  cr = classification_report(labels, y_pred, labels=list(range(len(label_names))), target_names=label_names, output_dict=True)\n",
        "  cf = multilabel_confusion_matrix(test_df_labels, \n",
        "  y_pred)\n",
        "\n",
        "  recall_macro = recall_score(labels, y_pred, average=\"macro\")\n",
        "  precision_macro = precision_score(labels, y_pred, average=\"macro\")\n",
        "  f1_macro = f1_score(labels, y_pred, average=\"macro\")\n",
        "  \n",
        "  model_metrics = {}\n",
        "  model_metrics[\"AUC_ROC_score\"] = ROC_AUC_score\n",
        "  model_metrics[\"subset_accuracy\"] = accuracy\n",
        "  model_metrics[\"hamming_loss\"]= hloss\n",
        "\n",
        "  if write_to_file:\n",
        "    model_metrics[\"Classification_report\"] = cr\n",
        "\n",
        "    for i,val in enumerate(LABEL_COLUMNS):\n",
        "      model_metrics['confusion_matrix' + '_' + val] = str(cf[i].flatten())\n",
        "  \n",
        "    model_metrics[\"y_pred\"] = str(y_pred)\n",
        "    model_metrics[\"y_labels\"] = str(test_df_labels)\n",
        "\n",
        "\n",
        "    if threshold != 0.5:\n",
        "      th = \"calculated_threshold\"\n",
        "    else:\n",
        "      th = threshold\n",
        "\n",
        "    model_metrics[\"threshold\"] = th\n",
        "    output_file = \"eval_results_\" + model_name + \"_\"+str(th) +\"_\"+ \".json\"\n",
        "    \n",
        "    with open(output_file, \"w\" ) as writer:\n",
        "        json.dump(model_metrics,writer)\n",
        "  \n",
        "  return model_metrics\n",
        "  # print(\"\\n ROC-AUC score: %.6f \\n\" % (ROC_AUC_score))\n",
        "  # print(\"\\n Subset accuracy : %.6f \\n\" % (accuracy))\n",
        "  # print(\"\\n hamming_loss : %.6f \\n\" % (hloss))\n",
        "\n",
        "  # print(\"  Saving the metrics into a file: \" + output_file + \" with threshold :\" + str(threshold))"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0K6faAuogef"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return classification_metrics(predictions,labels,MODEL_NAME,0.5)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_tlFJp2T_RX"
      },
      "source": [
        "## XLNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GRmqfA6TVsh"
      },
      "source": [
        "# Number of epochs \n",
        "N_EPOCHS = 2\n",
        "\n",
        "# Batch_size \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Model name \n",
        "MODEL = 'xlnet-base-cased'\n",
        "\n",
        "MODEL_NAME = 'xlnet'\n",
        "\n",
        "# Learning rate \n",
        "learning_rate = 2.49816047538945e-05\n",
        "\n",
        "# Number of labels \n",
        "num_labels = 7"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOdpSOKlTYay",
        "outputId": "3bb88338-0355-4dd7-b7ab-afa7e05f6abe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels).to('cuda')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l1P7dUUUi94"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH1KJEg_UIDs"
      },
      "source": [
        "batch_size = BATCH_SIZE\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"stereotype_classification\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=N_EPOCHS,\n",
        "    weight_decay=0.01\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1jdw-WdzKHv"
      },
      "source": [
        "train_dataset,val_dataset, test_dataset = create_train_val_datasets(tokenizer)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSqaP9K_3ZZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a8c70d-799f-4bb4-d2a3-6c23f6d07da0"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.ExplicitStereotypeDataset at 0x7fbf82cec790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhYyHd1m0ENu"
      },
      "source": [
        "sample = train_dataset[0]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObmHmJ4s2wpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d30c27-6e36-4805-bcd4-981792cf9de3"
      },
      "source": [
        "sample['labels']"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pu48Sx7UBmB"
      },
      "source": [
        "multi_trainer = MultilabelTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPYsFuqnUoab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "ed04d1b1-72d7-41af-b293-819a23bbe5b7"
      },
      "source": [
        "write_to_file = False # Disable logging the metrics to file \n",
        "multi_trainer.train()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='724' max='724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [724/724 10:36, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Auc Roc Score</th>\n",
              "      <th>Subset Accuracy</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.234616</td>\n",
              "      <td>0.505025</td>\n",
              "      <td>0.514907</td>\n",
              "      <td>0.107862</td>\n",
              "      <td>15.102200</td>\n",
              "      <td>164.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.288100</td>\n",
              "      <td>0.207713</td>\n",
              "      <td>0.502472</td>\n",
              "      <td>0.569299</td>\n",
              "      <td>0.096581</td>\n",
              "      <td>15.030100</td>\n",
              "      <td>165.135000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=724, training_loss=0.2600429123936437, metrics={'train_runtime': 637.3775, 'train_samples_per_second': 1.136, 'total_flos': 624664919553432.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 8192, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1441792, 'train_mem_gpu_alloc_delta': 1427509248, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 1445477888})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y6Jv-k7rk1Rd",
        "outputId": "add5711b-f30c-4499-d9e5-402172c05a5b"
      },
      "source": [
        "write_to_file = True # Enable logging the metrics to file \n",
        "multi_trainer.evaluate(test_dataset,metric_key_prefix=\"test\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='234' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 04:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"{'Ethnicity': {'precision': 0.9108433734939759, 'recall': 0.9642857142857143, 'f1-score': 0.9368029739776952, 'support': 784}, 'gender': {'precision': 0.850909090909091, 'recall': 0.7697368421052632, 'f1-score': 0.8082901554404146, 'support': 304}, 'profession': {'precision': 0.8705357142857143, 'recall': 0.8351177730192719, 'f1-score': 0.8524590163934426, 'support': 467}, 'religion': {'precision': 0.9862068965517241, 'recall': 0.9761092150170648, 'f1-score': 0.9811320754716981, 'support': 293}, 'Anti-stereotype': {'precision': 0.7899159663865546, 'recall': 0.36246786632390743, 'f1-score': 0.4969162995594713, 'support': 778}, 'stereotype': {'precision': 0.8040540540540541, 'recall': 0.6672897196261682, 'f1-score': 0.7293156281920328, 'support': 1070}, 'unrelated': {'precision': 0.983271375464684, 'recall': 0.8343848580441641, 'f1-score': 0.902730375426621, 'support': 634}, 'micro avg': {'precision': 0.8800330943188086, 'recall': 0.7369515011547344, 'f1-score': 0.8021618903971846, 'support': 4330}, 'macro avg': {'precision': 0.8851052101636855, 'recall': 0.772770284060222, 'f1-score': 0.8153780749230537, 'support': 4330}, 'weighted avg': {'precision': 0.8698759536831204, 'recall': 0.7369515011547344, 'f1-score': 0.7863841706384679, 'support': 4330}, 'samples avg': {'precision': 0.8642224012892828, 'recall': 0.7493956486704271, 'f1-score': 0.7875369325812516, 'support': 4330}}\" of type <class 'dict'> for key \"train/test_Classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1624   74   28  756]\" of type <class 'str'> for key \"train/test_confusion_matrix_Ethnicity\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[2137   41   70  234]\" of type <class 'str'> for key \"train/test_confusion_matrix_gender\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1957   58   77  390]\" of type <class 'str'> for key \"train/test_confusion_matrix_profession\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[2185    4    7  286]\" of type <class 'str'> for key \"train/test_confusion_matrix_religion\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1629   75  496  282]\" of type <class 'str'> for key \"train/test_confusion_matrix_Anti-stereotype\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1238  174  356  714]\" of type <class 'str'> for key \"train/test_confusion_matrix_stereotype\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1839    9  105  529]\" of type <class 'str'> for key \"train/test_confusion_matrix_unrelated\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[[0 0 0 ... 0 0 1]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [1 0 0 ... 1 0 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " [1 0 0 ... 1 0 0]]\" of type <class 'str'> for key \"train/test_y_pred\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\n",
            "0           0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "1           0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "2           0.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "3           0.0     1.0         0.0  ...              0.0         1.0        0.0\n",
            "4           1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "...         ...     ...         ...  ...              ...         ...        ...\n",
            "2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\n",
            "2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\n",
            "\n",
            "[2482 rows x 7 columns]\" of type <class 'str'> for key \"train/test_y_labels\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_mem_cpu_alloc_delta': 0,\n",
              " 'eval_mem_cpu_peaked_delta': 0,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 115342848,\n",
              " 'test_AUC_ROC_score': 0.9541083160855942,\n",
              " 'test_Classification_report': {'Anti-stereotype': {'f1-score': 0.4969162995594713,\n",
              "   'precision': 0.7899159663865546,\n",
              "   'recall': 0.36246786632390743,\n",
              "   'support': 778},\n",
              "  'Ethnicity': {'f1-score': 0.9368029739776952,\n",
              "   'precision': 0.9108433734939759,\n",
              "   'recall': 0.9642857142857143,\n",
              "   'support': 784},\n",
              "  'gender': {'f1-score': 0.8082901554404146,\n",
              "   'precision': 0.850909090909091,\n",
              "   'recall': 0.7697368421052632,\n",
              "   'support': 304},\n",
              "  'macro avg': {'f1-score': 0.8153780749230537,\n",
              "   'precision': 0.8851052101636855,\n",
              "   'recall': 0.772770284060222,\n",
              "   'support': 4330},\n",
              "  'micro avg': {'f1-score': 0.8021618903971846,\n",
              "   'precision': 0.8800330943188086,\n",
              "   'recall': 0.7369515011547344,\n",
              "   'support': 4330},\n",
              "  'profession': {'f1-score': 0.8524590163934426,\n",
              "   'precision': 0.8705357142857143,\n",
              "   'recall': 0.8351177730192719,\n",
              "   'support': 467},\n",
              "  'religion': {'f1-score': 0.9811320754716981,\n",
              "   'precision': 0.9862068965517241,\n",
              "   'recall': 0.9761092150170648,\n",
              "   'support': 293},\n",
              "  'samples avg': {'f1-score': 0.7875369325812516,\n",
              "   'precision': 0.8642224012892828,\n",
              "   'recall': 0.7493956486704271,\n",
              "   'support': 4330},\n",
              "  'stereotype': {'f1-score': 0.7293156281920328,\n",
              "   'precision': 0.8040540540540541,\n",
              "   'recall': 0.6672897196261682,\n",
              "   'support': 1070},\n",
              "  'unrelated': {'f1-score': 0.902730375426621,\n",
              "   'precision': 0.983271375464684,\n",
              "   'recall': 0.8343848580441641,\n",
              "   'support': 634},\n",
              "  'weighted avg': {'f1-score': 0.7863841706384679,\n",
              "   'precision': 0.8698759536831204,\n",
              "   'recall': 0.7369515011547344,\n",
              "   'support': 4330}},\n",
              " 'test_confusion_matrix_Anti-stereotype': '[1629   75  496  282]',\n",
              " 'test_confusion_matrix_Ethnicity': '[1624   74   28  756]',\n",
              " 'test_confusion_matrix_gender': '[2137   41   70  234]',\n",
              " 'test_confusion_matrix_profession': '[1957   58   77  390]',\n",
              " 'test_confusion_matrix_religion': '[2185    4    7  286]',\n",
              " 'test_confusion_matrix_stereotype': '[1238  174  356  714]',\n",
              " 'test_confusion_matrix_unrelated': '[1839    9  105  529]',\n",
              " 'test_hamming_loss': 0.09059514216645562,\n",
              " 'test_loss': 0.1960292011499405,\n",
              " 'test_runtime': 15.4288,\n",
              " 'test_samples_per_second': 160.868,\n",
              " 'test_subset_accuracy': 0.5882352941176471,\n",
              " 'test_threshold': 0.5,\n",
              " 'test_y_labels': '      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\\n0           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n1           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2           0.0     0.0         0.0  ...              0.0         1.0        0.0\\n3           0.0     1.0         0.0  ...              0.0         1.0        0.0\\n4           1.0     0.0         0.0  ...              0.0         1.0        0.0\\n...         ...     ...         ...  ...              ...         ...        ...\\n2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\\n2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\\n\\n[2482 rows x 7 columns]',\n",
              " 'test_y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [1 0 0 ... 1 0 0]\\n [1 0 0 ... 0 1 0]\\n [1 0 0 ... 1 0 0]]'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfvwnG1k_KQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca19944-80d9-42ea-b42f-2a1681261689"
      },
      "source": [
        "!zip -r /content/file.zip /content/stereotype_classification"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/stereotype_classification/ (stored 0%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/ (stored 0%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/trainer_state.json (deflated 49%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/optimizer.pt (deflated 21%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/special_tokens_map.json (deflated 48%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/training_args.bin (deflated 46%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/config.json (deflated 56%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/spiece.model (deflated 49%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/scheduler.pt (deflated 49%)\n",
            "  adding: content/stereotype_classification/checkpoint-500/tokenizer_config.json (deflated 48%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfjkw5Pk_tJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2b09857-dfdf-4adf-f5ff-fa921429b31b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c96c2f39-530d-4f68-a4e2-150cdf42c2f7\", \"file.zip\", 1173303070)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SOZhrnhUqr5"
      },
      "source": [
        "## GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQoFDj_1gd0"
      },
      "source": [
        "# Number of epochs \n",
        "N_EPOCHS = 2\n",
        "\n",
        "# Batch_size \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Model name \n",
        "MODEL = 'gpt2'\n",
        "\n",
        "MODEL_NAME = 'gpt-2'\n",
        "\n",
        "# Learning rate \n",
        "learning_rate = 2.49816047538945e-05\n",
        "\n",
        "# Number of labels \n",
        "num_labels = 7"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIPeaQSbrXnt",
        "outputId": "28feab68-f2c8-48be-95f2-8d725cc71152"
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels).to('cuda')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0KGSCvFAFmo"
      },
      "source": [
        "# model = GPT2ForMultiLabelSequenceClassification.from_pretrained(MODEL, num_labels=num_labels).to('cuda')"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6g7M8_H-VxW"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "special_tokens_dict = {'pad_token': '[PAD]'}\n",
        "\n",
        "# default to left padding\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Define PAD Token = EOS Token = 50256\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfUDTLOR5I7i"
      },
      "source": [
        "# resize model embedding to match new tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# fix model padding token id\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSRT-a47-0wu"
      },
      "source": [
        "batch_size = BATCH_SIZE\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"stereotype_classification_gpt-2\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=N_EPOCHS,\n",
        "    weight_decay=0.01\n",
        ")"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMxFh7RrGl0"
      },
      "source": [
        "train_dataset,val_dataset, test_dataset = create_train_val_datasets(tokenizer)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI10-X_YowwH"
      },
      "source": [
        "multi_trainer = MultilabelTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2tvwbqaz8dv",
        "outputId": "ea7dceaa-07d2-4ba8-f448-179759de3f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "write_to_file = False # Disable logging the metrics to file \n",
        "multi_trainer.train()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='724' max='724' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [724/724 10:21, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Auc Roc Score</th>\n",
              "      <th>Subset Accuracy</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.341349</td>\n",
              "      <td>0.495610</td>\n",
              "      <td>0.247381</td>\n",
              "      <td>0.174571</td>\n",
              "      <td>18.134400</td>\n",
              "      <td>136.867000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.443000</td>\n",
              "      <td>0.308771</td>\n",
              "      <td>0.492717</td>\n",
              "      <td>0.278002</td>\n",
              "      <td>0.158570</td>\n",
              "      <td>18.444400</td>\n",
              "      <td>134.567000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=724, training_loss=0.4092355696535901, metrics={'train_runtime': 622.1446, 'train_samples_per_second': 1.164, 'total_flos': 601079198773248.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -984981504, 'train_mem_gpu_alloc_delta': 1514442240, 'train_mem_cpu_peaked_delta': 985219072, 'train_mem_gpu_peaked_delta': 2092218880})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_bRsghhkH-F",
        "outputId": "715b8617-7b68-4782-e8b3-2d061d116512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "write_to_file = True # Enable logging the metrics to file \n",
        "multi_trainer.evaluate(test_dataset,metric_key_prefix=\"test\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='234' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 03:47]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation metrics for test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Trainer is attempting to log a value of \"{'Ethnicity': {'precision': 0.8746397694524496, 'recall': 0.7742346938775511, 'f1-score': 0.8213802435723951, 'support': 784}, 'gender': {'precision': 0.8524590163934426, 'recall': 0.17105263157894737, 'f1-score': 0.28493150684931506, 'support': 304}, 'profession': {'precision': 0.7324675324675325, 'recall': 0.6038543897216274, 'f1-score': 0.6619718309859154, 'support': 467}, 'religion': {'precision': 0.9249146757679181, 'recall': 0.9249146757679181, 'f1-score': 0.9249146757679181, 'support': 293}, 'Anti-stereotype': {'precision': 0.40476190476190477, 'recall': 0.021850899742930592, 'f1-score': 0.041463414634146344, 'support': 778}, 'stereotype': {'precision': 0.8901098901098901, 'recall': 0.30280373831775703, 'f1-score': 0.45188284518828453, 'support': 1070}, 'unrelated': {'precision': 0.8678861788617886, 'recall': 0.6735015772870663, 'f1-score': 0.7584369449378331, 'support': 634}, 'micro avg': {'precision': 0.8494208494208494, 'recall': 0.45727482678983833, 'f1-score': 0.5945053295301005, 'support': 4330}, 'macro avg': {'precision': 0.7924627096878466, 'recall': 0.4960303723276853, 'f1-score': 0.563568780276544, 'support': 4330}, 'weighted avg': {'precision': 0.7795588082257234, 'recall': 0.45727482678983833, 'f1-score': 0.5328739810948704, 'support': 4330}, 'samples avg': {'precision': 0.6631748589846898, 'recall': 0.48489121676067687, 'f1-score': 0.5437550362610797, 'support': 4330}}\" of type <class 'dict'> for key \"train/test_Classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1611   87  177  607]\" of type <class 'str'> for key \"train/test_confusion_matrix_Ethnicity\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[2169    9  252   52]\" of type <class 'str'> for key \"train/test_confusion_matrix_gender\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1912  103  185  282]\" of type <class 'str'> for key \"train/test_confusion_matrix_profession\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[2167   22   22  271]\" of type <class 'str'> for key \"train/test_confusion_matrix_religion\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1679   25  761   17]\" of type <class 'str'> for key \"train/test_confusion_matrix_Anti-stereotype\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1372   40  746  324]\" of type <class 'str'> for key \"train/test_confusion_matrix_stereotype\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[1783   65  207  427]\" of type <class 'str'> for key \"train/test_confusion_matrix_unrelated\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"[[0 0 0 ... 0 0 1]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]]\" of type <class 'str'> for key \"train/test_y_pred\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\n",
            "0           0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "1           0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "2           0.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "3           0.0     1.0         0.0  ...              0.0         1.0        0.0\n",
            "4           1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "...         ...     ...         ...  ...              ...         ...        ...\n",
            "2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\n",
            "2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\n",
            "2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\n",
            "2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\n",
            "\n",
            "[2482 rows x 7 columns]\" of type <class 'str'> for key \"train/test_y_labels\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_mem_cpu_alloc_delta': 0,\n",
              " 'eval_mem_cpu_peaked_delta': 0,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 283736064,\n",
              " 'test_AUC_ROC_score': 0.8914286769035048,\n",
              " 'test_Classification_report': {'Anti-stereotype': {'f1-score': 0.041463414634146344,\n",
              "   'precision': 0.40476190476190477,\n",
              "   'recall': 0.021850899742930592,\n",
              "   'support': 778},\n",
              "  'Ethnicity': {'f1-score': 0.8213802435723951,\n",
              "   'precision': 0.8746397694524496,\n",
              "   'recall': 0.7742346938775511,\n",
              "   'support': 784},\n",
              "  'gender': {'f1-score': 0.28493150684931506,\n",
              "   'precision': 0.8524590163934426,\n",
              "   'recall': 0.17105263157894737,\n",
              "   'support': 304},\n",
              "  'macro avg': {'f1-score': 0.563568780276544,\n",
              "   'precision': 0.7924627096878466,\n",
              "   'recall': 0.4960303723276853,\n",
              "   'support': 4330},\n",
              "  'micro avg': {'f1-score': 0.5945053295301005,\n",
              "   'precision': 0.8494208494208494,\n",
              "   'recall': 0.45727482678983833,\n",
              "   'support': 4330},\n",
              "  'profession': {'f1-score': 0.6619718309859154,\n",
              "   'precision': 0.7324675324675325,\n",
              "   'recall': 0.6038543897216274,\n",
              "   'support': 467},\n",
              "  'religion': {'f1-score': 0.9249146757679181,\n",
              "   'precision': 0.9249146757679181,\n",
              "   'recall': 0.9249146757679181,\n",
              "   'support': 293},\n",
              "  'samples avg': {'f1-score': 0.5437550362610797,\n",
              "   'precision': 0.6631748589846898,\n",
              "   'recall': 0.48489121676067687,\n",
              "   'support': 4330},\n",
              "  'stereotype': {'f1-score': 0.45188284518828453,\n",
              "   'precision': 0.8901098901098901,\n",
              "   'recall': 0.30280373831775703,\n",
              "   'support': 1070},\n",
              "  'unrelated': {'f1-score': 0.7584369449378331,\n",
              "   'precision': 0.8678861788617886,\n",
              "   'recall': 0.6735015772870663,\n",
              "   'support': 634},\n",
              "  'weighted avg': {'f1-score': 0.5328739810948704,\n",
              "   'precision': 0.7795588082257234,\n",
              "   'recall': 0.45727482678983833,\n",
              "   'support': 4330}},\n",
              " 'test_confusion_matrix_Anti-stereotype': '[1679   25  761   17]',\n",
              " 'test_confusion_matrix_Ethnicity': '[1611   87  177  607]',\n",
              " 'test_confusion_matrix_gender': '[2169    9  252   52]',\n",
              " 'test_confusion_matrix_profession': '[1912  103  185  282]',\n",
              " 'test_confusion_matrix_religion': '[2167   22   22  271]',\n",
              " 'test_confusion_matrix_stereotype': '[1372   40  746  324]',\n",
              " 'test_confusion_matrix_unrelated': '[1783   65  207  427]',\n",
              " 'test_hamming_loss': 0.15546218487394958,\n",
              " 'test_loss': 0.3037174344062805,\n",
              " 'test_runtime': 18.9558,\n",
              " 'test_samples_per_second': 130.936,\n",
              " 'test_subset_accuracy': 0.29331184528605964,\n",
              " 'test_threshold': 0.5,\n",
              " 'test_y_labels': '      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\\n0           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n1           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2           0.0     0.0         0.0  ...              0.0         1.0        0.0\\n3           0.0     1.0         0.0  ...              0.0         1.0        0.0\\n4           1.0     0.0         0.0  ...              0.0         1.0        0.0\\n...         ...     ...         ...  ...              ...         ...        ...\\n2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\\n2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\\n\\n[2482 rows x 7 columns]',\n",
              " 'test_y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 0 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [1 0 0 ... 0 1 0]\\n [0 0 0 ... 0 0 0]]'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8qbmKghm5Z8",
        "outputId": "631bcd9f-58df-4f82-bdd8-e1dcb174ba97"
      },
      "source": [
        "!zip -r /content/gpt2.zip /content/stereotype_classification_gpt-2"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/stereotype_classification_gpt-2/ (stored 0%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/ (stored 0%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/trainer_state.json (deflated 48%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/optimizer.pt (deflated 31%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/pytorch_model.bin (deflated 9%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/special_tokens_map.json (deflated 60%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/training_args.bin (deflated 46%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/config.json (deflated 56%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/vocab.json (deflated 59%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/merges.txt (deflated 53%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/scheduler.pt (deflated 49%)\n",
            "  adding: content/stereotype_classification_gpt-2/checkpoint-500/tokenizer_config.json (deflated 40%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "aRDdwuovm5aD",
        "outputId": "af3f238b-e575-42da-97ac-79225ca3a94b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6757fb95-b4ec-4fa8-a7e7-61a2e1478714\", \"file.zip\", 1173303070)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}