{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language model and visualization notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x9wPoTMfekOS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Language_model_and_visualization_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2i-Sr614PTb"
      },
      "source": [
        "# BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "Link : https://arxiv.org/pdf/1810.04805.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bBbM9kk4dNL"
      },
      "source": [
        "Bert - **B**idirecitonal **E**ncoder **R**epresentations from **t**ransformers\n",
        "\n",
        "Architecture :\n",
        "  * Multi-layer, **bidirectional** , encoder - based transformer \n",
        "  * BERT-base - 12 Encoder stacks, 768 hidden size, 12 - Self attention head\n",
        "\n",
        "Framework :\n",
        "\n",
        "  * BERT-tokenizer :\n",
        "    * **WordPiece** embeddings \n",
        "    * Vocab size : 30,000\n",
        "    * First token `[CLS]` - The final hidden state of the 12th encoder stack\n",
        "    * Two sequences seperated by `[SEP]` \n",
        "    * End of sequence indicated by `[EOS]`\n",
        "\n",
        "    * Input : Input sentence\n",
        "    * Output : Token embedding + segment embedding + Position embedding \n",
        "    * \"BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vocabulary token.\" [RoBERTa paper]\n",
        "  \n",
        "  * Pre-training\n",
        "    * Task1 : Masked language model (MLM)\n",
        "      * For deep bidirectional representation, mask 15 % of all Wordpiece tokens in each sequence at random and predict the masked tokens.\n",
        "    * Task2 : Next sentence prediction (NSP)\n",
        "      * Trained to understand relationships between two sentences (Q&A, NLI)\n",
        "    * Data\n",
        "      * BookCorpus (800M words)\n",
        "      * English wikipedia (2500M words)\n",
        "  * Fine-tuning\n",
        "    * Plug-in task specific output layer and fine-tune all the parameters end to end.\n",
        "    * At output, the first token i.e `[CLS]` representation is fed into output layer for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3L3CzAu4eKL"
      },
      "source": [
        "# GPT-2\n",
        "\n",
        "Links :\n",
        "\n",
        "1. http://www.persagen.com/files/misc/radford2019language.pdf\n",
        "2. https://youtu.be/Ck9-0YkJD_Q?t=936\n",
        "3. https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf \n",
        "\n",
        "Model specification :\n",
        "  * 12- layer decoder- only transformer \n",
        "  * Masked self-attention heads (768 dimentional states and 12 attention heads)\n",
        "\n",
        "Basic Blocks of GPT architecture [2,3] :\n",
        "  1. GPT2 Tokenizer \n",
        "    * **Input** : Sentence \n",
        "    * **Output** : input_ids, attention_mask, labels \n",
        "    * **Vocab size** : 50k \n",
        "    * **Encoding** : Byte pair encoding (BPE) - \"Middle gorund between character level encoding and word level encoding\" [1] \n",
        "  2. GPT-2 embedding block [2] :\n",
        "      * Consists of \n",
        "        * Word embedding layer \n",
        "        * Position embedding layer\n",
        "      * **Input** : (input_ids, attention_mask, labels)- size (1,8) [*sentence, tokens*]\n",
        "      * **Output** : size (1,8,768) - [*sentence, tokens, embeddings per token*]\n",
        "  3. GPT-2 Decoder Block (x12):\n",
        "    * Consists of \n",
        "      * Attention block [2]:\n",
        "        * Consists of \n",
        "          * Self attention mechanism  ( generating Query, key, value pairs etc. of transformer) \n",
        "      * Multi layer perceptron block/ feedforward layer (MLP - Block)\n",
        "          * Consists of \n",
        "            * layer norm, convolution , activaiton function, dropout \n",
        "      * Layer Normalization \n",
        "  4. LM head layer \n",
        "    * Consists of Linear layer projected to vocab  \n",
        "\n",
        "Training data :\n",
        "  * WebText ( Web pages curated and filtered by humans - 45 million)\n",
        "    * Starting point, scraped outbound links from reddit (>3 karma)\n",
        "\n",
        "GPT2 for text classification :\n",
        "  * Remove the LM head and attach a classification layer with the output dimention equal to size of labels. \n",
        "  * Grab the output of last word embedding in the seqence because it has context information (L-R LM) until that word int he input sequence.\n",
        "    * transformer_output[0] (https://huggingface.co/transformers/_modules/transformers/models/gpt2/modeling_gpt2.html#GPT2ForSequenceClassification) \n",
        "    * Sequence classification architecture:\n",
        "      * Pooled output (last output as mentioned above)\n",
        "      * Dropout(0.1)\n",
        "      * Linear classifier layer with size (input_dim, num_class lables)\n",
        "      * Sigmoid Layer + BCE loss function \n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyiRQ5uL6EAL"
      },
      "source": [
        "# XL-Net\n",
        "\n",
        "Link : https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICnoEFo6BwM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtg-Q1jo6HVy"
      },
      "source": [
        "# RoBERTa \n",
        "\n",
        "Link : https://arxiv.org/abs/1907.11692\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JnFSdIBGDbT"
      },
      "source": [
        "Optimized version of BERT with the following modificatoins:\n",
        "  * Removing next sentence prediction (NSP) \n",
        "  * Training on more data and bigger batches\n",
        "  * Training on longer sequences \n",
        "  * Dynamically changing the masking pattern applied to training data.\n",
        "\n",
        "Data:\n",
        "  * Increase data size to imrpove end task performance \n",
        "  * BookCorpus + Wikipedia : Original data used by BERT \n",
        "  * CC- News : English portion of CommonCrawl News dataset ( \"63 million articles crawled between September 2016 and feb 2019 - 76 GB after filtering\")\n",
        "  * OPENWEBTEXT - \"The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB)\"\n",
        "  * \"STORIES, a dataset introduced in Trinh and Le\n",
        "(2018) containing a subset of CommonCrawl\n",
        "data filtered to match the story-like style of\n",
        "Winograd schemas. (31GB)\".\n",
        "\n",
        "Training procedure :\n",
        "  * Statis (BERT) vs Dynamic (RoBERTa) masking\n",
        "    * Dynamic masking is a strategy where masking patterns are generated for every sequence being fed into the model rather than relying on randomly masking and predicting.\n",
        "  * Model input format and Next sentence prediction \n",
        "    * Doument-sentences : Inputs to the model are packed with full sentences which do not cross document boundaries. Remove NSP loss \n",
        "  * Training with larger batch sizes \n",
        "    * 8k batch size increased when compared to BERT.\n",
        "  * Text encoding :\n",
        "    * Byte Pair encoding [BPE] which relies on subword units extracted from the training corpus compared to  BERT character level "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wPoTMfekOS"
      },
      "source": [
        "# Visualization \n",
        "\n",
        "Link: https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568\n",
        "\n",
        "\n",
        "1. https://jalammar.github.io/explaining-transformers/\n",
        "2. https://jalammar.github.io/hidden-states/\n",
        "3. BerViz : https://github.com/jessevig/bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnkFSkge2TE"
      },
      "source": [
        "Bert-base:\n",
        "  * 12 encoder-layer stack for building contextualized embeddings.\n",
        "  * 100 million tuneable parameters.\n",
        "  * As bert model offers its embeddings to input, its useful to viusalize layers to analyze the patterns learned on unseen data.\n",
        "\n",
        "\n",
        "Why?\n",
        "  * After training viusalize, how well each layer seperates over epochs.\n",
        "\n",
        "How?\n",
        "  * BertForSequenceClassification consists of :\n",
        "    * 1 - BertEmbedding layer -> 12 - Bertlayer -> 1 - Bertpooler -> Tanh - activation -> Dropout layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AwkYpm6M5b"
      },
      "source": [
        "## Compilation of results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQfIxc4HYEio"
      },
      "source": [
        "bert = open('/content/eval_results_BERT_0.5_.json','r')\n",
        "roberta = open('/content/eval_results_RoBERTa_0.5_.json','r')\n",
        "gpt2 = open('/content/eval_results_gpt-2_0.5_.json','r')\n",
        "xlnet = open('/content/eval_results_xlnet_0.5_.json','r')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUcTXUWaJpS"
      },
      "source": [
        "bert_metrics = json.load(bert)\n",
        "roberta_metrics = json.load(roberta)\n",
        "gpt2_metrics = json.load(gpt2)\n",
        "xlnet_metrics = json.load(xlnet)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPzxCq942f-"
      },
      "source": [
        "### Micro_avg_scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5rHpvIaczXV"
      },
      "source": [
        "micro_avg_lms = pd.DataFrame([bert_metrics['Classification_report']['micro avg'],roberta_metrics['Classification_report']['micro avg'],gpt2_metrics['Classification_report']['micro avg'],xlnet_metrics['Classification_report']['micro avg']],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'])"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHb9yRfJlyjv"
      },
      "source": [
        "micro_avg_lms.columns = pd.MultiIndex.from_product([micro_avg_lms.columns, ['micro avg']])"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_bKtSHhnuYT",
        "outputId": "c58c658c-5a95-4627-d794-4172def0ee0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "micro_avg_lms"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.824537</td>\n",
              "      <td>0.822633</td>\n",
              "      <td>0.823584</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.864859</td>\n",
              "      <td>0.869053</td>\n",
              "      <td>0.866951</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.849421</td>\n",
              "      <td>0.457275</td>\n",
              "      <td>0.594505</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.880033</td>\n",
              "      <td>0.736952</td>\n",
              "      <td>0.802162</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  precision    recall  f1-score   support\n",
              "                  micro avg micro avg micro avg micro avg\n",
              "bert-base-uncased  0.824537  0.822633  0.823584      4330\n",
              "roberta-base       0.864859  0.869053  0.866951      4330\n",
              "gpt2               0.849421  0.457275  0.594505      4330\n",
              "xlnet-base-cased   0.880033  0.736952  0.802162      4330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEmRv45d48lH"
      },
      "source": [
        "### Macro_avg_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I7zBk303KI3"
      },
      "source": [
        "macro_avg_lms = pd.DataFrame([bert_metrics['Classification_report']['macro avg'],roberta_metrics['Classification_report']['macro avg'],gpt2_metrics['Classification_report']['macro avg'],xlnet_metrics['Classification_report']['macro avg']],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'])"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aciRTa7a3SZH"
      },
      "source": [
        "macro_avg_lms.columns = pd.MultiIndex.from_product([macro_avg_lms.columns, ['macro avg']])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_c3sIuP3b90",
        "outputId": "df967292-1943-477c-8fac-ba101e7842c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "macro_avg_lms"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.854382</td>\n",
              "      <td>0.842521</td>\n",
              "      <td>0.847510</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.880783</td>\n",
              "      <td>0.884021</td>\n",
              "      <td>0.882198</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.792463</td>\n",
              "      <td>0.496030</td>\n",
              "      <td>0.563569</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.885105</td>\n",
              "      <td>0.772770</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  precision    recall  f1-score   support\n",
              "                  macro avg macro avg macro avg macro avg\n",
              "bert-base-uncased  0.854382  0.842521  0.847510      4330\n",
              "roberta-base       0.880783  0.884021  0.882198      4330\n",
              "gpt2               0.792463  0.496030  0.563569      4330\n",
              "xlnet-base-cased   0.885105  0.772770  0.815378      4330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7xFNtXApdx"
      },
      "source": [
        "### Hamming_loss, subset_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-utZSCDs_pZe"
      },
      "source": [
        "bert = bert_metrics['hamming_loss'],bert_metrics['subset_accuracy'],bert_metrics['AUC_ROC_score']\n",
        "roberta = roberta_metrics['hamming_loss'],roberta_metrics['subset_accuracy'],roberta_metrics['AUC_ROC_score']\n",
        "gpt2 = gpt2_metrics['hamming_loss'],gpt2_metrics['subset_accuracy'],gpt2_metrics['AUC_ROC_score']\n",
        "xlnet = xlnet_metrics['hamming_loss'],xlnet_metrics['subset_accuracy'],xlnet_metrics['AUC_ROC_score']"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H_fewWDJZ8U"
      },
      "source": [
        "hs_metrics = pd.DataFrame([bert,roberta,gpt2,xlnet],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'],columns=['hamming_loss','subset_accuracy','AUC_ROC_score'])"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjK5Ya5bJrxE",
        "outputId": "740e397a-d40d-4e2d-b330-c170aa5f72a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "hs_metrics"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hamming_loss</th>\n",
              "      <th>subset_accuracy</th>\n",
              "      <th>AUC_ROC_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.087832</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.952434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.070911</td>\n",
              "      <td>0.771555</td>\n",
              "      <td>0.966717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.155462</td>\n",
              "      <td>0.293312</td>\n",
              "      <td>0.891429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.090595</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.954108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   hamming_loss  subset_accuracy  AUC_ROC_score\n",
              "bert-base-uncased      0.087832         0.666398       0.952434\n",
              "roberta-base           0.070911         0.771555       0.966717\n",
              "gpt2                   0.155462         0.293312       0.891429\n",
              "xlnet-base-cased       0.090595         0.588235       0.954108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIJD9PYLCzO"
      },
      "source": [
        "### per_class_precision_recall_fmeasure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19FUoVteTqV4"
      },
      "source": [
        "Labels = ['Ethnicity','gender','profession','religion','Anti-stereotype','stereotype']\n",
        "model = ['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased']\n",
        "metrics = ['bert_metrics','roberta_metrics','gpt2_metrics','xlnet_metrics']"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgisvfCLPa2",
        "outputId": "521cf198-1bd4-4dc4-91e4-03c242f8b8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for \n",
        "bert = bert_metrics['Classification_report']['Ethnicity'],bert_metrics['Classification_report']['gender'], bert_metrics['Classification_report']['profession'],bert_metrics['Classification_report']['religion'], bert_metrics['Classification_report']['Anti-stereotype'],bert_metrics['Classification_report']['stereotype'],bert_metrics['Classification_report']['unrelated']"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'precision': 0.9317617866004962, 'recall': 0.9579081632653061, 'f1-score': 0.9446540880503144, 'support': 784}, {'precision': 0.8661971830985915, 'recall': 0.8092105263157895, 'f1-score': 0.8367346938775511, 'support': 304}, {'precision': 0.8682008368200836, 'recall': 0.8886509635974305, 'f1-score': 0.8783068783068781, 'support': 467}, {'precision': 0.9795918367346939, 'recall': 0.9829351535836177, 'f1-score': 0.9812606473594548, 'support': 293}, {'precision': 0.6590584878744651, 'recall': 0.5938303341902313, 'f1-score': 0.6247464503042597, 'support': 778}, {'precision': 0.7173174872665535, 'recall': 0.7897196261682243, 'f1-score': 0.7517793594306049, 'support': 1070}, {'precision': 0.9585492227979274, 'recall': 0.8753943217665615, 'f1-score': 0.9150865622423742, 'support': 634})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-FoMxyHKP_4"
      },
      "source": [
        "### Macro_average_per_class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7lmEhsYLQY3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7CD0MzgzR3x"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "bert = pd.read_json('/content/eval_results_BERT_0.5_.json')\n",
        "roberta = pd.read_json('/content/eval_results_RoBERTa_0.5_.json')\n",
        "gpt2 = pd.read_json('/content/eval_results_gpt-2_0.5_.json')\n",
        "xlnet = pd.read_json('/content/eval_results_xlnet_0.5_.json')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50dcfCeF2Be6",
        "outputId": "34401b1d-0437-417a-f3d2-b326da4abcf9"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUC_ROC_score</th>\n",
              "      <th>subset_accuracy</th>\n",
              "      <th>hamming_loss</th>\n",
              "      <th>Classification_report</th>\n",
              "      <th>confusion_matrix_Ethnicity</th>\n",
              "      <th>confusion_matrix_gender</th>\n",
              "      <th>confusion_matrix_profession</th>\n",
              "      <th>confusion_matrix_religion</th>\n",
              "      <th>confusion_matrix_Anti-stereotype</th>\n",
              "      <th>confusion_matrix_stereotype</th>\n",
              "      <th>confusion_matrix_unrelated</th>\n",
              "      <th>y_pred</th>\n",
              "      <th>y_labels</th>\n",
              "      <th>threshold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.6590584878744651, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ethnicity</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.931761786600496, 'recall': 0.9...</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8661971830985911, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8543824058846871, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>micro avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8245370370370371, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>profession</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8682008368200831, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>religion</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.9795918367346931, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>samples avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8385710448562981, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stereotype</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.7173174872665531, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unrelated</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.9585492227979271, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8254717795636111, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 AUC_ROC_score  ...  threshold\n",
              "Anti-stereotype       0.952434  ...        0.5\n",
              "Ethnicity             0.952434  ...        0.5\n",
              "gender                0.952434  ...        0.5\n",
              "macro avg             0.952434  ...        0.5\n",
              "micro avg             0.952434  ...        0.5\n",
              "profession            0.952434  ...        0.5\n",
              "religion              0.952434  ...        0.5\n",
              "samples avg           0.952434  ...        0.5\n",
              "stereotype            0.952434  ...        0.5\n",
              "unrelated             0.952434  ...        0.5\n",
              "weighted avg          0.952434  ...        0.5\n",
              "\n",
              "[11 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AmCy17JWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}