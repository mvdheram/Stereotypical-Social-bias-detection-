{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language model and visualization notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x9wPoTMfekOS"
      ],
      "authorship_tag": "ABX9TyNMHofM/WlWFcCX4S9SFkZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Language_model_and_visualization_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2i-Sr614PTb"
      },
      "source": [
        "# BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "Link : https://arxiv.org/pdf/1810.04805.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bBbM9kk4dNL"
      },
      "source": [
        "Bert - **B**idirecitonal **E**ncoder **R**epresentations from **t**ransformers\n",
        "\n",
        "**Introduction**:\n",
        "\n",
        "  * "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3L3CzAu4eKL"
      },
      "source": [
        "# GPT-2\n",
        "\n",
        "Link : http://www.persagen.com/files/misc/radford2019language.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyiRQ5uL6EAL"
      },
      "source": [
        "# XL-Net\n",
        "\n",
        "Link : https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICnoEFo6BwM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtg-Q1jo6HVy"
      },
      "source": [
        "# RoBERTa \n",
        "\n",
        "Link : https://arxiv.org/abs/1907.11692"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wPoTMfekOS"
      },
      "source": [
        "# Visualization \n",
        "\n",
        "Link: https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568\n",
        "\n",
        "\n",
        "1. https://jalammar.github.io/explaining-transformers/\n",
        "2. https://jalammar.github.io/hidden-states/\n",
        "3. BerViz : https://github.com/jessevig/bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnkFSkge2TE"
      },
      "source": [
        "Bert-base:\n",
        "  * 12 encoder-layer stack for building contextualized embeddings.\n",
        "  * 100 million tuneable parameters.\n",
        "  * As bert model offers its embeddings to input, its useful to viusalize layers to analyze the patterns learned on unseen data.\n",
        "\n",
        "\n",
        "Why?\n",
        "  * After training viusalize, how well each layer seperates over epochs.\n",
        "\n",
        "How?\n",
        "  * BertForSequenceClassification consists of :\n",
        "    * 1 - BertEmbedding layer -> 12 - Bertlayer -> 1 - Bertpooler -> Tanh - activation -> Dropout layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AwkYpm6M5b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DO3x9Q56OZi"
      },
      "source": [
        ""
      ]
    }
  ]
}