{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language model and visualization notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x9wPoTMfekOS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Language_model_and_visualization_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2i-Sr614PTb"
      },
      "source": [
        "# BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "Link : https://arxiv.org/pdf/1810.04805.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bBbM9kk4dNL"
      },
      "source": [
        "Bert - **B**idirecitonal **E**ncoder **R**epresentations from **t**ransformers\n",
        "\n",
        "Architecture :\n",
        "  * Multi-layer, **bidirectional** , encoder - based transformer \n",
        "  * BERT-base - 12 Encoder stacks, 768 hidden size, 12 - Self attention head\n",
        "\n",
        "Framework :\n",
        "\n",
        "  * BERT-tokenizer :\n",
        "    * **WordPiece** embeddings \n",
        "    * Vocab size : 30,000\n",
        "    * First token `[CLS]` - The final hidden state of the 12th encoder stack\n",
        "    * Two sequences seperated by `[SEP]` \n",
        "    * End of sequence indicated by `[EOS]`\n",
        "\n",
        "    * Input : Input sentence\n",
        "    * Output : Token embedding + segment embedding + Position embedding \n",
        "    * \"BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vocabulary token.\" [RoBERTa paper]\n",
        "  \n",
        "  * Pre-training\n",
        "    * Task1 : Masked language model (MLM)\n",
        "      * For deep bidirectional representation, mask 15 % of all Wordpiece tokens in each sequence at random and predict the masked tokens.\n",
        "    * Task2 : Next sentence prediction (NSP)\n",
        "      * Trained to understand relationships between two sentences (Q&A, NLI)\n",
        "    * Data\n",
        "      * BookCorpus (800M words)\n",
        "      * English wikipedia (2500M words)\n",
        "  * Fine-tuning\n",
        "    * Plug-in task specific output layer and fine-tune all the parameters end to end.\n",
        "    * At output, the first token i.e `[CLS]` representation is fed into output layer for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3L3CzAu4eKL"
      },
      "source": [
        "# GPT-2\n",
        "\n",
        "Links :\n",
        "\n",
        "1. http://www.persagen.com/files/misc/radford2019language.pdf\n",
        "2. https://youtu.be/Ck9-0YkJD_Q?t=936\n",
        "3. https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf \n",
        "\n",
        "Model specification :\n",
        "  * 12- layer decoder- only transformer \n",
        "  * Masked self-attention heads (768 dimentional states and 12 attention heads)\n",
        "\n",
        "Basic Blocks of GPT architecture [2,3] :\n",
        "  1. GPT2 Tokenizer \n",
        "    * **Input** : Sentence \n",
        "    * **Output** : input_ids, attention_mask, labels \n",
        "    * **Vocab size** : 50k \n",
        "    * **Encoding** : Byte pair encoding (BPE) - \"Middle gorund between character level encoding and word level encoding\" [1] \n",
        "  2. GPT-2 embedding block [2] :\n",
        "      * Consists of \n",
        "        * Word embedding layer \n",
        "        * Position embedding layer\n",
        "      * **Input** : (input_ids, attention_mask, labels)- size (1,8) [*sentence, tokens*]\n",
        "      * **Output** : size (1,8,768) - [*sentence, tokens, embeddings per token*]\n",
        "  3. GPT-2 Decoder Block (x12):\n",
        "    * Consists of \n",
        "      * Attention block [2]:\n",
        "        * Consists of \n",
        "          * Self attention mechanism  ( generating Query, key, value pairs etc. of transformer) \n",
        "      * Multi layer perceptron block/ feedforward layer (MLP - Block)\n",
        "          * Consists of \n",
        "            * layer norm, convolution , activaiton function, dropout \n",
        "      * Layer Normalization \n",
        "  4. LM head layer \n",
        "    * Consists of Linear layer projected to vocab  \n",
        "\n",
        "Training data :\n",
        "  * WebText ( Web pages curated and filtered by humans - 45 million)\n",
        "    * Starting point, scraped outbound links from reddit (>3 karma)\n",
        "\n",
        "GPT2 for text classification :\n",
        "  * Remove the LM head and attach a classification layer with the output dimention equal to size of labels. \n",
        "  * Grab the output of last word embedding in the seqence because it has context information (L-R LM) until that word int he input sequence.\n",
        "    * transformer_output[0] (https://huggingface.co/transformers/_modules/transformers/models/gpt2/modeling_gpt2.html#GPT2ForSequenceClassification) \n",
        "    * Sequence classification architecture:\n",
        "      * Pooled output (last output as mentioned above)\n",
        "      * Dropout(0.1)\n",
        "      * Linear classifier layer with size (input_dim, num_class lables)\n",
        "      * Sigmoid Layer + BCE loss function \n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyiRQ5uL6EAL"
      },
      "source": [
        "# XL-Net\n",
        "\n",
        "Link : https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICnoEFo6BwM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtg-Q1jo6HVy"
      },
      "source": [
        "# RoBERTa \n",
        "\n",
        "Link : https://arxiv.org/abs/1907.11692\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JnFSdIBGDbT"
      },
      "source": [
        "Optimized version of BERT with the following modificatoins:\n",
        "  * Removing next sentence prediction (NSP) \n",
        "  * Training on more data and bigger batches\n",
        "  * Training on longer sequences \n",
        "  * Dynamically changing the masking pattern applied to training data.\n",
        "\n",
        "Data:\n",
        "  * Increase data size to imrpove end task performance \n",
        "  * BookCorpus + Wikipedia : Original data used by BERT \n",
        "  * CC- News : English portion of CommonCrawl News dataset ( \"63 million articles crawled between September 2016 and feb 2019 - 76 GB after filtering\")\n",
        "  * OPENWEBTEXT - \"The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB)\"\n",
        "  * \"STORIES, a dataset introduced in Trinh and Le\n",
        "(2018) containing a subset of CommonCrawl\n",
        "data filtered to match the story-like style of\n",
        "Winograd schemas. (31GB)\".\n",
        "\n",
        "Training procedure :\n",
        "  * Statis (BERT) vs Dynamic (RoBERTa) masking\n",
        "    * Dynamic masking is a strategy where masking patterns are generated for every sequence being fed into the model rather than relying on randomly masking and predicting.\n",
        "  * Model input format and Next sentence prediction \n",
        "    * Doument-sentences : Inputs to the model are packed with full sentences which do not cross document boundaries. Remove NSP loss \n",
        "  * Training with larger batch sizes \n",
        "    * 8k batch size increased when compared to BERT.\n",
        "  * Text encoding :\n",
        "    * Byte Pair encoding [BPE] which relies on subword units extracted from the training corpus compared to  BERT character level "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wPoTMfekOS"
      },
      "source": [
        "# Visualization \n",
        "\n",
        "Link: https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568\n",
        "\n",
        "\n",
        "1. https://jalammar.github.io/explaining-transformers/\n",
        "2. https://jalammar.github.io/hidden-states/\n",
        "3. BerViz : https://github.com/jessevig/bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnkFSkge2TE"
      },
      "source": [
        "Bert-base:\n",
        "  * 12 encoder-layer stack for building contextualized embeddings.\n",
        "  * 100 million tuneable parameters.\n",
        "  * As bert model offers its embeddings to input, its useful to viusalize layers to analyze the patterns learned on unseen data.\n",
        "\n",
        "\n",
        "Why?\n",
        "  * After training viusalize, how well each layer seperates over epochs.\n",
        "\n",
        "How?\n",
        "  * BertForSequenceClassification consists of :\n",
        "    * 1 - BertEmbedding layer -> 12 - Bertlayer -> 1 - Bertpooler -> Tanh - activation -> Dropout layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AwkYpm6M5b"
      },
      "source": [
        "## Compilation of results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQfIxc4HYEio"
      },
      "source": [
        "bert = open('/content/eval_results_BERT_0.5_.json','r')\n",
        "roberta = open('/content/eval_results_RoBERTa_0.5_.json','r')\n",
        "gpt2 = open('/content/eval_results_gpt-2_0.5_.json','r')\n",
        "xlnet = open('/content/eval_results_xlnet_0.5_.json','r')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUcTXUWaJpS"
      },
      "source": [
        "import json \n",
        "import pandas as pd\n",
        "\n",
        "bert_metrics = json.load(bert)\n",
        "roberta_metrics = json.load(roberta)\n",
        "gpt2_metrics = json.load(gpt2)\n",
        "xlnet_metrics = json.load(xlnet)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPzxCq942f-"
      },
      "source": [
        "### Micro_avg_scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5rHpvIaczXV"
      },
      "source": [
        "micro_avg_lms = pd.DataFrame([bert_metrics['Classification_report']['micro avg'],roberta_metrics['Classification_report']['micro avg'],gpt2_metrics['Classification_report']['micro avg'],xlnet_metrics['Classification_report']['micro avg']],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHb9yRfJlyjv"
      },
      "source": [
        "micro_avg_lms.columns = pd.MultiIndex.from_product([micro_avg_lms.columns, ['micro avg']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_bKtSHhnuYT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c58c658c-5a95-4627-d794-4172def0ee0e"
      },
      "source": [
        "micro_avg_lms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "      <th>micro avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.824537</td>\n",
              "      <td>0.822633</td>\n",
              "      <td>0.823584</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.864859</td>\n",
              "      <td>0.869053</td>\n",
              "      <td>0.866951</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.849421</td>\n",
              "      <td>0.457275</td>\n",
              "      <td>0.594505</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.880033</td>\n",
              "      <td>0.736952</td>\n",
              "      <td>0.802162</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  precision    recall  f1-score   support\n",
              "                  micro avg micro avg micro avg micro avg\n",
              "bert-base-uncased  0.824537  0.822633  0.823584      4330\n",
              "roberta-base       0.864859  0.869053  0.866951      4330\n",
              "gpt2               0.849421  0.457275  0.594505      4330\n",
              "xlnet-base-cased   0.880033  0.736952  0.802162      4330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEmRv45d48lH"
      },
      "source": [
        "### Macro_avg_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I7zBk303KI3"
      },
      "source": [
        "macro_avg_lms = pd.DataFrame([bert_metrics['Classification_report']['macro avg'],roberta_metrics['Classification_report']['macro avg'],gpt2_metrics['Classification_report']['macro avg'],xlnet_metrics['Classification_report']['macro avg']],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aciRTa7a3SZH"
      },
      "source": [
        "macro_avg_lms.columns = pd.MultiIndex.from_product([macro_avg_lms.columns, ['macro avg']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_c3sIuP3b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df967292-1943-477c-8fac-ba101e7842c8"
      },
      "source": [
        "macro_avg_lms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>macro avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.854382</td>\n",
              "      <td>0.842521</td>\n",
              "      <td>0.847510</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.880783</td>\n",
              "      <td>0.884021</td>\n",
              "      <td>0.882198</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.792463</td>\n",
              "      <td>0.496030</td>\n",
              "      <td>0.563569</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.885105</td>\n",
              "      <td>0.772770</td>\n",
              "      <td>0.815378</td>\n",
              "      <td>4330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  precision    recall  f1-score   support\n",
              "                  macro avg macro avg macro avg macro avg\n",
              "bert-base-uncased  0.854382  0.842521  0.847510      4330\n",
              "roberta-base       0.880783  0.884021  0.882198      4330\n",
              "gpt2               0.792463  0.496030  0.563569      4330\n",
              "xlnet-base-cased   0.885105  0.772770  0.815378      4330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj7xFNtXApdx"
      },
      "source": [
        "### Hamming_loss, subset_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-utZSCDs_pZe"
      },
      "source": [
        "bert = bert_metrics['hamming_loss'],bert_metrics['subset_accuracy'],bert_metrics['AUC_ROC_score']\n",
        "roberta = roberta_metrics['hamming_loss'],roberta_metrics['subset_accuracy'],roberta_metrics['AUC_ROC_score']\n",
        "gpt2 = gpt2_metrics['hamming_loss'],gpt2_metrics['subset_accuracy'],gpt2_metrics['AUC_ROC_score']\n",
        "xlnet = xlnet_metrics['hamming_loss'],xlnet_metrics['subset_accuracy'],xlnet_metrics['AUC_ROC_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H_fewWDJZ8U"
      },
      "source": [
        "hs_metrics = pd.DataFrame([bert,roberta,gpt2,xlnet],index=['bert-base-uncased','roberta-base','gpt2','xlnet-base-cased'],columns=['hamming_loss','subset_accuracy','AUC_ROC_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjK5Ya5bJrxE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "740e397a-d40d-4e2d-b330-c170aa5f72a6"
      },
      "source": [
        "hs_metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hamming_loss</th>\n",
              "      <th>subset_accuracy</th>\n",
              "      <th>AUC_ROC_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bert-base-uncased</th>\n",
              "      <td>0.087832</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.952434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base</th>\n",
              "      <td>0.070911</td>\n",
              "      <td>0.771555</td>\n",
              "      <td>0.966717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2</th>\n",
              "      <td>0.155462</td>\n",
              "      <td>0.293312</td>\n",
              "      <td>0.891429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlnet-base-cased</th>\n",
              "      <td>0.090595</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.954108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   hamming_loss  subset_accuracy  AUC_ROC_score\n",
              "bert-base-uncased      0.087832         0.666398       0.952434\n",
              "roberta-base           0.070911         0.771555       0.966717\n",
              "gpt2                   0.155462         0.293312       0.891429\n",
              "xlnet-base-cased       0.090595         0.588235       0.954108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIJD9PYLCzO"
      },
      "source": [
        "### per_class_precision_recall_fmeasure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19FUoVteTqV4"
      },
      "source": [
        "Labels = ['Ethnicity','gender','profession','religion','Anti-stereotype','stereotype','unrelated']\n",
        "# model = ['bert_base_uncased','roberta-base','gpt2','xlnet-base-cased']\n",
        "metrics = [bert_metrics,roberta_metrics,gpt2_metrics,xlnet_metrics]\n",
        "model_name = ['bert','roberta','gpt2','xlnet']\n",
        "model = {}"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuGPzJvTrl8j"
      },
      "source": [
        "model.clear()"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0PUVngs56B",
        "outputId": "1e52bd81-5a5c-4964-e073-41254ec2d8c5"
      },
      "source": [
        "for index,metric in enumerate(metrics):\n",
        "  print(metric)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'AUC_ROC_score': 0.9515636613746575, 'subset_accuracy': 0.6700241740531829, 'hamming_loss': 0.09048002762748936, 'hammingsScore/Accuracy': 0.7811912436207331, 'Classification_report': {'Ethnicity': {'precision': 0.9272503082614056, 'recall': 0.9591836734693877, 'f1-score': 0.9429467084639498, 'support': 784}, 'gender': {'precision': 0.8705035971223022, 'recall': 0.7960526315789473, 'f1-score': 0.8316151202749142, 'support': 304}, 'profession': {'precision': 0.8645833333333334, 'recall': 0.8886509635974305, 'f1-score': 0.8764519535374868, 'support': 467}, 'religion': {'precision': 0.9761904761904762, 'recall': 0.9795221843003413, 'f1-score': 0.9778534923339012, 'support': 293}, 'Anti-stereotype': {'precision': 0.6132075471698113, 'recall': 0.6683804627249358, 'f1-score': 0.6396063960639606, 'support': 778}, 'stereotype': {'precision': 0.7450787401574803, 'recall': 0.7074766355140187, 'f1-score': 0.725790987535954, 'support': 1070}, 'unrelated': {'precision': 0.9575551782682513, 'recall': 0.889589905362776, 'f1-score': 0.9223221586263287, 'support': 634}, 'micro avg': {'precision': 0.8195088044485634, 'recall': 0.8168591224018475, 'f1-score': 0.8181818181818181, 'support': 4330}, 'macro avg': {'precision': 0.8506241686432944, 'recall': 0.8412652080782624, 'f1-score': 0.8452266881194994, 'support': 4330}, 'weighted avg': {'precision': 0.8228132258848238, 'recall': 0.8168591224018475, 'f1-score': 0.8191357350867148, 'support': 4330}, 'samples avg': {'precision': 0.8337362342197152, 'recall': 0.8261482675261885, 'f1-score': 0.8245500940102068, 'support': 4330}}, 'confusion_matrix_Ethnicity': '[1639   59   32  752]', 'confusion_matrix_gender': '[2142   36   62  242]', 'confusion_matrix_profession': '[1950   65   52  415]', 'confusion_matrix_religion': '[2182    7    6  287]', 'confusion_matrix_Anti-stereotype': '[1376  328  258  520]', 'confusion_matrix_stereotype': '[1153  259  313  757]', 'confusion_matrix_unrelated': '[1823   25   70  564]', 'y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [1 0 0 ... 1 0 0]\\n [1 0 0 ... 0 1 0]\\n [1 0 0 ... 1 0 0]]', 'y_labels': '[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 1. 0.]\\n ...\\n [1. 0. 0. ... 0. 1. 0.]\\n [1. 0. 0. ... 0. 1. 0.]\\n [1. 0. 0. ... 1. 0. 0.]]', 'threshold': 0.5}\n",
            "{'AUC_ROC_score': 0.9667167090411658, 'subset_accuracy': 0.7715551974214343, 'hamming_loss': 0.07091055600322321, 'hammingsScore/Accuracy': 0.8364558152027899, 'Classification_report': {'Ethnicity': {'precision': 0.9440298507462687, 'recall': 0.9681122448979592, 'f1-score': 0.9559193954659949, 'support': 784}, 'gender': {'precision': 0.8333333333333334, 'recall': 0.8881578947368421, 'f1-score': 0.8598726114649682, 'support': 304}, 'profession': {'precision': 0.8961864406779662, 'recall': 0.9057815845824411, 'f1-score': 0.9009584664536742, 'support': 467}, 'religion': {'precision': 0.9830508474576272, 'recall': 0.9897610921501706, 'f1-score': 0.9863945578231292, 'support': 293}, 'Anti-stereotype': {'precision': 0.7267525035765379, 'recall': 0.6529562982005142, 'f1-score': 0.6878808395396072, 'support': 778}, 'stereotype': {'precision': 0.7732758620689655, 'recall': 0.8383177570093457, 'f1-score': 0.8044843049327354, 'support': 1070}, 'unrelated': {'precision': 0.965, 'recall': 0.9132492113564669, 'f1-score': 0.9384116693679092, 'support': 634}, 'micro avg': {'precision': 0.8557648139641709, 'recall': 0.8605080831408776, 'f1-score': 0.8581298940580377, 'support': 4330}, 'macro avg': {'precision': 0.8745184054086712, 'recall': 0.8794765832762484, 'f1-score': 0.876274549292574, 'support': 4330}, 'weighted avg': {'precision': 0.8555737465626887, 'recall': 0.8605080831408776, 'f1-score': 0.8571653087819466, 'support': 4330}, 'samples avg': {'precision': 0.8684528605962933, 'recall': 0.8672441579371475, 'f1-score': 0.8668143969916734, 'support': 4330}}, 'confusion_matrix_Ethnicity': '[1653   45   25  759]', 'confusion_matrix_gender': '[2124   54   34  270]', 'confusion_matrix_profession': '[1966   49   44  423]', 'confusion_matrix_religion': '[2184    5    3  290]', 'confusion_matrix_Anti-stereotype': '[1513  191  270  508]', 'confusion_matrix_stereotype': '[1149  263  173  897]', 'confusion_matrix_unrelated': '[1827   21   55  579]', 'y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [1 0 0 ... 1 0 0]\\n [1 0 0 ... 0 1 0]\\n [1 0 0 ... 1 0 0]]', 'y_labels': '[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 1. 0.]\\n ...\\n [1. 0. 0. ... 0. 1. 0.]\\n [1. 0. 0. ... 0. 1. 0.]\\n [1. 0. 0. ... 1. 0. 0.]]', 'threshold': 0.5}\n",
            "{'AUC_ROC_score': 0.8914286769035048, 'subset_accuracy': 0.29331184528605964, 'hamming_loss': 0.15546218487394958, 'Classification_report': {'Ethnicity': {'precision': 0.8746397694524496, 'recall': 0.7742346938775511, 'f1-score': 0.8213802435723951, 'support': 784}, 'gender': {'precision': 0.8524590163934426, 'recall': 0.17105263157894737, 'f1-score': 0.28493150684931506, 'support': 304}, 'profession': {'precision': 0.7324675324675325, 'recall': 0.6038543897216274, 'f1-score': 0.6619718309859154, 'support': 467}, 'religion': {'precision': 0.9249146757679181, 'recall': 0.9249146757679181, 'f1-score': 0.9249146757679181, 'support': 293}, 'Anti-stereotype': {'precision': 0.40476190476190477, 'recall': 0.021850899742930592, 'f1-score': 0.041463414634146344, 'support': 778}, 'stereotype': {'precision': 0.8901098901098901, 'recall': 0.30280373831775703, 'f1-score': 0.45188284518828453, 'support': 1070}, 'unrelated': {'precision': 0.8678861788617886, 'recall': 0.6735015772870663, 'f1-score': 0.7584369449378331, 'support': 634}, 'micro avg': {'precision': 0.8494208494208494, 'recall': 0.45727482678983833, 'f1-score': 0.5945053295301005, 'support': 4330}, 'macro avg': {'precision': 0.7924627096878466, 'recall': 0.4960303723276853, 'f1-score': 0.563568780276544, 'support': 4330}, 'weighted avg': {'precision': 0.7795588082257234, 'recall': 0.45727482678983833, 'f1-score': 0.5328739810948704, 'support': 4330}, 'samples avg': {'precision': 0.6631748589846898, 'recall': 0.48489121676067687, 'f1-score': 0.5437550362610797, 'support': 4330}}, 'confusion_matrix_Ethnicity': '[1611   87  177  607]', 'confusion_matrix_gender': '[2169    9  252   52]', 'confusion_matrix_profession': '[1912  103  185  282]', 'confusion_matrix_religion': '[2167   22   22  271]', 'confusion_matrix_Anti-stereotype': '[1679   25  761   17]', 'confusion_matrix_stereotype': '[1372   40  746  324]', 'confusion_matrix_unrelated': '[1783   65  207  427]', 'y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 0 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [0 0 0 ... 0 0 0]\\n [1 0 0 ... 0 1 0]\\n [0 0 0 ... 0 0 0]]', 'y_labels': '      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\\n0           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n1           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2           0.0     0.0         0.0  ...              0.0         1.0        0.0\\n3           0.0     1.0         0.0  ...              0.0         1.0        0.0\\n4           1.0     0.0         0.0  ...              0.0         1.0        0.0\\n...         ...     ...         ...  ...              ...         ...        ...\\n2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\\n2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\\n\\n[2482 rows x 7 columns]', 'threshold': 0.5}\n",
            "{'AUC_ROC_score': 0.9541083160855942, 'subset_accuracy': 0.5882352941176471, 'hamming_loss': 0.09059514216645562, 'Classification_report': {'Ethnicity': {'precision': 0.9108433734939759, 'recall': 0.9642857142857143, 'f1-score': 0.9368029739776952, 'support': 784}, 'gender': {'precision': 0.850909090909091, 'recall': 0.7697368421052632, 'f1-score': 0.8082901554404146, 'support': 304}, 'profession': {'precision': 0.8705357142857143, 'recall': 0.8351177730192719, 'f1-score': 0.8524590163934426, 'support': 467}, 'religion': {'precision': 0.9862068965517241, 'recall': 0.9761092150170648, 'f1-score': 0.9811320754716981, 'support': 293}, 'Anti-stereotype': {'precision': 0.7899159663865546, 'recall': 0.36246786632390743, 'f1-score': 0.4969162995594713, 'support': 778}, 'stereotype': {'precision': 0.8040540540540541, 'recall': 0.6672897196261682, 'f1-score': 0.7293156281920328, 'support': 1070}, 'unrelated': {'precision': 0.983271375464684, 'recall': 0.8343848580441641, 'f1-score': 0.902730375426621, 'support': 634}, 'micro avg': {'precision': 0.8800330943188086, 'recall': 0.7369515011547344, 'f1-score': 0.8021618903971846, 'support': 4330}, 'macro avg': {'precision': 0.8851052101636855, 'recall': 0.772770284060222, 'f1-score': 0.8153780749230537, 'support': 4330}, 'weighted avg': {'precision': 0.8698759536831204, 'recall': 0.7369515011547344, 'f1-score': 0.7863841706384679, 'support': 4330}, 'samples avg': {'precision': 0.8642224012892828, 'recall': 0.7493956486704271, 'f1-score': 0.7875369325812516, 'support': 4330}}, 'confusion_matrix_Ethnicity': '[1624   74   28  756]', 'confusion_matrix_gender': '[2137   41   70  234]', 'confusion_matrix_profession': '[1957   58   77  390]', 'confusion_matrix_religion': '[2185    4    7  286]', 'confusion_matrix_Anti-stereotype': '[1629   75  496  282]', 'confusion_matrix_stereotype': '[1238  174  356  714]', 'confusion_matrix_unrelated': '[1839    9  105  529]', 'y_pred': '[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 0 ... 0 1 0]\\n ...\\n [1 0 0 ... 1 0 0]\\n [1 0 0 ... 0 1 0]\\n [1 0 0 ... 1 0 0]]', 'y_labels': '      Ethnicity  gender  profession  ...  Anti-stereotype  stereotype  unrelated\\n0           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n1           0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2           0.0     0.0         0.0  ...              0.0         1.0        0.0\\n3           0.0     1.0         0.0  ...              0.0         1.0        0.0\\n4           1.0     0.0         0.0  ...              0.0         1.0        0.0\\n...         ...     ...         ...  ...              ...         ...        ...\\n2477        0.0     0.0         0.0  ...              0.0         0.0        1.0\\n2478        0.0     1.0         0.0  ...              1.0         0.0        0.0\\n2479        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2480        1.0     0.0         0.0  ...              0.0         1.0        0.0\\n2481        1.0     0.0         0.0  ...              1.0         0.0        0.0\\n\\n[2482 rows x 7 columns]', 'threshold': 0.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjgisvfCLPa2"
      },
      "source": [
        "for index,metric in enumerate(metrics):\n",
        "  for label in Labels:\n",
        "    model[model_name[index] + \"_\"+ label] = metric['Classification_report'][label]"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9_mPyTJ4w3c"
      },
      "source": [
        "df = pd.DataFrame(model)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb4m2qCCEDLZ",
        "outputId": "a66e8ac7-32fd-429e-e7bd-4977c5058cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "df.iloc[:,14:21]"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gpt2_Ethnicity</th>\n",
              "      <th>gpt2_gender</th>\n",
              "      <th>gpt2_profession</th>\n",
              "      <th>gpt2_religion</th>\n",
              "      <th>gpt2_Anti-stereotype</th>\n",
              "      <th>gpt2_stereotype</th>\n",
              "      <th>gpt2_unrelated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.874640</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.732468</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.867886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.774235</td>\n",
              "      <td>0.171053</td>\n",
              "      <td>0.603854</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.021851</td>\n",
              "      <td>0.302804</td>\n",
              "      <td>0.673502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.821380</td>\n",
              "      <td>0.284932</td>\n",
              "      <td>0.661972</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.041463</td>\n",
              "      <td>0.451883</td>\n",
              "      <td>0.758437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>784.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>293.000000</td>\n",
              "      <td>778.000000</td>\n",
              "      <td>1070.000000</td>\n",
              "      <td>634.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           gpt2_Ethnicity  gpt2_gender  ...  gpt2_stereotype  gpt2_unrelated\n",
              "precision        0.874640     0.852459  ...         0.890110        0.867886\n",
              "recall           0.774235     0.171053  ...         0.302804        0.673502\n",
              "f1-score         0.821380     0.284932  ...         0.451883        0.758437\n",
              "support        784.000000   304.000000  ...      1070.000000      634.000000\n",
              "\n",
              "[4 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0OXBkzb5IgP"
      },
      "source": [
        "df_bert  = df.iloc[:,:7]\n",
        "df_bert_metrics = df_bert.copy()\n",
        "df_bert_metrics['Model_name'] = 'bert_base_uncased'\n",
        "df_bert_metrics =df_bert_metrics.set_index(['Model_name',df_bert.index])\n",
        "df_bert_metrics.columns = Labels"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOhV8z3j9Tot"
      },
      "source": [
        "df_roberta = df.iloc[:,7:14]\n",
        "df_roberta_metrics = df_roberta.copy()\n",
        "df_roberta_metrics['Model_name'] = 'roberta-base'\n",
        "df_roberta_metrics = df_roberta_metrics.set_index(['Model_name',df_bert.index])\n",
        "df_roberta_metrics.columns = Labels"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847pgQ97E4UA"
      },
      "source": [
        "df_GPT2 = df.iloc[:,14:21]\n",
        "df_GPT2_metrics = df_GPT2.copy()\n",
        "df_GPT2_metrics['Model_name'] = 'gpt2'\n",
        "df_GPT2_metrics = df_GPT2_metrics.set_index(['Model_name',df_bert.index])\n",
        "df_GPT2_metrics.columns = Labels"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGDUAtECJQg"
      },
      "source": [
        "df_XLNet = df.iloc[:,21:]\n",
        "df_XLNet_metrics = df_XLNet.copy()\n",
        "df_XLNet_metrics['Model_name'] = 'xlnet-base-cased'\n",
        "df_XLNet_metrics = df_XLNet_metrics.set_index(['Model_name',df_bert.index])\n",
        "df_XLNet_metrics.columns = Labels"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K75tImcOFJ43"
      },
      "source": [
        "df_per_label = pd.concat([df_bert_metrics,df_roberta_metrics,df_XLNet_metrics,df_GPT2_metrics])"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQjalw7pGEGI",
        "outputId": "f84e91b9-5195-408f-984a-666b1242f169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        }
      },
      "source": [
        "df_per_label"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>gender</th>\n",
              "      <th>profession</th>\n",
              "      <th>religion</th>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>unrelated</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">bert_base_uncased</th>\n",
              "      <th>precision</th>\n",
              "      <td>0.927250</td>\n",
              "      <td>0.870504</td>\n",
              "      <td>0.864583</td>\n",
              "      <td>0.976190</td>\n",
              "      <td>0.613208</td>\n",
              "      <td>0.745079</td>\n",
              "      <td>0.957555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.959184</td>\n",
              "      <td>0.796053</td>\n",
              "      <td>0.888651</td>\n",
              "      <td>0.979522</td>\n",
              "      <td>0.668380</td>\n",
              "      <td>0.707477</td>\n",
              "      <td>0.889590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.942947</td>\n",
              "      <td>0.831615</td>\n",
              "      <td>0.876452</td>\n",
              "      <td>0.977853</td>\n",
              "      <td>0.639606</td>\n",
              "      <td>0.725791</td>\n",
              "      <td>0.922322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>784.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>293.000000</td>\n",
              "      <td>778.000000</td>\n",
              "      <td>1070.000000</td>\n",
              "      <td>634.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">roberta-base</th>\n",
              "      <th>precision</th>\n",
              "      <td>0.944030</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.896186</td>\n",
              "      <td>0.983051</td>\n",
              "      <td>0.726753</td>\n",
              "      <td>0.773276</td>\n",
              "      <td>0.965000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.968112</td>\n",
              "      <td>0.888158</td>\n",
              "      <td>0.905782</td>\n",
              "      <td>0.989761</td>\n",
              "      <td>0.652956</td>\n",
              "      <td>0.838318</td>\n",
              "      <td>0.913249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.955919</td>\n",
              "      <td>0.859873</td>\n",
              "      <td>0.900958</td>\n",
              "      <td>0.986395</td>\n",
              "      <td>0.687881</td>\n",
              "      <td>0.804484</td>\n",
              "      <td>0.938412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>784.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>293.000000</td>\n",
              "      <td>778.000000</td>\n",
              "      <td>1070.000000</td>\n",
              "      <td>634.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">xlnet-base-cased</th>\n",
              "      <th>precision</th>\n",
              "      <td>0.910843</td>\n",
              "      <td>0.850909</td>\n",
              "      <td>0.870536</td>\n",
              "      <td>0.986207</td>\n",
              "      <td>0.789916</td>\n",
              "      <td>0.804054</td>\n",
              "      <td>0.983271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.769737</td>\n",
              "      <td>0.835118</td>\n",
              "      <td>0.976109</td>\n",
              "      <td>0.362468</td>\n",
              "      <td>0.667290</td>\n",
              "      <td>0.834385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.936803</td>\n",
              "      <td>0.808290</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.981132</td>\n",
              "      <td>0.496916</td>\n",
              "      <td>0.729316</td>\n",
              "      <td>0.902730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>784.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>293.000000</td>\n",
              "      <td>778.000000</td>\n",
              "      <td>1070.000000</td>\n",
              "      <td>634.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt2</th>\n",
              "      <th>precision</th>\n",
              "      <td>0.874640</td>\n",
              "      <td>0.852459</td>\n",
              "      <td>0.732468</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.890110</td>\n",
              "      <td>0.867886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.774235</td>\n",
              "      <td>0.171053</td>\n",
              "      <td>0.603854</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.021851</td>\n",
              "      <td>0.302804</td>\n",
              "      <td>0.673502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.821380</td>\n",
              "      <td>0.284932</td>\n",
              "      <td>0.661972</td>\n",
              "      <td>0.924915</td>\n",
              "      <td>0.041463</td>\n",
              "      <td>0.451883</td>\n",
              "      <td>0.758437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>784.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>293.000000</td>\n",
              "      <td>778.000000</td>\n",
              "      <td>1070.000000</td>\n",
              "      <td>634.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Ethnicity      gender  ...   stereotype   unrelated\n",
              "Model_name                                           ...                         \n",
              "bert_base_uncased precision    0.927250    0.870504  ...     0.745079    0.957555\n",
              "                  recall       0.959184    0.796053  ...     0.707477    0.889590\n",
              "                  f1-score     0.942947    0.831615  ...     0.725791    0.922322\n",
              "                  support    784.000000  304.000000  ...  1070.000000  634.000000\n",
              "roberta-base      precision    0.944030    0.833333  ...     0.773276    0.965000\n",
              "                  recall       0.968112    0.888158  ...     0.838318    0.913249\n",
              "                  f1-score     0.955919    0.859873  ...     0.804484    0.938412\n",
              "                  support    784.000000  304.000000  ...  1070.000000  634.000000\n",
              "xlnet-base-cased  precision    0.910843    0.850909  ...     0.804054    0.983271\n",
              "                  recall       0.964286    0.769737  ...     0.667290    0.834385\n",
              "                  f1-score     0.936803    0.808290  ...     0.729316    0.902730\n",
              "                  support    784.000000  304.000000  ...  1070.000000  634.000000\n",
              "gpt2              precision    0.874640    0.852459  ...     0.890110    0.867886\n",
              "                  recall       0.774235    0.171053  ...     0.302804    0.673502\n",
              "                  f1-score     0.821380    0.284932  ...     0.451883    0.758437\n",
              "                  support    784.000000  304.000000  ...  1070.000000  634.000000\n",
              "\n",
              "[16 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 302
        }
      ]
    }
  ]
}