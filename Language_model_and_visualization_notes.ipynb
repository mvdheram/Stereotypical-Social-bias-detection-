{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language model and visualization notes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x9wPoTMfekOS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Language_model_and_visualization_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2i-Sr614PTb"
      },
      "source": [
        "# BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "Link : https://arxiv.org/pdf/1810.04805.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bBbM9kk4dNL"
      },
      "source": [
        "Bert - **B**idirecitonal **E**ncoder **R**epresentations from **t**ransformers\n",
        "\n",
        "Architecture :\n",
        "  * Multi-layer, **bidirectional** , encoder - based transformer \n",
        "  * BERT-base - 12 Encoder stacks, 768 hidden size, 12 - Self attention head\n",
        "\n",
        "Framework :\n",
        "\n",
        "  * BERT-tokenizer :\n",
        "    * **WordPiece** embeddings \n",
        "    * Vocab size : 30,000\n",
        "    * First token `[CLS]` - The final hidden state of the 12th encoder stack\n",
        "    * Two sequences seperated by `[SEP]` \n",
        "\n",
        "    * Input : Input sentence\n",
        "    * Output : Token embedding + segment embedding + Position embedding \n",
        "  \n",
        "  * Pre-training\n",
        "    * Task1 : Masked language model (MLM)\n",
        "      * For deep bidirectional representation, mask 15 % of all Wordpiece tokens in each sequence at random and predict the masked tokens.\n",
        "    * Task2 : Next sentence prediction (NSP)\n",
        "      * Trained to understand relationships between two sentences (Q&A, NLI)\n",
        "    * Data\n",
        "      * BookCorpus (800M words)\n",
        "      * English wikipedia (2500M words)\n",
        "  * Fine-tuning\n",
        "    * Plug-in task specific output layer and fine-tune all the parameters end to end.\n",
        "    * At output, the first token i.e `[CLS]` representation is fed into output layer for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3L3CzAu4eKL"
      },
      "source": [
        "# GPT-2\n",
        "\n",
        "Links :\n",
        "\n",
        "1. http://www.persagen.com/files/misc/radford2019language.pdf\n",
        "2. https://youtu.be/Ck9-0YkJD_Q?t=936\n",
        "3. https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf \n",
        "\n",
        "Model specification :\n",
        "  * 12- layer decoder- only transformer \n",
        "  * Masked self-attention heads (768 dimentional states and 12 attention heads)\n",
        "\n",
        "Basic Blocks of GPT architecture [2,3] :\n",
        "  1. GPT2 Tokenizer \n",
        "    * **Input** : Sentence \n",
        "    * **Output** : input_ids, attention_mask, labels \n",
        "    * **Vocab size** : 50k \n",
        "    * **Encoding** : Byte pair encoding (BPE) - \"Middle gorund between character level encoding and word level encoding\" [1] \n",
        "  2. GPT-2 embedding block [2] :\n",
        "      * Consists of \n",
        "        * Word embedding layer \n",
        "        * Position embedding layer\n",
        "      * **Input** : (input_ids, attention_mask, labels)- size (1,8) [*sentence, tokens*]\n",
        "      * **Output** : size (1,8,768) - [*sentence, tokens, embeddings per token*]\n",
        "  3. GPT-2 Decoder Block (x12):\n",
        "    * Consists of \n",
        "      * Attention block [2]:\n",
        "        * Consists of \n",
        "          * Self attention mechanism  ( generating Query, key, value pairs etc. of transformer) \n",
        "      * Multi layer perceptron block/ feedforward layer (MLP - Block)\n",
        "          * Consists of \n",
        "            * layer norm, convolution , activaiton function, dropout \n",
        "      * Layer Normalization \n",
        "  4. LM head layer \n",
        "    * Consists of Linear layer projected to vocab  \n",
        "\n",
        "Training data :\n",
        "  * WebText ( Web pages curated and filtered by humans - 45 million)\n",
        "    * Starting point, scraped outbound links from reddit (>3 karma)\n",
        "\n",
        "GPT2 for text classification :\n",
        "  * Remove the LM head and attach a classification layer with the output dimention equal to size of labels. \n",
        "  * Grab the output of last word embedding in the seqence because it has context information (L-R LM) until that word int he input sequence.\n",
        "    * transformer_output[0] (https://huggingface.co/transformers/_modules/transformers/models/gpt2/modeling_gpt2.html#GPT2ForSequenceClassification) \n",
        "    * Sequence classification architecture:\n",
        "      * Pooled output (last output as mentioned above)\n",
        "      * Dropout(0.1)\n",
        "      * Linear classifier layer with size (input_dim, num_class lables)\n",
        "      * Sigmoid Layer + BCE loss function \n",
        "\n",
        "\n",
        "   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyiRQ5uL6EAL"
      },
      "source": [
        "# XL-Net\n",
        "\n",
        "Link : https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LICnoEFo6BwM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtg-Q1jo6HVy"
      },
      "source": [
        "# RoBERTa \n",
        "\n",
        "Link : https://arxiv.org/abs/1907.11692\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wPoTMfekOS"
      },
      "source": [
        "# Visualization \n",
        "\n",
        "Link: https://towardsdatascience.com/visualize-bert-sequence-embeddings-an-unseen-way-1d6a351e4568\n",
        "\n",
        "\n",
        "1. https://jalammar.github.io/explaining-transformers/\n",
        "2. https://jalammar.github.io/hidden-states/\n",
        "3. BerViz : https://github.com/jessevig/bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnkFSkge2TE"
      },
      "source": [
        "Bert-base:\n",
        "  * 12 encoder-layer stack for building contextualized embeddings.\n",
        "  * 100 million tuneable parameters.\n",
        "  * As bert model offers its embeddings to input, its useful to viusalize layers to analyze the patterns learned on unseen data.\n",
        "\n",
        "\n",
        "Why?\n",
        "  * After training viusalize, how well each layer seperates over epochs.\n",
        "\n",
        "How?\n",
        "  * BertForSequenceClassification consists of :\n",
        "    * 1 - BertEmbedding layer -> 12 - Bertlayer -> 1 - Bertpooler -> Tanh - activation -> Dropout layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4AwkYpm6M5b"
      },
      "source": [
        "## Compilation of results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7CD0MzgzR3x"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "bert = pd.read_json('/content/eval_results_BERT_0.5_.json')\n",
        "roberta = pd.read_json('/content/eval_results_RoBERTa_0.5_.json')\n",
        "gpt2 = pd.read_json('/content/eval_results_gpt-2_0.5_.json')\n",
        "xlnet = pd.read_json('/content/eval_results_xlnet_0.5_.json')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "50dcfCeF2Be6",
        "outputId": "678f5e8f-4011-457f-a58a-58001a968440"
      },
      "source": [
        "bert"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AUC_ROC_score</th>\n",
              "      <th>subset_accuracy</th>\n",
              "      <th>hamming_loss</th>\n",
              "      <th>Classification_report</th>\n",
              "      <th>confusion_matrix_Ethnicity</th>\n",
              "      <th>confusion_matrix_gender</th>\n",
              "      <th>confusion_matrix_profession</th>\n",
              "      <th>confusion_matrix_religion</th>\n",
              "      <th>confusion_matrix_Anti-stereotype</th>\n",
              "      <th>confusion_matrix_stereotype</th>\n",
              "      <th>confusion_matrix_unrelated</th>\n",
              "      <th>y_pred</th>\n",
              "      <th>y_labels</th>\n",
              "      <th>threshold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Anti-stereotype</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.6590584878744651, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ethnicity</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.931761786600496, 'recall': 0.9...</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8661971830985911, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8543824058846871, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>micro avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8245370370370371, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>profession</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8682008368200831, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>religion</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.9795918367346931, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>samples avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8385710448562981, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stereotype</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.7173174872665531, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unrelated</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.9585492227979271, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>0.952434</td>\n",
              "      <td>0.666398</td>\n",
              "      <td>0.087832</td>\n",
              "      <td>{'precision': 0.8254717795636111, 'recall': 0....</td>\n",
              "      <td>[1643   55   33  751]</td>\n",
              "      <td>[2140   38   58  246]</td>\n",
              "      <td>[1952   63   52  415]</td>\n",
              "      <td>[2183    6    5  288]</td>\n",
              "      <td>[1465  239  316  462]</td>\n",
              "      <td>[1079  333  225  845]</td>\n",
              "      <td>[1824   24   79  555]</td>\n",
              "      <td>[[0 0 0 ... 0 0 1]\\n [1 0 0 ... 1 0 0]\\n [0 0 ...</td>\n",
              "      <td>[[0. 0. 0. ... 0. 0. 1.]\\n [0. 0. 0. ... 0. 0....</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 AUC_ROC_score  ...  threshold\n",
              "Anti-stereotype       0.952434  ...        0.5\n",
              "Ethnicity             0.952434  ...        0.5\n",
              "gender                0.952434  ...        0.5\n",
              "macro avg             0.952434  ...        0.5\n",
              "micro avg             0.952434  ...        0.5\n",
              "profession            0.952434  ...        0.5\n",
              "religion              0.952434  ...        0.5\n",
              "samples avg           0.952434  ...        0.5\n",
              "stereotype            0.952434  ...        0.5\n",
              "unrelated             0.952434  ...        0.5\n",
              "weighted avg          0.952434  ...        0.5\n",
              "\n",
              "[11 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AmCy17JWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}