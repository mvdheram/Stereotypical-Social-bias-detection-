{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyper-parameter search and class imbalance handling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvRAHuIPL5OXb7MPj4bbN8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvdheram/Stereotypical-Social-bias-detection-/blob/Pre-trained-LM-selection-and-training/Hyper_parameter_search_and_class_imbalance_handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-tvw1HUPWRY"
      },
      "source": [
        "# Hyper-parameter search Research\n",
        "\n",
        "Hyper-parameter : https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
        "\n",
        "Transformer hyper-parameter search: https://huggingface.co/blog/ray-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzX36IHDPdnQ"
      },
      "source": [
        "**What is hyper-parameter**?\n",
        "  * Parameters that are used to control the learning process of a model\n",
        "  * \"Model configuration parameters set by the developer to guide learning process for specific dataset\".\n",
        "\n",
        "**Difference between model parameters and model hyper-parameters**?\n",
        "  * Model parameters: \n",
        "    * Variables whose values are not set but learned during the training of a model for specific data.\n",
        "      * E.g. \n",
        "        * Weights (importance given to each feature of an instance) and biases (adjust the generalization of the model) in NN\n",
        "        * Support vectors in SVM\n",
        "        * Coefficients in regression models \n",
        "  * Model Hyper-perameter:\n",
        "    * Configuration variable set before training to improve the training process or reduce the loss function.\n",
        "    * E.g.\n",
        "      * Learning rate for NN\n",
        "      * K in KNN\n",
        "\n",
        "**Hyper-parameter search/tuning/optimization:**\n",
        "  * No rule of thumb to set hyper parameters and it is required to search for best hyper-parameters of a model on a dataset.\n",
        "  * Hyper-parameter for a model is searched in search space where each dimention represents hyper-parameter and point represent one model configuration.\n",
        "  * Goal of hyper-parameter search is to find an optimal configuration parameters (vector) from search space.\n",
        "  * Different algorithms\n",
        "    * Random search: randomly sample points from bounded domain of search space\n",
        "      * More time to search \n",
        "      *`RandomizedSearchCV(model,space)` from sklearn, space is a dictionary of parameters to be searched\n",
        "    * Grid search:  Search space as grid of hyper-parameters and evaluate every\n",
        " point in the grid.\n",
        "      * More defined search in the search space\n",
        "      * `GridSearchCV(model,space)` from sklearn, space is a dictionary of parameters to be searched.\n",
        "    * Advanced:\n",
        "      * Bayesian optimization \n",
        "      * Population based training\n",
        "\n",
        "\n",
        "**Transformers Hyper-parameter tuning :**\n",
        "\n",
        "Library : RayTune (python library for experiment execution and hyperparameter tuning)\n",
        "\n",
        "Steps:\n",
        "  1. Define search space\n",
        "      * BERT Model fine-tune Hyper-parameters(baseline : https://www.aclweb.org/anthology/N19-1423/):\n",
        "        * Batch_size : [16,32]\n",
        "        * Learning rate (adam) : 5e-5,3e-5,2e-5\n",
        "        * Number of epochs : 2,3,4\n",
        "      * RoBERTa Model fine-tune hyper-parameters in paper(baseline : https://arxiv.org/abs/1907.11692):\n",
        "        * Batch_size : [16,32]\n",
        "        * Learning rate (adam) : 1e-5,2e-5,3e-5\n",
        "        * Max number of epochs (adam) : 10\n",
        "        * Weight decay : 0.1\n",
        "        * Learning rate decay : Linear\n",
        "        * Warmup ratio : 0.06 \n",
        "      * GPT-2 Model fine-tune hyper-parameters in paper(baseline : http://www.persagen.com/files/misc/radford2019language.pdf):\n",
        "        * Auto-regressive model\n",
        "      * XLNet-large fine-tune Model hyper-parameters in paper(baseline : https://arxiv.org/pdf/1906.08237.pdf):\n",
        "        * Same as BERT \n",
        "        * Batch_size : [16,32]\n",
        "        * Learning rate (adam) : 5e-5,3e-5,2e-5\n",
        "        * Number of epochs : 2,3,4\n",
        "  2. Load Model tokenizer\n",
        "  3. Load training and evaluation dataset\n",
        "  4. Define metrics to be evaluated \n",
        "    * `Datasets` library from transformers contain metrics which can be used \n",
        "    * https://huggingface.co/metrics\n",
        "  5. Encode training examples\n",
        "  6. Initialize model \n",
        "    * `AutoModelForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True)`\n",
        "  7. Define `trainer` from transformers\n",
        "    * Trainer classes provide feature complete API\n",
        "    * Before instantiating trainer, training arguments should be created to access customization during training\n",
        "    * https://huggingface.co/transformers/main_classes/trainer.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlG1vlbMEIEd"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ppyN4lMF1xY",
        "outputId": "04a32485-8ce6-4d24-c08e-66ab903757eb"
      },
      "source": [
        "! pip install optuna --quiet\n",
        "! pip install ray[tune] --quiet\n",
        "!pip install transformers==4.5.1 --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 307kB 14.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 23.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 21.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 24.3MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 49.4MB 78kB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 49.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 56.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 78.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 43.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 43.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 63.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 45.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 13.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 57.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 62.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBJnu5tbHidZ",
        "outputId": "0c685d28-b522-4183-861b-3b2548629106"
      },
      "source": [
        "pip install \"ray[tune]\" transformers datasets --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 245kB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 20.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 22.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2naGBoqBu7Z"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buFJdqBDODwK"
      },
      "source": [
        "MAX_LEN = 50\n",
        "RANDOM_SEED = 47\n",
        "# pl.seed_everything(RANDOM_SEED)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZppTTtNMfEJ"
      },
      "source": [
        "df = pd.read_csv(\"/content/explicitbias_Categorized.csv\", index_col = 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz6FFL7bPx6z",
        "outputId": "0aadb27e-7f31-4ec7-89b9-1e5b7c59c901"
      },
      "source": [
        "df.bias_type.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ethnicity              2559\n",
              "profession             1637\n",
              "gender                  656\n",
              "religion                256\n",
              "socioeconomic           157\n",
              "age                      73\n",
              "sexual-orientation       72\n",
              "disability               57\n",
              "physical-appearance      52\n",
              "Name: bias_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxH3N6uvNLry",
        "outputId": "185b22f6-0fcb-4fa8-c040-ef2835287a8d"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "\n",
        "labels = df['bias_type']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z5rUi-pngRW",
        "outputId": "94aa3556-ae38-4c21-f4e2-f6efcfecf249"
      },
      "source": [
        "for cls in range(len(encoder.classes_)):\n",
        "  print(cls,\":\" ,encoder.classes_[cls])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 : Ethnicity\n",
            "1 : age\n",
            "2 : disability\n",
            "3 : gender\n",
            "4 : physical-appearance\n",
            "5 : profession\n",
            "6 : religion\n",
            "7 : sexual-orientation\n",
            "8 : socioeconomic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdeAqCiKngRa"
      },
      "source": [
        "encoded_labels = {}\n",
        "for cls in range(len(encoder.classes_)):\n",
        "  encoded_labels[encoder.classes_[cls]] = cls"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7b42Wbh-4QY"
      },
      "source": [
        "dfLabelEncod = df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kDthFG2MhCd"
      },
      "source": [
        "dfLabelEncod['Label_encoded'] = dfLabelEncod['bias_type'].replace(encoded_labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jog2ydUbngRb"
      },
      "source": [
        "dfLabelEncod.drop(columns=['bias_type'],inplace=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "AdXtMrlpRpfI",
        "outputId": "a293b16f-51bd-48ca-9a3f-05640d93eede"
      },
      "source": [
        "dfLabelEncod"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stereotype</th>\n",
              "      <th>Label_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5397</th>\n",
              "      <td>The prisoner had a reputation for being violent.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1574</th>\n",
              "      <td>The game was won by the chess player. He is a ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>Though I've never been there, Jordan promised ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3237</th>\n",
              "      <td>The couple went to Norway. The locals were pro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>Due to poverty most African American families ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>The construction worker was from rural Kentucky.</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1670</th>\n",
              "      <td>The tailor was preparing to open up shop for t...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2379</th>\n",
              "      <td>Watching the news today, we were introduced to...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5399</th>\n",
              "      <td>Italy is known for it's great food.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3831</th>\n",
              "      <td>My hispanic neighbors are always smelly!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5519 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             stereotype  Label_encoded\n",
              "5397   The prisoner had a reputation for being violent.              5\n",
              "1574  The game was won by the chess player. He is a ...              5\n",
              "3056  Though I've never been there, Jordan promised ...              0\n",
              "3237  The couple went to Norway. The locals were pro...              0\n",
              "288   Due to poverty most African American families ...              0\n",
              "...                                                 ...            ...\n",
              "401    The construction worker was from rural Kentucky.              8\n",
              "1670  The tailor was preparing to open up shop for t...              5\n",
              "2379  Watching the news today, we were introduced to...              5\n",
              "5399                Italy is known for it's great food.              0\n",
              "3831           My hispanic neighbors are always smelly!              0\n",
              "\n",
              "[5519 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6DbzfdQTONl"
      },
      "source": [
        "Stratified sampling :\n",
        "\n",
        "* Why?\n",
        "  * With very small or very imbalanced data sets, it's quite possible that the random split could completely eliminate a class from one of the train/test splits.\n",
        "  * hence, setting `stratify = dependent_variable` makes sure that train and test splits have the same proportion of sampling. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23d8lleuNIyQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df_text, test_df_text, train_df_labels,test_df_labels = train_test_split(dfLabelEncod['stereotype'],dfLabelEncod['Label_encoded'], test_size=0.3, random_state=RANDOM_SEED, stratify = df['Label_encoded'])\n",
        "val_df_text, test_df_text, val_df_labels,test_df_labels = train_test_split(test_df_text,test_df_labels, test_size=0.5, random_state=RANDOM_SEED,stratify = test_df_labels)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTV_EZJJzfvN"
      },
      "source": [
        "train_df = pd.concat([train_df_text,train_df_labels],axis=1)\n",
        "val_df = pd.concat([val_df_text,val_df_labels], axis = 1)\n",
        "test_df = pd.concat([test_df_text,test_df_labels], axis = 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1XF5bynHs57"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGrJwTvsyfSn"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBJssqF6JiQc"
      },
      "source": [
        "class ExplicitStereotypeDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data: pd.DataFrame, tokenizer,max_token_len: int = 50):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.max_token_len = max_token_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[0]\n",
        "    text = data_row[0]\n",
        "    labels = data_row[1]\n",
        "    # labels = data_row.iloc[2:].to_dict().values() # To handle one-hot encoded categorical values [0-8] \n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_token_len,\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return dict(\n",
        "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "      input_ids=encoding[\"input_ids\"].flatten(),\n",
        "      token_type_ids=encoding[\"token_type_ids\"].flatten(),\n",
        "      label= torch.tensor(labels)\n",
        "    )"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv9QPUvAOUzs"
      },
      "source": [
        "train_dataset = ExplicitStereotypeDataset(\n",
        "  train_df,\n",
        "  tokenizer,\n",
        "  max_token_len=MAX_LEN\n",
        ")"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bjBluXzwbNQ"
      },
      "source": [
        "val_dataset = ExplicitStereotypeDataset(\n",
        "  val_df,\n",
        "  tokenizer,\n",
        "  max_token_len=MAX_LEN\n",
        ")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slb1M5xXLtbz"
      },
      "source": [
        "sample = train_dataset[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPRiHdrfO6Jx",
        "outputId": "c71f9f47-6658-4390-d765-9308fd0d37b0"
      },
      "source": [
        "sample.keys()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'input_ids', 'token_type_ids', 'label'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAxzdye6gtJu",
        "outputId": "ad6c7759-6c19-4e80-a306-b1a525ce6d8b"
      },
      "source": [
        "sample['label']"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-jnMN2oLUBO"
      },
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained('bert-base-cased', return_dict = True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLAexOe4XyV5"
      },
      "source": [
        "sample_batch = next(iter(DataLoader(train_dataset,batch_size=32)))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RarMClNneO6Y",
        "outputId": "4e525cfb-951f-4c4f-e8e1-5670411a17d5"
      },
      "source": [
        "bert = model_init()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7sqWWmFfT9W"
      },
      "source": [
        "output = bert(sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"],labels = sample_batch['label'])"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9apQYOjedjX",
        "outputId": "5ff99f3c-3d0f-497d-8cd9-b9a8036e51b4"
      },
      "source": [
        "output.logits"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831],\n",
              "        [-0.2094, -0.3831]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jGq0tpYLZHe"
      },
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax()\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88kH21iOKaNq",
        "outputId": "43fea5b2-b95f-440d-9ca6-9c44e8a56856"
      },
      "source": [
        "# Evaluate during training and a bit more often than the default to be able to prune bad trials early.\n",
        "# Disabling tqdm is a matter of preference.\n",
        "# training_args = TrainingArguments(\"test\", evaluate_during_training=True, eval_steps=500, disable_tqdm=True)\n",
        "trainer = Trainer(\n",
        "    # args=training_args,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=val_dataset, \n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wyzOy97ak5Z"
      },
      "source": [
        "# Defaut objective is the sum of all metrics when metrics are provided, so we have to maximize it.\n",
        "trainer.hyperparameter_search(n_trials=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqzARLEIEDfJ"
      },
      "source": [
        "## XL-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLtHqVNUEPnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J26KgkN6EQMx"
      },
      "source": [
        "## Roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2D69HW_ERou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml3E-qoOESQ3"
      },
      "source": [
        "## GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnnZSaDUETI8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXyYcICXjOSa"
      },
      "source": [
        "# Class imbalance handling methods\n",
        "\n",
        "Link 1 :\n",
        "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n",
        "\n",
        "Link 2 : https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwHWwyDVjRZA"
      },
      "source": [
        "What?\n",
        "  * Imbalance is most common problem\n",
        "  * Class1 - 80 samples\n",
        "  * Class2 - 20 samples \n",
        "\n",
        "Accuracy Paradox:\n",
        "  * Accuracy metric may reflect the underlying class distribution.\n",
        "    * Just predict class 1 irrespective of the input due to its class distribution.\n",
        "    * Accuracy = `(80/100)*100 = 80%` \n",
        "    * But the model didnot learn anything.\n",
        "\n",
        "\n",
        "Strategies:\n",
        "\n",
        "1. Collect more data\n",
        "2. Change performance metric:\n",
        "  * Confusion matrix : Breaking the predictions into\n",
        "    * Correct predictions:\n",
        "      * True positive \n",
        "      * True Negative\n",
        "    * Incorrect predictions:\n",
        "      * False positive\n",
        "      * False negative \n",
        "  * Precision : \n",
        "    * **correct positive prediction** out of **total positive predictions** (correct and incorrect).\n",
        "  * Recall (sensitivity/TPR) : \n",
        "    * **Identified correct positive** predictions out of **total positive class in the dataset**.  \n",
        "  * F1 score : \n",
        "    * Weighted average of precision and recall.\n",
        "  * Kappa score:\n",
        "    * Classification score normalized by the imbalance of classes in data.\n",
        "    * Range from -1/0 - 1(perfect) \n",
        "  * ROC curve : \n",
        "    * TP (sensitivity) plotted against FP (1 – specificity) for each threshold used.\n",
        "    * Useful for threshold selection \n",
        "      * Selecting threshold based on the dataset \n",
        "      * e.g.: Cancer screening : \n",
        "          * High FP along with TP is fine, as it is important to identify sufferers than having false negative.\n",
        "    * ROC_AUC score : Gives performance of classifier over entire operating range.\n",
        "    * Classifier comparison : Compare two models using ROC_AUC score. \n",
        "3. Resampling data \n",
        "  * Over-sampling:\n",
        "      * Add copies from under-represented class.\n",
        "      * Algorithms:\n",
        "        * SMOTE(Synthetic minority over sampling technique)\n",
        "          * Compute k-NN from minority class and impute.\n",
        "        * Random over-sampling\n",
        "      * Dis-advantage:\n",
        "        * Impact generalization and may overfit the data.\n",
        "  * Under-sampling:\n",
        "    * Delete copies from over-represented class.\n",
        "    * Algorithms\n",
        "      * NearMiss\n",
        "      * Random under-sampling\n",
        "    * Dis-advantage:\n",
        "      * May loose important information \n",
        "  * Points:\n",
        "    * Consider testing random split and non-random (e.g. stratified) splits.\n",
        "4. Different ML model:\n",
        "  * Decision trees \n",
        "    * CART\n",
        "    * Random forest\n",
        "5. Penalized models:\n",
        "  * Impose additional cost when predicting minority class to pay more attention.\n",
        "    * Train model with class weights \n",
        "6. Different problem\n",
        "  * Anamoly detection\n",
        "    * One-class classifier \n",
        "  * Change detection \n",
        "\n",
        "\n"
      ]
    }
  ]
}